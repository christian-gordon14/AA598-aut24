{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 2 libs\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle # for the project\n",
    "\n",
    "from aa598.hw2_helper import simulate_dynamics\n",
    "import cvxpy as cp\n",
    "from cbfax.dynamics import *\n",
    "\n",
    "# Homework 1 libs\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import aa598.hw1_helper as hw1_helper\n",
    "\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "rc('text', usetex=False) # set to False if latex is not set up on your computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = DynamicallyExtendedSimpleCar() # robot dynamics\n",
    "human = DynamicallyExtendedSimpleCar() # human dynamics\n",
    "\n",
    "@jax.jit\n",
    "def obstacle_constraint(state, obstacle, radius):\n",
    "    return jnp.linalg.norm(state[:2] - obstacle[:2]) - radius\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_horizon = 25\n",
    "num_time_steps = 30\n",
    "num_sqp_iterations = 15\n",
    "dt = 0.1\n",
    "t = 0. # this doesn't affect anything, but a value is needed \n",
    "radius = 1. # minimum collision distance\n",
    "\n",
    "v_max = 1.5\n",
    "v_min = 0.\n",
    "acceleration_max = 1.0\n",
    "acceleration_min = -1.0\n",
    "steering_max = 0.3\n",
    "steering_min = -0.3\n",
    "\n",
    "human_control_prediction_noise_limit = 0.25\n",
    "human_control_prediction_variance = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = cp.Variable([planning_horizon+1, robot.state_dim])  # cvx variable for states\n",
    "us = cp.Variable([planning_horizon, robot.control_dim])  # cvx variable for controls\n",
    "slack = cp.Variable(1) # slack variable to make sure the problem is feasible\n",
    "As = [cp.Parameter([robot.state_dim, robot.state_dim]) for _ in range(planning_horizon)]  # parameters for linearized dynamics\n",
    "Bs = [cp.Parameter([robot.state_dim, robot.control_dim]) for _ in range(planning_horizon)] # parameters for linearized dynamics\n",
    "Cs = [cp.Parameter([robot.state_dim]) for _ in range(planning_horizon)] # parameters for linearized dynamics\n",
    "\n",
    "Gs = [cp.Parameter([robot.state_dim]) for _ in range(planning_horizon+1)] # parameters for linearized constraints\n",
    "hs = [cp.Parameter(1) for _ in range(planning_horizon+1)] # parameters for linearized constraints\n",
    "\n",
    "xs_previous = cp.Parameter([planning_horizon+1, robot.state_dim]) # parameter for previous solution\n",
    "us_previous = cp.Parameter([planning_horizon, robot.control_dim]) # parameter for previous solution\n",
    "initial_state = cp.Parameter([robot.state_dim]) # parameter for current robot state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = 0.2 # coefficient for control effort\n",
    "beta2 = 2. # coefficient for progress\n",
    "beta3 = 10. # coefficient for trust region\n",
    "slack_penalty = 1000. # coefficient for slack variable\n",
    "markup = 1.0\n",
    "\n",
    "objective = beta2 * (xs[-1,2]**2 + xs[-1,1]**2 - xs[-1,0]) + beta3 * (cp.sum_squares(xs - xs_previous) + cp.sum_squares(us - us_previous)) + slack_penalty * slack**2\n",
    "constraints = [xs[0] == initial_state, slack >= 0] # initial state and slack constraint\n",
    "for t in range(planning_horizon):\n",
    "    objective += beta1 * cp.sum_squares(us[t]) * markup**t\n",
    "    constraints += [xs[t+1] == As[t] @ xs[t] + Bs[t] @ us[t] + Cs[t]] # dynamics constraint\n",
    "    constraints += [xs[t,-1] <= v_max, xs[t,-1] >= v_min, us[t,0] <= acceleration_max, us[t,0] >= acceleration_min, us[t,1] <= steering_max, us[t,1] >= steering_min] # control limit constraints\n",
    "    constraints += [Gs[t] @ xs[t] + hs[t] >= -slack] # linearized collision avoidance constraint\n",
    "constraints += [xs[planning_horizon,-1] <= v_max, xs[planning_horizon,-1] >= v_min, Gs[planning_horizon] @ xs[planning_horizon] + hs[planning_horizon] >= 0] # constraints for last planning horizon step\n",
    "prob = cp.Problem(cp.Minimize(objective), constraints) # construct problem\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial states\n",
    "robot_state = jnp.array([-3.0, -0., 0., 1.])  # robot starting state\n",
    "human_state = jnp.array([-1., -2., jnp.pi/2, 1.]) # human starting state\n",
    "\n",
    "robot_trajectory = [robot_state] # list to collect robot's state as it replans\n",
    "human_trajectory = [human_state] # list to collect humans's state\n",
    "robot_control_list = []  # list to collect robot's constrols as it replans\n",
    "robot_trajectory_list = [] # list to collect robot's planned trajectories\n",
    "human_control_list = []\n",
    "\n",
    "# initial robot planned state and controls\n",
    "previous_controls = jnp.zeros([planning_horizon, robot.control_dim]) # initial guess for robot controls\n",
    "previous_states =  simulate_dynamics(robot, robot_state, previous_controls, dt) # initial guess for robot states\n",
    "xs_previous.value = np.array(previous_states) # set xs_previous parameter value\n",
    "us_previous.value = np.array(previous_controls) # set us_previous parameter value \n",
    "\n",
    "# jit the linearize dynamics and constraint functions to make it run faster\n",
    "linearize_dynamics = jax.jit(lambda states, controls, ti: jax.vmap(linearize, [None, 0, 0, None])(lambda s, c, t: robot.discrete_step(s, c, t, dt), states, controls, ti))\n",
    "linearize_obstacle = jax.jit(lambda states, controls, radius: jax.vmap(jax.grad(obstacle_constraint), [0, 0, None])(states, controls, radius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 0\n",
      "timestep: 1\n",
      "timestep: 2\n",
      "timestep: 3\n",
      "timestep: 4\n",
      "timestep: 5\n",
      "timestep: 6\n",
      "timestep: 7\n",
      "timestep: 8\n",
      "timestep: 9\n",
      "timestep: 10\n",
      "timestep: 11\n",
      "timestep: 12\n",
      "timestep: 13\n",
      "timestep: 14\n",
      "timestep: 15\n",
      "timestep: 16\n",
      "timestep: 17\n",
      "timestep: 18\n",
      "timestep: 19\n",
      "timestep: 20\n",
      "timestep: 21\n",
      "timestep: 22\n",
      "timestep: 23\n",
      "timestep: 24\n",
      "timestep: 25\n",
      "timestep: 26\n",
      "timestep: 27\n",
      "timestep: 28\n",
      "timestep: 29\n"
     ]
    }
   ],
   "source": [
    "solver = cp.CLARABEL\n",
    "\n",
    "for t in range(num_time_steps):\n",
    "    print(\"timestep: %i\"% t)\n",
    "    initial_state.value = np.array(robot_state)\n",
    "    # simulate human future trajectory, assuming some noisy behavior\n",
    "    noisy_human_control = jnp.clip(jnp.array(np.random.randn(planning_horizon, human.control_dim) * human_control_prediction_variance), -human_control_prediction_noise_limit, human_control_prediction_noise_limit)\n",
    "    human_future = simulate_dynamics(human, human_state, noisy_human_control, dt)\n",
    "    \n",
    "    for i in range(num_sqp_iterations):\n",
    "        # As_value, Bs_value, Cs_value = jax.vmap(linearize, [None, 0, 0, None])(lambda s, c, t: robot.discrete_step(s, c, t, dt), previous_states[:-1], previous_controls, t)\n",
    "        As_value, Bs_value, Cs_value = linearize_dynamics( previous_states[:-1], previous_controls, t)\n",
    "        # Gs_value = jax.vmap(jax.grad(obstacle_constraint), [0, 0, None])(previous_states, human_future, radius)\n",
    "        Gs_value = linearize_obstacle(previous_states, human_future, radius)\n",
    "        hs_value = jax.vmap(obstacle_constraint, [0, 0, None])(previous_states, human_future, radius) - jax.vmap(jnp.dot, [0, 0])(Gs_value, previous_states)\n",
    "\n",
    "        for i in range(planning_horizon):\n",
    "            As[i].value = np.array(As_value[i])\n",
    "            Bs[i].value = np.array(Bs_value[i])\n",
    "            Cs[i].value = np.array(Cs_value[i])\n",
    "            Gs[i].value = np.array(Gs_value[i])\n",
    "            hs[i].value = np.array(hs_value[i:i+1])\n",
    "        Gs[planning_horizon].value = np.array(Gs_value[planning_horizon])\n",
    "        hs[planning_horizon].value = np.array(hs_value[planning_horizon:planning_horizon+1])\n",
    "        \n",
    "        result = prob.solve(solver=solver)\n",
    "\n",
    "        # previous_states = xs.value\n",
    "        previous_controls = us.value\n",
    "        previous_states =  simulate_dynamics(robot, robot_state, previous_controls, dt)\n",
    "        xs_previous.value = np.array(previous_states)\n",
    "        us_previous.value = np.array(previous_controls)\n",
    "       \n",
    "    robot_control = previous_controls[0]\n",
    "    robot_control_list.append(robot_control)\n",
    "    # robot takes a step\n",
    "    robot_state = robot.discrete_step(robot_state, robot_control, 0., dt)\n",
    "    robot_trajectory.append(robot_state)\n",
    "    robot_trajectory_list.append(previous_states)\n",
    "\n",
    "    \n",
    "    human_random_control = jnp.clip(jnp.array(np.random.randn( human.control_dim) * human_control_prediction_variance), -human_control_prediction_noise_limit, human_control_prediction_noise_limit)\n",
    "    human_control_list.append(human_random_control)\n",
    "    # human states a step\n",
    "    human_state = human.discrete_step(human_state, human_random_control, 0., dt)\n",
    "    human_trajectory.append(human_state)\n",
    "\n",
    "human_traj = [human_trajectory]\n",
    "human_random_cont = (human_random_control)\n",
    "robot_trajectory = jnp.stack(robot_trajectory)\n",
    "human_trajectory = jnp.stack(human_trajectory)\n",
    "robot_controls = jnp.stack(robot_control_list)\n",
    "human_controls = jnp.stack(human_control_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #Project code\n",
    "# history_trajectory = jnp.array([robot_trajectory], [robot_control_list],[human_trajectory], [human_random_control])\n",
    "# with open('history.pickle', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(history_trajectory, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open('future.pickle', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(singleTrainData, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# # Training data\n",
    "# trajectoryTrainingData = {'xPosition' : robot_trajectory[:-5, 0], 'yPosition' : robot_trajectory[:-5, 1],\n",
    "#         'heading' : robot_trajectory[:-5, 2], 'velocity' : robot_trajectory[:-5, 3]}\n",
    "# with open('robot_trajectory_train_v3.pickle', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(trajectoryTrainingData, f, pickle.HIGHEST_PROTOCOL)\n",
    "# # Testing data\n",
    "# trajectoryTestingData = {'xPosition' : robot_trajectory[-5:, 0], 'yPosition' : robot_trajectory[-5:, 1],\n",
    "#         'heading' : robot_trajectory[-5:, 2], 'velocity' : robot_trajectory[-5:, 3]}\n",
    "# with open('robot_trajectory_test_v3.pickle', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(trajectoryTestingData, f, pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570f1c80dd834734b296d623b997762c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=14, description='i', max=29), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting\n",
    "@interact(i=(0,num_time_steps-1))\n",
    "def plot(i):\n",
    "    fig, axs = plt.subplots(1,2, figsize=(18,8))\n",
    "    ax = axs[0]\n",
    "    robot_position = robot_trajectory[i, :2]\n",
    "    human_position = human_trajectory[i, :2]\n",
    "    circle1 = plt.Circle(robot_position, radius / 2, color='C0', alpha=0.4)\n",
    "    circle2 = plt.Circle(human_position, radius / 2, color='C1', alpha=0.4)\n",
    "    ax.add_patch(circle1)\n",
    "    ax.add_patch(circle2)\n",
    "    # ax.plot(human_samples[i,:,:,0].T, human_samples[i,:,:,1].T, \"o-\", alpha=0.1, markersize=2, color='C1')\n",
    "    ax.plot(robot_trajectory[:,0], robot_trajectory[:,1], \"o-\", markersize=3, color='C0')\n",
    "    ax.plot(robot_trajectory_list[i][:,0], robot_trajectory_list[i][:,1], \"o-\", markersize=3, color='C2', label=\"planned\")\n",
    "    print(robot_trajectory[i])\n",
    "\n",
    "    ax.plot(human_trajectory[:,0], human_trajectory[:,1], \"o-\", markersize=3, color='C1')\n",
    "    ax.scatter(robot_trajectory[i:i+1,0], robot_trajectory[i:i+1,1], s=30,  color='C0', label=\"Robot\")\n",
    "    ax.scatter(human_trajectory[i:i+1,0], human_trajectory[i:i+1,1], s=30,  color='C1', label=\"Human\")\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_xlim([-4,4])\n",
    "    ax.set_ylim([-3, 2])\n",
    "    ax.axis(\"equal\")\n",
    "\n",
    "    ax.set_title(\"heading=%.2f velocity=%.2f\"%(robot_trajectory[i,2], robot_trajectory[i,3]))\n",
    "    \n",
    "    ax = axs[1]\n",
    "    plt.plot(robot_controls)\n",
    "    plt.scatter([i], robot_controls[i:i+1, 0], label=\"Acceleration\")\n",
    "    plt.scatter([i], robot_controls[i:i+1, 1], label=\"Steering\")\n",
    "    ax.plot(robot_trajectory[:,-1], \"o-\", markersize=3, color='C0', label=\"Velocity\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.77215546 -0.8842893  -0.9601989  -0.9967745  -0.99251807]\n",
      " [-0.33736357 -0.6201609  -0.83809453 -0.96837014 -0.99736214]\n",
      " [-0.9241497  -0.9772044  -0.9993569  -0.98990667 -0.9491525 ]\n",
      " ...\n",
      " [ 0.8635387   0.84961313  0.8350685   0.8199155   0.80416495]\n",
      " [-0.78483945 -0.86998636 -0.934877   -0.9780004  -0.9983526 ]\n",
      " [ 0.9903264   0.98666096  0.8932651   0.7186327   0.47864532]]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/hw1/wave_data_train.pickle\", 'rb') as handle:\n",
    "    wave_data = pickle.load(handle)\n",
    "history = wave_data[\"history\"]\n",
    "future = wave_data[\"future\"]\n",
    "print((future))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "zero_row = jnp.zeros((1, 2))\n",
    "if (robot_controls[0] != 0).all():\n",
    "    robot_controls = jnp.vstack([zero_row, robot_controls])\n",
    "if (human_controls[0] != 0).all():\n",
    "    human_controls = jnp.vstack([zero_row, human_controls])\n",
    "\n",
    "totalMatrix = jnp.concatenate([robot_trajectory, robot_controls, human_trajectory, human_controls], axis = 1) \n",
    "# standardMatrix = jax.nn.standardize(totalMatrix)\n",
    "totalMatrix = np.array(totalMatrix)\n",
    "print(type(totalMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.5973402  -0.11299384 -0.06318497  0.5539429   0.23945051  0.07150198\n",
      " -1.101018    1.1584742   1.6394137   1.043159    0.25       -0.22132698]\n"
     ]
    }
   ],
   "source": [
    "print(totalMatrix[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54116279 0.15551187 0.21744933 0.37292977]\n",
      " [0.51762252 0.30367043 0.72263538 0.81886209]\n",
      " [0.7259154  0.571052   0.8778375  0.42623151]\n",
      " [0.22647988 0.80727154 0.25902346 0.23710195]]\n"
     ]
    }
   ],
   "source": [
    "A = np.random.rand(4, 4)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.delete(A, [1, 2], axis=1)\n",
    "# \n",
    "# print(np.size(totalMatrix))\n",
    "# print(totalMatrix[:, 4:6])\n",
    "onlyHistoryData = np.delete(totalMatrix, [4, 6], axis = 1) # deleting the 4, 5 row because this is 'x'\n",
    "onlyTestData = (totalMatrix[:, 4:6])\n",
    "train_data_history = np.transpose(onlyHistoryData[:15, :])\n",
    "train_data_future = np.transpose(onlyHistoryData[:15, :])\n",
    "test_data_history = np.transpose(onlyTestData[:15, :])\n",
    "test_data_future = np.transpose(onlyTestData[15:, :])\n",
    "train_data = {'history' : train_data_history, 'future' : train_data_future}\n",
    "test_data = {'history' : test_data_history, 'future' : test_data_future}\n",
    "# print(len(train_data['history']))\n",
    "# print(train_data['future'])\n",
    "# print(len(train_data['history']))\n",
    "# print((test_data['future']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the training data\n",
    "with open('train.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(train_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "# This will be the testing data\n",
    "with open('test.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(test_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "# When training will have to leave out the testing robot controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.pickle\", 'rb') as handle:\n",
    "    train = pickle.load(handle)\n",
    "with open(\"test.pickle\", 'rb') as handle:\n",
    "    test = pickle.load(handle)\n",
    "# print((test.shape))\n",
    "# print((train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.arange(0,31)\n",
    "# print((totalMatrix))\n",
    "# plt.plot(t, standardMatrix[0:31, 0], color  = 'r') # Robot controls\n",
    "# plt.plot(t, standardMatrix[0:31, 6], color  = 'b') # Human controls\n",
    "plt.plot(t, totalMatrix[0:31, 0], color  = 'y') # Robot controls\n",
    "plt.plot(t, totalMatrix[0:31, 6], color  = 'g') # Human controls\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to poke around the data\n",
    "# THIS NEEDS TO BE REVIEWED\n",
    "test_data = hw1_helper.TrajectoryData(\"test\")\n",
    "train_data = hw1_helper.TrajectoryData(\"train\")\n",
    "\n",
    "\n",
    "# history_length = 3 # Number of backward steps considered to train\n",
    "# future_length = 3 # Number of forward considered to train\n",
    "# input_size = 10\n",
    "# output_size = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 10)\n",
      "<class 'dict'>\n",
      "(tensor([-0.0039,  0.0352]), tensor([0., 0.]))\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/hw1/wave_data_test.pickle\", 'rb') as handle:\n",
    "    wave_data = pickle.load(handle)\n",
    "print(wave_data['history'].shape)\n",
    "print(type(wave_data))\n",
    "print(test_data[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # HINT: Use Pytorch built-in functions for LSTM and Linear layers.\n",
    "        # HINT: batch dimension is dim=0\n",
    "\n",
    "        '''\n",
    "        looking at the code provided on pytorch\n",
    "\n",
    "        \n",
    "        nn.LSTM is looking for an input and an output without specifying that it is\n",
    "        being used as an encoder or a decoder so it can be reversed?\n",
    "\n",
    "        nn.Linear is a transformation that turn the output into a linearized equation where y = Ax + b\n",
    "            b is the bias\n",
    "            A is the weight\n",
    "            y is the output\n",
    "        '''\n",
    "        \n",
    "        # TODO: Define encoder LSTM.\n",
    "        self.encoder = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        ############################\n",
    "        \n",
    "        \n",
    "        # TODO: Define decoder LSTM\n",
    "        self.decoder = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        ############################\n",
    "        \n",
    "        \n",
    "        #TODO: Define linear project from hidden_dim to output_dim\n",
    "        # The output is a scalar but it needs to be an array [~,4]\n",
    "        self.projection = torch.nn.Linear(hidden_dim, output_dim, bias = False)\n",
    "        ############################\n",
    "        \n",
    "\n",
    "    def forward(self, x, t_max, y=None, prob=1.):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM model.\n",
    "        x: The input sequence [batch_size, seq_len, input_dim]\n",
    "        t_max: maximum time steps to unroll\n",
    "        y: The target sequence for teacher forcing (optional, used if teacher forcing is applied) [batch_size, t_max, output_dim]\n",
    "        prob: Probability to apply teacher forcing (0 to 1). 1 means 100% teacher forcing, \n",
    "        \"\"\"\n",
    "        \n",
    "        # making sure x and y is the appropriate size.\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(-1)\n",
    "        if y is not None and len(y.shape) == 2:\n",
    "            y = y.unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "        ys = [] # collect outputs\n",
    "        # TODO: Run input through encoder to get initial hidden state for decoder\n",
    "        _, h = self.encoder(x)\n",
    "        ############################\n",
    "        \n",
    "        \n",
    "        # TODO: initial state for decoder is last input state\n",
    "        y_input = x[:,-1]\n",
    "        # print('y_input = ', y_input)\n",
    "        y_input = y_input.unsqueeze(1)\n",
    "        ############################\n",
    "        \n",
    "\n",
    "        # TODO: unroll decoder \n",
    "        # TODO: if eval or no teacher forcing, use prediction from previous step\n",
    "        # TODO: if train and using teacher forcing, use prob to determine whether to use ground truth or previous prediction\n",
    "        \n",
    "        ############################\n",
    "        for i in range(t_max):\n",
    "            output, h = self.decoder(y_input, h)\n",
    "            output = self.projection(output)\n",
    "            # TODO: y_input = ...?\n",
    "            random = torch.rand([1, 1])\n",
    "            # if random >= 0.7:\n",
    "            #     y_input = output\n",
    "            # else:\n",
    "            #     y_input = y_input\n",
    "            y_input = output\n",
    "            ys.append(output.squeeze(1))\n",
    "        ys = torch.cat(ys, dim=1)\n",
    "    \n",
    "        \n",
    "        return ys # [batch_size, ts_max, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1\n",
    "future_length = 15\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "model = LSTM(input_size, output_size, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 10 # This is the number of data points in a array\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "prob = 0.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "def prob_schedule(i):\n",
    "    return 1 - jax.nn.sigmoid(20 * (i - 0.5)).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [0/1], Loss: 1.3268\n",
      "Epoch 1 completed with average loss: 1.3268\n",
      "Epoch [2/10], Step [0/1], Loss: 1.3255\n",
      "Epoch 2 completed with average loss: 1.3255\n",
      "Epoch [3/10], Step [0/1], Loss: 1.3242\n",
      "Epoch 3 completed with average loss: 1.3242\n",
      "Epoch [4/10], Step [0/1], Loss: 1.3229\n",
      "Epoch 4 completed with average loss: 1.3229\n",
      "Epoch [5/10], Step [0/1], Loss: 1.3217\n",
      "Epoch 5 completed with average loss: 1.3217\n",
      "Epoch [6/10], Step [0/1], Loss: 1.3205\n",
      "Epoch 6 completed with average loss: 1.3205\n",
      "Epoch [7/10], Step [0/1], Loss: 1.3192\n",
      "Epoch 7 completed with average loss: 1.3192\n",
      "Epoch [8/10], Step [0/1], Loss: 1.3180\n",
      "Epoch 8 completed with average loss: 1.3180\n",
      "Epoch [9/10], Step [0/1], Loss: 1.3168\n",
      "Epoch 9 completed with average loss: 1.3168\n",
      "Epoch [10/10], Step [0/1], Loss: 1.3156\n",
      "Epoch 10 completed with average loss: 1.3156\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# we use a slightly different training loop to account for teacher forcing\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    prob = prob_schedule((epoch + 1)/num_epochs)\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()        # Zero the gradients\n",
    "        output = model(data, future_length, target, prob)         # Forward pass\n",
    "        loss = criterion(output, target)  # Compute loss\n",
    "        loss.backward()              # Backpropagation\n",
    "        optimizer.step()             # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f16888a7874f78a55d001b5a2fc48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Index:', max=1), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function aa598.hw1_helper.plot_data_regression(history, future, prediction, index, xlims=[-11, 5], ylims=[-2, 2])>"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history, future_length)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# try with different prediction horizons\n",
    "prediction_horizon = 15\n",
    "prediction = model(history, prediction_horizon)\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "test_dataloader = list(DataLoader(test_data, batch_size=1, shuffle=False))\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_dataloader)-1, step=1, description='Index:')\n",
    "xlims = [-16, prediction_horizon + 2]\n",
    "ylims = [-2,2]\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  tensor([ 0.0489, -0.1595, -0.2846, -0.3492, -0.3809, -0.3964, -0.4040, -0.4077,\n",
      "        -0.4096, -0.4105, -0.4110, -0.4112, -0.4114, -0.4114, -0.4114],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "1  tensor([ 0.0442, -0.1635, -0.2867, -0.3503, -0.3814, -0.3966, -0.4041, -0.4078,\n",
      "        -0.4096, -0.4105, -0.4110, -0.4112, -0.4114, -0.4114, -0.4114],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "future [[-0.00387421  0.05044828 -0.00443483 -0.14180897 -0.03551263  0.06864742\n",
      "   0.14013654  0.18809564  0.21916485  0.23815946  0.24857408  0.25293195\n",
      "   0.2530586   0.2502687   0.24550675  0.23945051]\n",
      " [ 0.03522268  0.04250023  0.0416534   0.03474009  0.04036722  0.04646078\n",
      "   0.05113149  0.05497469  0.05830288  0.0612583   0.06388623  0.06618173\n",
      "   0.06811833  0.06966566  0.07079853  0.07150198]]\n"
     ]
    }
   ],
   "source": [
    "print('0 ', prediction[0, :])\n",
    "print('1 ',prediction[1, :])\n",
    "# print('history', test['history'])\n",
    "print('future', test['future'])\n",
    "# print('history', test['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "[[ 0.0000000e+00 -9.0000528e-01 -1.0000000e+00 -9.5578152e-01\n",
      "  -7.4095374e-01 -5.6258792e-01 -4.7597197e-01 -3.4102097e-01\n",
      "  -2.3188767e-01 -1.4394751e-01 -5.5125856e-01 -3.9765233e-01\n",
      "  -2.3510808e-01 -1.1261089e-01 -2.0596191e-02]\n",
      " [ 0.0000000e+00 -2.9999998e-01 -3.0000001e-01 -2.9999995e-01\n",
      "  -1.6630402e-01 -8.1035547e-02 -5.7633467e-02 -1.3073515e-02\n",
      "   1.6388962e-02  3.5917170e-02 -5.4274812e-02 -2.5512984e-02\n",
      "   5.7829340e-04  1.8761056e-02  3.1675894e-02]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[ 0.04887867 -0.1594983  -0.28455615 -0.3491968  -0.38090858 -0.39637\n",
      "  -0.40396464 -0.4077252  -0.40959746 -0.41053256 -0.4110004  -0.41123474\n",
      "  -0.41135204 -0.4114109  -0.41144034]\n",
      " [ 0.04418157 -0.16351093 -0.2867476  -0.35028544 -0.3814388  -0.3966294\n",
      "  -0.40409273 -0.40778887 -0.40962917 -0.41054842 -0.41100833 -0.41123864\n",
      "  -0.41135406 -0.41141185 -0.41144085]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x140543380>"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.linspace(0,16, 16)\n",
    "# plt.plot(t, prediction[:, 0].detach().numpy())\n",
    "print(type(prediction))\n",
    "# print(prediction[:, :])\n",
    "predict = prediction.detach().numpy()\n",
    "# future = future.detach().numpy()\n",
    "print(future)\n",
    "print(type(predict))\n",
    "print(predict)\n",
    "plt.scatter(predict[0, :], predict[1, :], marker = 'o',  color = 'r')\n",
    "plt.scatter(future[0, :], future[1, :], marker = 'o', color = 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'y axis')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = jnp.arange(0, len(trainXPosition), 1)\n",
    "j = jnp.arange(0, len(testXPosition), 1)\n",
    "plt.scatter(t, trainXPosition, color='b')\n",
    "plt.scatter(j+len(trainXPosition), testXPosition , color = 'k')\n",
    "# plt.scatter(j+len(trainXPosition), prediction)\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # plotting\n",
    "# @interact(i=(0,num_time_steps-1))\n",
    "# def plot(i):\n",
    "#     fig, axs = plt.subplots(1,2, figsize=(18,8))\n",
    "#     ax = axs[0]\n",
    "#     robot_position = train_data[i]\n",
    "#     circle1 = plt.Circle(robot_position, radius / 2, color='C0', alpha=0.4)\n",
    "#     ax.add_patch(circle1)\n",
    "#     ax.add_patch(circle2)\n",
    "#     # ax.plot(human_samples[i,:,:,0].T, human_samples[i,:,:,1].T, \"o-\", alpha=0.1, markersize=2, color='C1')\n",
    "#     ax.plot(train_data[0], train_data[1], \"o-\", markersize=3, color='C0')\n",
    "#     ax.plot(train_data[i][0], train_data[i][1], \"o-\", markersize=3, color='C2', label=\"planned\")\n",
    "#     ax.scatter(train_data[i:i+1], train_data[i:i+1], s=30,  color='C0', label=\"Robot\")\n",
    "#     ax.grid()\n",
    "#     ax.legend()\n",
    "\n",
    "#     ax.set_xlim([-4,4])\n",
    "#     ax.set_ylim([-3, 2])\n",
    "#     ax.axis(\"equal\")\n",
    "\n",
    "#     ax.set_title(\"heading=%.2f velocity=%.2f\"%(robot_trajectory[i,2], robot_trajectory[i,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple MLP model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, history_length, future_length, hidden_size=32):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # TODO: construct MLP network\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(history_length, 20),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(20, 40),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(40, 25),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(25, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, future_length),\n",
    "        )\n",
    "        #############################\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 4\n",
    "history_length = 15\n",
    "future_length = 15\n",
    "\n",
    "model = MLP(history_length, future_length, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 1 completed with average loss: 0.0023\n",
      "Epoch [2/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 2 completed with average loss: 0.0023\n",
      "Epoch [3/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 3 completed with average loss: 0.0023\n",
      "Epoch [4/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 4 completed with average loss: 0.0023\n",
      "Epoch [5/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 5 completed with average loss: 0.0023\n",
      "Epoch [6/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 6 completed with average loss: 0.0023\n",
      "Epoch [7/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 7 completed with average loss: 0.0022\n",
      "Epoch [8/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 8 completed with average loss: 0.0022\n",
      "Epoch [9/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 9 completed with average loss: 0.0022\n",
      "Epoch [10/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 10 completed with average loss: 0.0022\n",
      "Epoch [11/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 11 completed with average loss: 0.0022\n",
      "Epoch [12/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 12 completed with average loss: 0.0022\n",
      "Epoch [13/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 13 completed with average loss: 0.0022\n",
      "Epoch [14/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 14 completed with average loss: 0.0022\n",
      "Epoch [15/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 15 completed with average loss: 0.0022\n",
      "Epoch [16/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 16 completed with average loss: 0.0021\n",
      "Epoch [17/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 17 completed with average loss: 0.0021\n",
      "Epoch [18/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 18 completed with average loss: 0.0021\n",
      "Epoch [19/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 19 completed with average loss: 0.0021\n",
      "Epoch [20/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 20 completed with average loss: 0.0021\n",
      "Epoch [21/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 21 completed with average loss: 0.0021\n",
      "Epoch [22/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 22 completed with average loss: 0.0021\n",
      "Epoch [23/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 23 completed with average loss: 0.0021\n",
      "Epoch [24/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 24 completed with average loss: 0.0021\n",
      "Epoch [25/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 25 completed with average loss: 0.0021\n",
      "Epoch [26/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 26 completed with average loss: 0.0021\n",
      "Epoch [27/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 27 completed with average loss: 0.0020\n",
      "Epoch [28/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 28 completed with average loss: 0.0020\n",
      "Epoch [29/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 29 completed with average loss: 0.0020\n",
      "Epoch [30/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 30 completed with average loss: 0.0020\n",
      "Epoch [31/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 31 completed with average loss: 0.0020\n",
      "Epoch [32/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 32 completed with average loss: 0.0020\n",
      "Epoch [33/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 33 completed with average loss: 0.0020\n",
      "Epoch [34/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 34 completed with average loss: 0.0020\n",
      "Epoch [35/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 35 completed with average loss: 0.0020\n",
      "Epoch [36/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 36 completed with average loss: 0.0020\n",
      "Epoch [37/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 37 completed with average loss: 0.0020\n",
      "Epoch [38/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 38 completed with average loss: 0.0020\n",
      "Epoch [39/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 39 completed with average loss: 0.0019\n",
      "Epoch [40/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 40 completed with average loss: 0.0019\n",
      "Epoch [41/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 41 completed with average loss: 0.0019\n",
      "Epoch [42/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 42 completed with average loss: 0.0019\n",
      "Epoch [43/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 43 completed with average loss: 0.0019\n",
      "Epoch [44/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 44 completed with average loss: 0.0019\n",
      "Epoch [45/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 45 completed with average loss: 0.0019\n",
      "Epoch [46/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 46 completed with average loss: 0.0019\n",
      "Epoch [47/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 47 completed with average loss: 0.0019\n",
      "Epoch [48/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 48 completed with average loss: 0.0019\n",
      "Epoch [49/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 49 completed with average loss: 0.0019\n",
      "Epoch [50/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 50 completed with average loss: 0.0019\n",
      "Epoch [51/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 51 completed with average loss: 0.0019\n",
      "Epoch [52/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 52 completed with average loss: 0.0019\n",
      "Epoch [53/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 53 completed with average loss: 0.0018\n",
      "Epoch [54/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 54 completed with average loss: 0.0018\n",
      "Epoch [55/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 55 completed with average loss: 0.0018\n",
      "Epoch [56/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 56 completed with average loss: 0.0018\n",
      "Epoch [57/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 57 completed with average loss: 0.0018\n",
      "Epoch [58/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 58 completed with average loss: 0.0018\n",
      "Epoch [59/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 59 completed with average loss: 0.0018\n",
      "Epoch [60/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 60 completed with average loss: 0.0018\n",
      "Epoch [61/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 61 completed with average loss: 0.0018\n",
      "Epoch [62/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 62 completed with average loss: 0.0018\n",
      "Epoch [63/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 63 completed with average loss: 0.0018\n",
      "Epoch [64/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 64 completed with average loss: 0.0018\n",
      "Epoch [65/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 65 completed with average loss: 0.0018\n",
      "Epoch [66/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 66 completed with average loss: 0.0018\n",
      "Epoch [67/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 67 completed with average loss: 0.0018\n",
      "Epoch [68/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 68 completed with average loss: 0.0017\n",
      "Epoch [69/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 69 completed with average loss: 0.0017\n",
      "Epoch [70/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 70 completed with average loss: 0.0017\n",
      "Epoch [71/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 71 completed with average loss: 0.0017\n",
      "Epoch [72/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 72 completed with average loss: 0.0017\n",
      "Epoch [73/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 73 completed with average loss: 0.0017\n",
      "Epoch [74/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 74 completed with average loss: 0.0017\n",
      "Epoch [75/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 75 completed with average loss: 0.0017\n",
      "Epoch [76/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 76 completed with average loss: 0.0017\n",
      "Epoch [77/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 77 completed with average loss: 0.0017\n",
      "Epoch [78/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 78 completed with average loss: 0.0017\n",
      "Epoch [79/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 79 completed with average loss: 0.0017\n",
      "Epoch [80/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 80 completed with average loss: 0.0017\n",
      "Epoch [81/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 81 completed with average loss: 0.0017\n",
      "Epoch [82/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 82 completed with average loss: 0.0017\n",
      "Epoch [83/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 83 completed with average loss: 0.0017\n",
      "Epoch [84/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 84 completed with average loss: 0.0017\n",
      "Epoch [85/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 85 completed with average loss: 0.0017\n",
      "Epoch [86/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 86 completed with average loss: 0.0016\n",
      "Epoch [87/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 87 completed with average loss: 0.0016\n",
      "Epoch [88/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 88 completed with average loss: 0.0016\n",
      "Epoch [89/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 89 completed with average loss: 0.0016\n",
      "Epoch [90/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 90 completed with average loss: 0.0016\n",
      "Epoch [91/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 91 completed with average loss: 0.0016\n",
      "Epoch [92/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 92 completed with average loss: 0.0016\n",
      "Epoch [93/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 93 completed with average loss: 0.0016\n",
      "Epoch [94/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 94 completed with average loss: 0.0016\n",
      "Epoch [95/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 95 completed with average loss: 0.0016\n",
      "Epoch [96/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 96 completed with average loss: 0.0016\n",
      "Epoch [97/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 97 completed with average loss: 0.0016\n",
      "Epoch [98/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 98 completed with average loss: 0.0016\n",
      "Epoch [99/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 99 completed with average loss: 0.0016\n",
      "Epoch [100/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 100 completed with average loss: 0.0016\n",
      "Epoch [101/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 101 completed with average loss: 0.0016\n",
      "Epoch [102/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 102 completed with average loss: 0.0016\n",
      "Epoch [103/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 103 completed with average loss: 0.0016\n",
      "Epoch [104/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 104 completed with average loss: 0.0016\n",
      "Epoch [105/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 105 completed with average loss: 0.0016\n",
      "Epoch [106/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 106 completed with average loss: 0.0015\n",
      "Epoch [107/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 107 completed with average loss: 0.0015\n",
      "Epoch [108/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 108 completed with average loss: 0.0015\n",
      "Epoch [109/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 109 completed with average loss: 0.0015\n",
      "Epoch [110/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 110 completed with average loss: 0.0015\n",
      "Epoch [111/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 111 completed with average loss: 0.0015\n",
      "Epoch [112/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 112 completed with average loss: 0.0015\n",
      "Epoch [113/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 113 completed with average loss: 0.0015\n",
      "Epoch [114/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 114 completed with average loss: 0.0015\n",
      "Epoch [115/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 115 completed with average loss: 0.0015\n",
      "Epoch [116/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 116 completed with average loss: 0.0015\n",
      "Epoch [117/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 117 completed with average loss: 0.0015\n",
      "Epoch [118/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 118 completed with average loss: 0.0015\n",
      "Epoch [119/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 119 completed with average loss: 0.0015\n",
      "Epoch [120/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 120 completed with average loss: 0.0015\n",
      "Epoch [121/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 121 completed with average loss: 0.0015\n",
      "Epoch [122/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 122 completed with average loss: 0.0015\n",
      "Epoch [123/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 123 completed with average loss: 0.0015\n",
      "Epoch [124/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 124 completed with average loss: 0.0015\n",
      "Epoch [125/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 125 completed with average loss: 0.0015\n",
      "Epoch [126/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 126 completed with average loss: 0.0015\n",
      "Epoch [127/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 127 completed with average loss: 0.0015\n",
      "Epoch [128/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 128 completed with average loss: 0.0015\n",
      "Epoch [129/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 129 completed with average loss: 0.0014\n",
      "Epoch [130/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 130 completed with average loss: 0.0014\n",
      "Epoch [131/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 131 completed with average loss: 0.0014\n",
      "Epoch [132/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 132 completed with average loss: 0.0014\n",
      "Epoch [133/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 133 completed with average loss: 0.0014\n",
      "Epoch [134/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 134 completed with average loss: 0.0014\n",
      "Epoch [135/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 135 completed with average loss: 0.0014\n",
      "Epoch [136/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 136 completed with average loss: 0.0014\n",
      "Epoch [137/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 137 completed with average loss: 0.0014\n",
      "Epoch [138/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 138 completed with average loss: 0.0014\n",
      "Epoch [139/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 139 completed with average loss: 0.0014\n",
      "Epoch [140/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 140 completed with average loss: 0.0014\n",
      "Epoch [141/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 141 completed with average loss: 0.0014\n",
      "Epoch [142/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 142 completed with average loss: 0.0014\n",
      "Epoch [143/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 143 completed with average loss: 0.0014\n",
      "Epoch [144/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 144 completed with average loss: 0.0014\n",
      "Epoch [145/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 145 completed with average loss: 0.0014\n",
      "Epoch [146/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 146 completed with average loss: 0.0014\n",
      "Epoch [147/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 147 completed with average loss: 0.0014\n",
      "Epoch [148/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 148 completed with average loss: 0.0014\n",
      "Epoch [149/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 149 completed with average loss: 0.0014\n",
      "Epoch [150/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 150 completed with average loss: 0.0014\n",
      "Epoch [151/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 151 completed with average loss: 0.0014\n",
      "Epoch [152/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 152 completed with average loss: 0.0014\n",
      "Epoch [153/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 153 completed with average loss: 0.0014\n",
      "Epoch [154/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 154 completed with average loss: 0.0014\n",
      "Epoch [155/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 155 completed with average loss: 0.0014\n",
      "Epoch [156/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 156 completed with average loss: 0.0014\n",
      "Epoch [157/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 157 completed with average loss: 0.0013\n",
      "Epoch [158/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 158 completed with average loss: 0.0013\n",
      "Epoch [159/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 159 completed with average loss: 0.0013\n",
      "Epoch [160/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 160 completed with average loss: 0.0013\n",
      "Epoch [161/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 161 completed with average loss: 0.0013\n",
      "Epoch [162/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 162 completed with average loss: 0.0013\n",
      "Epoch [163/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 163 completed with average loss: 0.0013\n",
      "Epoch [164/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 164 completed with average loss: 0.0013\n",
      "Epoch [165/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 165 completed with average loss: 0.0013\n",
      "Epoch [166/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 166 completed with average loss: 0.0013\n",
      "Epoch [167/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 167 completed with average loss: 0.0013\n",
      "Epoch [168/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 168 completed with average loss: 0.0013\n",
      "Epoch [169/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 169 completed with average loss: 0.0013\n",
      "Epoch [170/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 170 completed with average loss: 0.0013\n",
      "Epoch [171/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 171 completed with average loss: 0.0013\n",
      "Epoch [172/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 172 completed with average loss: 0.0013\n",
      "Epoch [173/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 173 completed with average loss: 0.0013\n",
      "Epoch [174/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 174 completed with average loss: 0.0013\n",
      "Epoch [175/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 175 completed with average loss: 0.0013\n",
      "Epoch [176/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 176 completed with average loss: 0.0013\n",
      "Epoch [177/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 177 completed with average loss: 0.0013\n",
      "Epoch [178/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 178 completed with average loss: 0.0013\n",
      "Epoch [179/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 179 completed with average loss: 0.0013\n",
      "Epoch [180/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 180 completed with average loss: 0.0013\n",
      "Epoch [181/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 181 completed with average loss: 0.0013\n",
      "Epoch [182/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 182 completed with average loss: 0.0013\n",
      "Epoch [183/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 183 completed with average loss: 0.0013\n",
      "Epoch [184/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 184 completed with average loss: 0.0013\n",
      "Epoch [185/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 185 completed with average loss: 0.0013\n",
      "Epoch [186/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 186 completed with average loss: 0.0012\n",
      "Epoch [187/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 187 completed with average loss: 0.0012\n",
      "Epoch [188/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 188 completed with average loss: 0.0012\n",
      "Epoch [189/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 189 completed with average loss: 0.0012\n",
      "Epoch [190/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 190 completed with average loss: 0.0012\n",
      "Epoch [191/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 191 completed with average loss: 0.0012\n",
      "Epoch [192/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 192 completed with average loss: 0.0012\n",
      "Epoch [193/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 193 completed with average loss: 0.0012\n",
      "Epoch [194/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 194 completed with average loss: 0.0012\n",
      "Epoch [195/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 195 completed with average loss: 0.0012\n",
      "Epoch [196/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 196 completed with average loss: 0.0012\n",
      "Epoch [197/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 197 completed with average loss: 0.0012\n",
      "Epoch [198/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 198 completed with average loss: 0.0012\n",
      "Epoch [199/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 199 completed with average loss: 0.0012\n",
      "Epoch [200/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 200 completed with average loss: 0.0012\n",
      "Epoch [201/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 201 completed with average loss: 0.0012\n",
      "Epoch [202/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 202 completed with average loss: 0.0012\n",
      "Epoch [203/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 203 completed with average loss: 0.0012\n",
      "Epoch [204/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 204 completed with average loss: 0.0012\n",
      "Epoch [205/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 205 completed with average loss: 0.0012\n",
      "Epoch [206/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 206 completed with average loss: 0.0012\n",
      "Epoch [207/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 207 completed with average loss: 0.0012\n",
      "Epoch [208/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 208 completed with average loss: 0.0012\n",
      "Epoch [209/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 209 completed with average loss: 0.0012\n",
      "Epoch [210/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 210 completed with average loss: 0.0012\n",
      "Epoch [211/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 211 completed with average loss: 0.0012\n",
      "Epoch [212/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 212 completed with average loss: 0.0012\n",
      "Epoch [213/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 213 completed with average loss: 0.0012\n",
      "Epoch [214/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 214 completed with average loss: 0.0012\n",
      "Epoch [215/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 215 completed with average loss: 0.0012\n",
      "Epoch [216/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 216 completed with average loss: 0.0012\n",
      "Epoch [217/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 217 completed with average loss: 0.0012\n",
      "Epoch [218/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 218 completed with average loss: 0.0011\n",
      "Epoch [219/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 219 completed with average loss: 0.0011\n",
      "Epoch [220/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 220 completed with average loss: 0.0011\n",
      "Epoch [221/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 221 completed with average loss: 0.0011\n",
      "Epoch [222/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 222 completed with average loss: 0.0011\n",
      "Epoch [223/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 223 completed with average loss: 0.0011\n",
      "Epoch [224/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 224 completed with average loss: 0.0011\n",
      "Epoch [225/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 225 completed with average loss: 0.0011\n",
      "Epoch [226/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 226 completed with average loss: 0.0011\n",
      "Epoch [227/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 227 completed with average loss: 0.0011\n",
      "Epoch [228/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 228 completed with average loss: 0.0011\n",
      "Epoch [229/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 229 completed with average loss: 0.0011\n",
      "Epoch [230/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 230 completed with average loss: 0.0011\n",
      "Epoch [231/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 231 completed with average loss: 0.0011\n",
      "Epoch [232/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 232 completed with average loss: 0.0011\n",
      "Epoch [233/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 233 completed with average loss: 0.0011\n",
      "Epoch [234/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 234 completed with average loss: 0.0011\n",
      "Epoch [235/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 235 completed with average loss: 0.0011\n",
      "Epoch [236/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 236 completed with average loss: 0.0011\n",
      "Epoch [237/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 237 completed with average loss: 0.0011\n",
      "Epoch [238/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 238 completed with average loss: 0.0011\n",
      "Epoch [239/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 239 completed with average loss: 0.0011\n",
      "Epoch [240/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 240 completed with average loss: 0.0011\n",
      "Epoch [241/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 241 completed with average loss: 0.0011\n",
      "Epoch [242/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 242 completed with average loss: 0.0011\n",
      "Epoch [243/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 243 completed with average loss: 0.0011\n",
      "Epoch [244/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 244 completed with average loss: 0.0011\n",
      "Epoch [245/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 245 completed with average loss: 0.0011\n",
      "Epoch [246/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 246 completed with average loss: 0.0011\n",
      "Epoch [247/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 247 completed with average loss: 0.0011\n",
      "Epoch [248/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 248 completed with average loss: 0.0011\n",
      "Epoch [249/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 249 completed with average loss: 0.0011\n",
      "Epoch [250/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 250 completed with average loss: 0.0011\n",
      "Epoch [251/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 251 completed with average loss: 0.0011\n",
      "Epoch [252/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 252 completed with average loss: 0.0011\n",
      "Epoch [253/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 253 completed with average loss: 0.0011\n",
      "Epoch [254/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 254 completed with average loss: 0.0010\n",
      "Epoch [255/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 255 completed with average loss: 0.0010\n",
      "Epoch [256/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 256 completed with average loss: 0.0010\n",
      "Epoch [257/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 257 completed with average loss: 0.0010\n",
      "Epoch [258/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 258 completed with average loss: 0.0010\n",
      "Epoch [259/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 259 completed with average loss: 0.0010\n",
      "Epoch [260/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 260 completed with average loss: 0.0010\n",
      "Epoch [261/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 261 completed with average loss: 0.0010\n",
      "Epoch [262/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 262 completed with average loss: 0.0010\n",
      "Epoch [263/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 263 completed with average loss: 0.0010\n",
      "Epoch [264/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 264 completed with average loss: 0.0010\n",
      "Epoch [265/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 265 completed with average loss: 0.0010\n",
      "Epoch [266/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 266 completed with average loss: 0.0010\n",
      "Epoch [267/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 267 completed with average loss: 0.0010\n",
      "Epoch [268/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 268 completed with average loss: 0.0010\n",
      "Epoch [269/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 269 completed with average loss: 0.0010\n",
      "Epoch [270/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 270 completed with average loss: 0.0010\n",
      "Epoch [271/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 271 completed with average loss: 0.0010\n",
      "Epoch [272/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 272 completed with average loss: 0.0010\n",
      "Epoch [273/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 273 completed with average loss: 0.0010\n",
      "Epoch [274/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 274 completed with average loss: 0.0010\n",
      "Epoch [275/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 275 completed with average loss: 0.0010\n",
      "Epoch [276/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 276 completed with average loss: 0.0010\n",
      "Epoch [277/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 277 completed with average loss: 0.0010\n",
      "Epoch [278/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 278 completed with average loss: 0.0010\n",
      "Epoch [279/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 279 completed with average loss: 0.0010\n",
      "Epoch [280/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 280 completed with average loss: 0.0010\n",
      "Epoch [281/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 281 completed with average loss: 0.0010\n",
      "Epoch [282/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 282 completed with average loss: 0.0010\n",
      "Epoch [283/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 283 completed with average loss: 0.0010\n",
      "Epoch [284/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 284 completed with average loss: 0.0010\n",
      "Epoch [285/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 285 completed with average loss: 0.0010\n",
      "Epoch [286/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 286 completed with average loss: 0.0010\n",
      "Epoch [287/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 287 completed with average loss: 0.0010\n",
      "Epoch [288/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 288 completed with average loss: 0.0010\n",
      "Epoch [289/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 289 completed with average loss: 0.0010\n",
      "Epoch [290/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 290 completed with average loss: 0.0010\n",
      "Epoch [291/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 291 completed with average loss: 0.0010\n",
      "Epoch [292/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 292 completed with average loss: 0.0010\n",
      "Epoch [293/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 293 completed with average loss: 0.0010\n",
      "Epoch [294/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 294 completed with average loss: 0.0009\n",
      "Epoch [295/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 295 completed with average loss: 0.0009\n",
      "Epoch [296/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 296 completed with average loss: 0.0009\n",
      "Epoch [297/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 297 completed with average loss: 0.0009\n",
      "Epoch [298/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 298 completed with average loss: 0.0009\n",
      "Epoch [299/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 299 completed with average loss: 0.0009\n",
      "Epoch [300/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 300 completed with average loss: 0.0009\n",
      "Epoch [301/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 301 completed with average loss: 0.0009\n",
      "Epoch [302/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 302 completed with average loss: 0.0009\n",
      "Epoch [303/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 303 completed with average loss: 0.0009\n",
      "Epoch [304/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 304 completed with average loss: 0.0009\n",
      "Epoch [305/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 305 completed with average loss: 0.0009\n",
      "Epoch [306/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 306 completed with average loss: 0.0009\n",
      "Epoch [307/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 307 completed with average loss: 0.0009\n",
      "Epoch [308/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 308 completed with average loss: 0.0009\n",
      "Epoch [309/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 309 completed with average loss: 0.0009\n",
      "Epoch [310/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 310 completed with average loss: 0.0009\n",
      "Epoch [311/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 311 completed with average loss: 0.0009\n",
      "Epoch [312/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 312 completed with average loss: 0.0009\n",
      "Epoch [313/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 313 completed with average loss: 0.0009\n",
      "Epoch [314/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 314 completed with average loss: 0.0009\n",
      "Epoch [315/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 315 completed with average loss: 0.0009\n",
      "Epoch [316/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 316 completed with average loss: 0.0009\n",
      "Epoch [317/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 317 completed with average loss: 0.0009\n",
      "Epoch [318/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 318 completed with average loss: 0.0009\n",
      "Epoch [319/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 319 completed with average loss: 0.0009\n",
      "Epoch [320/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 320 completed with average loss: 0.0009\n",
      "Epoch [321/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 321 completed with average loss: 0.0009\n",
      "Epoch [322/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 322 completed with average loss: 0.0009\n",
      "Epoch [323/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 323 completed with average loss: 0.0009\n",
      "Epoch [324/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 324 completed with average loss: 0.0009\n",
      "Epoch [325/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 325 completed with average loss: 0.0009\n",
      "Epoch [326/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 326 completed with average loss: 0.0009\n",
      "Epoch [327/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 327 completed with average loss: 0.0009\n",
      "Epoch [328/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 328 completed with average loss: 0.0009\n",
      "Epoch [329/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 329 completed with average loss: 0.0009\n",
      "Epoch [330/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 330 completed with average loss: 0.0009\n",
      "Epoch [331/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 331 completed with average loss: 0.0009\n",
      "Epoch [332/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 332 completed with average loss: 0.0009\n",
      "Epoch [333/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 333 completed with average loss: 0.0009\n",
      "Epoch [334/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 334 completed with average loss: 0.0009\n",
      "Epoch [335/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 335 completed with average loss: 0.0009\n",
      "Epoch [336/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 336 completed with average loss: 0.0009\n",
      "Epoch [337/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 337 completed with average loss: 0.0008\n",
      "Epoch [338/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 338 completed with average loss: 0.0008\n",
      "Epoch [339/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 339 completed with average loss: 0.0008\n",
      "Epoch [340/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 340 completed with average loss: 0.0008\n",
      "Epoch [341/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 341 completed with average loss: 0.0008\n",
      "Epoch [342/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 342 completed with average loss: 0.0008\n",
      "Epoch [343/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 343 completed with average loss: 0.0008\n",
      "Epoch [344/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 344 completed with average loss: 0.0008\n",
      "Epoch [345/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 345 completed with average loss: 0.0008\n",
      "Epoch [346/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 346 completed with average loss: 0.0008\n",
      "Epoch [347/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 347 completed with average loss: 0.0008\n",
      "Epoch [348/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 348 completed with average loss: 0.0008\n",
      "Epoch [349/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 349 completed with average loss: 0.0008\n",
      "Epoch [350/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 350 completed with average loss: 0.0008\n",
      "Epoch [351/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 351 completed with average loss: 0.0008\n",
      "Epoch [352/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 352 completed with average loss: 0.0008\n",
      "Epoch [353/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 353 completed with average loss: 0.0008\n",
      "Epoch [354/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 354 completed with average loss: 0.0008\n",
      "Epoch [355/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 355 completed with average loss: 0.0008\n",
      "Epoch [356/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 356 completed with average loss: 0.0008\n",
      "Epoch [357/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 357 completed with average loss: 0.0008\n",
      "Epoch [358/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 358 completed with average loss: 0.0008\n",
      "Epoch [359/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 359 completed with average loss: 0.0008\n",
      "Epoch [360/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 360 completed with average loss: 0.0008\n",
      "Epoch [361/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 361 completed with average loss: 0.0008\n",
      "Epoch [362/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 362 completed with average loss: 0.0008\n",
      "Epoch [363/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 363 completed with average loss: 0.0008\n",
      "Epoch [364/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 364 completed with average loss: 0.0008\n",
      "Epoch [365/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 365 completed with average loss: 0.0008\n",
      "Epoch [366/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 366 completed with average loss: 0.0008\n",
      "Epoch [367/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 367 completed with average loss: 0.0008\n",
      "Epoch [368/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 368 completed with average loss: 0.0008\n",
      "Epoch [369/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 369 completed with average loss: 0.0008\n",
      "Epoch [370/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 370 completed with average loss: 0.0008\n",
      "Epoch [371/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 371 completed with average loss: 0.0008\n",
      "Epoch [372/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 372 completed with average loss: 0.0008\n",
      "Epoch [373/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 373 completed with average loss: 0.0008\n",
      "Epoch [374/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 374 completed with average loss: 0.0008\n",
      "Epoch [375/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 375 completed with average loss: 0.0008\n",
      "Epoch [376/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 376 completed with average loss: 0.0008\n",
      "Epoch [377/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 377 completed with average loss: 0.0008\n",
      "Epoch [378/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 378 completed with average loss: 0.0008\n",
      "Epoch [379/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 379 completed with average loss: 0.0008\n",
      "Epoch [380/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 380 completed with average loss: 0.0008\n",
      "Epoch [381/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 381 completed with average loss: 0.0008\n",
      "Epoch [382/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 382 completed with average loss: 0.0008\n",
      "Epoch [383/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 383 completed with average loss: 0.0008\n",
      "Epoch [384/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 384 completed with average loss: 0.0008\n",
      "Epoch [385/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 385 completed with average loss: 0.0008\n",
      "Epoch [386/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 386 completed with average loss: 0.0007\n",
      "Epoch [387/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 387 completed with average loss: 0.0007\n",
      "Epoch [388/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 388 completed with average loss: 0.0007\n",
      "Epoch [389/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 389 completed with average loss: 0.0007\n",
      "Epoch [390/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 390 completed with average loss: 0.0007\n",
      "Epoch [391/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 391 completed with average loss: 0.0007\n",
      "Epoch [392/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 392 completed with average loss: 0.0007\n",
      "Epoch [393/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 393 completed with average loss: 0.0007\n",
      "Epoch [394/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 394 completed with average loss: 0.0007\n",
      "Epoch [395/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 395 completed with average loss: 0.0007\n",
      "Epoch [396/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 396 completed with average loss: 0.0007\n",
      "Epoch [397/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 397 completed with average loss: 0.0007\n",
      "Epoch [398/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 398 completed with average loss: 0.0007\n",
      "Epoch [399/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 399 completed with average loss: 0.0007\n",
      "Epoch [400/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 400 completed with average loss: 0.0007\n",
      "Epoch [401/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 401 completed with average loss: 0.0007\n",
      "Epoch [402/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 402 completed with average loss: 0.0007\n",
      "Epoch [403/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 403 completed with average loss: 0.0007\n",
      "Epoch [404/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 404 completed with average loss: 0.0007\n",
      "Epoch [405/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 405 completed with average loss: 0.0007\n",
      "Epoch [406/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 406 completed with average loss: 0.0007\n",
      "Epoch [407/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 407 completed with average loss: 0.0007\n",
      "Epoch [408/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 408 completed with average loss: 0.0007\n",
      "Epoch [409/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 409 completed with average loss: 0.0007\n",
      "Epoch [410/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 410 completed with average loss: 0.0007\n",
      "Epoch [411/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 411 completed with average loss: 0.0007\n",
      "Epoch [412/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 412 completed with average loss: 0.0007\n",
      "Epoch [413/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 413 completed with average loss: 0.0007\n",
      "Epoch [414/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 414 completed with average loss: 0.0007\n",
      "Epoch [415/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 415 completed with average loss: 0.0007\n",
      "Epoch [416/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 416 completed with average loss: 0.0007\n",
      "Epoch [417/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 417 completed with average loss: 0.0007\n",
      "Epoch [418/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 418 completed with average loss: 0.0007\n",
      "Epoch [419/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 419 completed with average loss: 0.0007\n",
      "Epoch [420/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 420 completed with average loss: 0.0007\n",
      "Epoch [421/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 421 completed with average loss: 0.0007\n",
      "Epoch [422/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 422 completed with average loss: 0.0007\n",
      "Epoch [423/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 423 completed with average loss: 0.0007\n",
      "Epoch [424/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 424 completed with average loss: 0.0007\n",
      "Epoch [425/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 425 completed with average loss: 0.0007\n",
      "Epoch [426/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 426 completed with average loss: 0.0007\n",
      "Epoch [427/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 427 completed with average loss: 0.0007\n",
      "Epoch [428/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 428 completed with average loss: 0.0007\n",
      "Epoch [429/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 429 completed with average loss: 0.0007\n",
      "Epoch [430/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 430 completed with average loss: 0.0007\n",
      "Epoch [431/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 431 completed with average loss: 0.0007\n",
      "Epoch [432/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 432 completed with average loss: 0.0007\n",
      "Epoch [433/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 433 completed with average loss: 0.0007\n",
      "Epoch [434/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 434 completed with average loss: 0.0007\n",
      "Epoch [435/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 435 completed with average loss: 0.0007\n",
      "Epoch [436/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 436 completed with average loss: 0.0007\n",
      "Epoch [437/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 437 completed with average loss: 0.0007\n",
      "Epoch [438/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 438 completed with average loss: 0.0007\n",
      "Epoch [439/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 439 completed with average loss: 0.0007\n",
      "Epoch [440/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 440 completed with average loss: 0.0007\n",
      "Epoch [441/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 441 completed with average loss: 0.0007\n",
      "Epoch [442/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 442 completed with average loss: 0.0006\n",
      "Epoch [443/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 443 completed with average loss: 0.0006\n",
      "Epoch [444/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 444 completed with average loss: 0.0006\n",
      "Epoch [445/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 445 completed with average loss: 0.0006\n",
      "Epoch [446/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 446 completed with average loss: 0.0006\n",
      "Epoch [447/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 447 completed with average loss: 0.0006\n",
      "Epoch [448/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 448 completed with average loss: 0.0006\n",
      "Epoch [449/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 449 completed with average loss: 0.0006\n",
      "Epoch [450/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 450 completed with average loss: 0.0006\n",
      "Epoch [451/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 451 completed with average loss: 0.0006\n",
      "Epoch [452/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 452 completed with average loss: 0.0006\n",
      "Epoch [453/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 453 completed with average loss: 0.0006\n",
      "Epoch [454/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 454 completed with average loss: 0.0006\n",
      "Epoch [455/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 455 completed with average loss: 0.0006\n",
      "Epoch [456/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 456 completed with average loss: 0.0006\n",
      "Epoch [457/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 457 completed with average loss: 0.0006\n",
      "Epoch [458/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 458 completed with average loss: 0.0006\n",
      "Epoch [459/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 459 completed with average loss: 0.0006\n",
      "Epoch [460/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 460 completed with average loss: 0.0006\n",
      "Epoch [461/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 461 completed with average loss: 0.0006\n",
      "Epoch [462/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 462 completed with average loss: 0.0006\n",
      "Epoch [463/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 463 completed with average loss: 0.0006\n",
      "Epoch [464/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 464 completed with average loss: 0.0006\n",
      "Epoch [465/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 465 completed with average loss: 0.0006\n",
      "Epoch [466/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 466 completed with average loss: 0.0006\n",
      "Epoch [467/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 467 completed with average loss: 0.0006\n",
      "Epoch [468/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 468 completed with average loss: 0.0006\n",
      "Epoch [469/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 469 completed with average loss: 0.0006\n",
      "Epoch [470/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 470 completed with average loss: 0.0006\n",
      "Epoch [471/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 471 completed with average loss: 0.0006\n",
      "Epoch [472/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 472 completed with average loss: 0.0006\n",
      "Epoch [473/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 473 completed with average loss: 0.0006\n",
      "Epoch [474/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 474 completed with average loss: 0.0006\n",
      "Epoch [475/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 475 completed with average loss: 0.0006\n",
      "Epoch [476/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 476 completed with average loss: 0.0006\n",
      "Epoch [477/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 477 completed with average loss: 0.0006\n",
      "Epoch [478/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 478 completed with average loss: 0.0006\n",
      "Epoch [479/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 479 completed with average loss: 0.0006\n",
      "Epoch [480/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 480 completed with average loss: 0.0006\n",
      "Epoch [481/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 481 completed with average loss: 0.0006\n",
      "Epoch [482/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 482 completed with average loss: 0.0006\n",
      "Epoch [483/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 483 completed with average loss: 0.0006\n",
      "Epoch [484/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 484 completed with average loss: 0.0006\n",
      "Epoch [485/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 485 completed with average loss: 0.0006\n",
      "Epoch [486/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 486 completed with average loss: 0.0006\n",
      "Epoch [487/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 487 completed with average loss: 0.0006\n",
      "Epoch [488/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 488 completed with average loss: 0.0006\n",
      "Epoch [489/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 489 completed with average loss: 0.0006\n",
      "Epoch [490/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 490 completed with average loss: 0.0006\n",
      "Epoch [491/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 491 completed with average loss: 0.0006\n",
      "Epoch [492/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 492 completed with average loss: 0.0006\n",
      "Epoch [493/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 493 completed with average loss: 0.0006\n",
      "Epoch [494/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 494 completed with average loss: 0.0006\n",
      "Epoch [495/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 495 completed with average loss: 0.0006\n",
      "Epoch [496/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 496 completed with average loss: 0.0006\n",
      "Epoch [497/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 497 completed with average loss: 0.0006\n",
      "Epoch [498/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 498 completed with average loss: 0.0006\n",
      "Epoch [499/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 499 completed with average loss: 0.0006\n",
      "Epoch [500/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 500 completed with average loss: 0.0006\n",
      "Epoch [501/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 501 completed with average loss: 0.0006\n",
      "Epoch [502/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 502 completed with average loss: 0.0006\n",
      "Epoch [503/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 503 completed with average loss: 0.0006\n",
      "Epoch [504/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 504 completed with average loss: 0.0006\n",
      "Epoch [505/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 505 completed with average loss: 0.0006\n",
      "Epoch [506/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 506 completed with average loss: 0.0005\n",
      "Epoch [507/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 507 completed with average loss: 0.0005\n",
      "Epoch [508/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 508 completed with average loss: 0.0005\n",
      "Epoch [509/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 509 completed with average loss: 0.0005\n",
      "Epoch [510/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 510 completed with average loss: 0.0005\n",
      "Epoch [511/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 511 completed with average loss: 0.0005\n",
      "Epoch [512/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 512 completed with average loss: 0.0005\n",
      "Epoch [513/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 513 completed with average loss: 0.0005\n",
      "Epoch [514/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 514 completed with average loss: 0.0005\n",
      "Epoch [515/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 515 completed with average loss: 0.0005\n",
      "Epoch [516/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 516 completed with average loss: 0.0005\n",
      "Epoch [517/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 517 completed with average loss: 0.0005\n",
      "Epoch [518/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 518 completed with average loss: 0.0005\n",
      "Epoch [519/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 519 completed with average loss: 0.0005\n",
      "Epoch [520/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 520 completed with average loss: 0.0005\n",
      "Epoch [521/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 521 completed with average loss: 0.0005\n",
      "Epoch [522/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 522 completed with average loss: 0.0005\n",
      "Epoch [523/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 523 completed with average loss: 0.0005\n",
      "Epoch [524/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 524 completed with average loss: 0.0005\n",
      "Epoch [525/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 525 completed with average loss: 0.0005\n",
      "Epoch [526/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 526 completed with average loss: 0.0005\n",
      "Epoch [527/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 527 completed with average loss: 0.0005\n",
      "Epoch [528/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 528 completed with average loss: 0.0005\n",
      "Epoch [529/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 529 completed with average loss: 0.0005\n",
      "Epoch [530/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 530 completed with average loss: 0.0005\n",
      "Epoch [531/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 531 completed with average loss: 0.0005\n",
      "Epoch [532/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 532 completed with average loss: 0.0005\n",
      "Epoch [533/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 533 completed with average loss: 0.0005\n",
      "Epoch [534/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 534 completed with average loss: 0.0005\n",
      "Epoch [535/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 535 completed with average loss: 0.0005\n",
      "Epoch [536/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 536 completed with average loss: 0.0005\n",
      "Epoch [537/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 537 completed with average loss: 0.0005\n",
      "Epoch [538/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 538 completed with average loss: 0.0005\n",
      "Epoch [539/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 539 completed with average loss: 0.0005\n",
      "Epoch [540/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 540 completed with average loss: 0.0005\n",
      "Epoch [541/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 541 completed with average loss: 0.0005\n",
      "Epoch [542/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 542 completed with average loss: 0.0005\n",
      "Epoch [543/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 543 completed with average loss: 0.0005\n",
      "Epoch [544/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 544 completed with average loss: 0.0005\n",
      "Epoch [545/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 545 completed with average loss: 0.0005\n",
      "Epoch [546/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 546 completed with average loss: 0.0005\n",
      "Epoch [547/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 547 completed with average loss: 0.0005\n",
      "Epoch [548/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 548 completed with average loss: 0.0005\n",
      "Epoch [549/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 549 completed with average loss: 0.0005\n",
      "Epoch [550/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 550 completed with average loss: 0.0005\n",
      "Epoch [551/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 551 completed with average loss: 0.0005\n",
      "Epoch [552/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 552 completed with average loss: 0.0005\n",
      "Epoch [553/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 553 completed with average loss: 0.0005\n",
      "Epoch [554/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 554 completed with average loss: 0.0005\n",
      "Epoch [555/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 555 completed with average loss: 0.0005\n",
      "Epoch [556/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 556 completed with average loss: 0.0005\n",
      "Epoch [557/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 557 completed with average loss: 0.0005\n",
      "Epoch [558/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 558 completed with average loss: 0.0005\n",
      "Epoch [559/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 559 completed with average loss: 0.0005\n",
      "Epoch [560/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 560 completed with average loss: 0.0005\n",
      "Epoch [561/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 561 completed with average loss: 0.0005\n",
      "Epoch [562/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 562 completed with average loss: 0.0005\n",
      "Epoch [563/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 563 completed with average loss: 0.0005\n",
      "Epoch [564/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 564 completed with average loss: 0.0005\n",
      "Epoch [565/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 565 completed with average loss: 0.0005\n",
      "Epoch [566/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 566 completed with average loss: 0.0005\n",
      "Epoch [567/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 567 completed with average loss: 0.0005\n",
      "Epoch [568/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 568 completed with average loss: 0.0005\n",
      "Epoch [569/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 569 completed with average loss: 0.0005\n",
      "Epoch [570/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 570 completed with average loss: 0.0005\n",
      "Epoch [571/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 571 completed with average loss: 0.0005\n",
      "Epoch [572/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 572 completed with average loss: 0.0005\n",
      "Epoch [573/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 573 completed with average loss: 0.0005\n",
      "Epoch [574/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 574 completed with average loss: 0.0005\n",
      "Epoch [575/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 575 completed with average loss: 0.0005\n",
      "Epoch [576/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 576 completed with average loss: 0.0005\n",
      "Epoch [577/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 577 completed with average loss: 0.0005\n",
      "Epoch [578/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 578 completed with average loss: 0.0005\n",
      "Epoch [579/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 579 completed with average loss: 0.0005\n",
      "Epoch [580/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 580 completed with average loss: 0.0005\n",
      "Epoch [581/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 581 completed with average loss: 0.0005\n",
      "Epoch [582/1000], Step [0/1], Loss: 0.0005\n",
      "Epoch 582 completed with average loss: 0.0005\n",
      "Epoch [583/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 583 completed with average loss: 0.0004\n",
      "Epoch [584/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 584 completed with average loss: 0.0004\n",
      "Epoch [585/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 585 completed with average loss: 0.0004\n",
      "Epoch [586/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 586 completed with average loss: 0.0004\n",
      "Epoch [587/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 587 completed with average loss: 0.0004\n",
      "Epoch [588/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 588 completed with average loss: 0.0004\n",
      "Epoch [589/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 589 completed with average loss: 0.0004\n",
      "Epoch [590/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 590 completed with average loss: 0.0004\n",
      "Epoch [591/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 591 completed with average loss: 0.0004\n",
      "Epoch [592/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 592 completed with average loss: 0.0004\n",
      "Epoch [593/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 593 completed with average loss: 0.0004\n",
      "Epoch [594/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 594 completed with average loss: 0.0004\n",
      "Epoch [595/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 595 completed with average loss: 0.0004\n",
      "Epoch [596/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 596 completed with average loss: 0.0004\n",
      "Epoch [597/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 597 completed with average loss: 0.0004\n",
      "Epoch [598/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 598 completed with average loss: 0.0004\n",
      "Epoch [599/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 599 completed with average loss: 0.0004\n",
      "Epoch [600/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 600 completed with average loss: 0.0004\n",
      "Epoch [601/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 601 completed with average loss: 0.0004\n",
      "Epoch [602/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 602 completed with average loss: 0.0004\n",
      "Epoch [603/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 603 completed with average loss: 0.0004\n",
      "Epoch [604/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 604 completed with average loss: 0.0004\n",
      "Epoch [605/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 605 completed with average loss: 0.0004\n",
      "Epoch [606/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 606 completed with average loss: 0.0004\n",
      "Epoch [607/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 607 completed with average loss: 0.0004\n",
      "Epoch [608/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 608 completed with average loss: 0.0004\n",
      "Epoch [609/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 609 completed with average loss: 0.0004\n",
      "Epoch [610/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 610 completed with average loss: 0.0004\n",
      "Epoch [611/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 611 completed with average loss: 0.0004\n",
      "Epoch [612/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 612 completed with average loss: 0.0004\n",
      "Epoch [613/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 613 completed with average loss: 0.0004\n",
      "Epoch [614/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 614 completed with average loss: 0.0004\n",
      "Epoch [615/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 615 completed with average loss: 0.0004\n",
      "Epoch [616/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 616 completed with average loss: 0.0004\n",
      "Epoch [617/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 617 completed with average loss: 0.0004\n",
      "Epoch [618/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 618 completed with average loss: 0.0004\n",
      "Epoch [619/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 619 completed with average loss: 0.0004\n",
      "Epoch [620/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 620 completed with average loss: 0.0004\n",
      "Epoch [621/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 621 completed with average loss: 0.0004\n",
      "Epoch [622/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 622 completed with average loss: 0.0004\n",
      "Epoch [623/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 623 completed with average loss: 0.0004\n",
      "Epoch [624/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 624 completed with average loss: 0.0004\n",
      "Epoch [625/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 625 completed with average loss: 0.0004\n",
      "Epoch [626/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 626 completed with average loss: 0.0004\n",
      "Epoch [627/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 627 completed with average loss: 0.0004\n",
      "Epoch [628/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 628 completed with average loss: 0.0004\n",
      "Epoch [629/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 629 completed with average loss: 0.0004\n",
      "Epoch [630/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 630 completed with average loss: 0.0004\n",
      "Epoch [631/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 631 completed with average loss: 0.0004\n",
      "Epoch [632/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 632 completed with average loss: 0.0004\n",
      "Epoch [633/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 633 completed with average loss: 0.0004\n",
      "Epoch [634/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 634 completed with average loss: 0.0004\n",
      "Epoch [635/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 635 completed with average loss: 0.0004\n",
      "Epoch [636/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 636 completed with average loss: 0.0004\n",
      "Epoch [637/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 637 completed with average loss: 0.0004\n",
      "Epoch [638/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 638 completed with average loss: 0.0004\n",
      "Epoch [639/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 639 completed with average loss: 0.0004\n",
      "Epoch [640/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 640 completed with average loss: 0.0004\n",
      "Epoch [641/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 641 completed with average loss: 0.0004\n",
      "Epoch [642/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 642 completed with average loss: 0.0004\n",
      "Epoch [643/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 643 completed with average loss: 0.0004\n",
      "Epoch [644/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 644 completed with average loss: 0.0004\n",
      "Epoch [645/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 645 completed with average loss: 0.0004\n",
      "Epoch [646/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 646 completed with average loss: 0.0004\n",
      "Epoch [647/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 647 completed with average loss: 0.0004\n",
      "Epoch [648/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 648 completed with average loss: 0.0004\n",
      "Epoch [649/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 649 completed with average loss: 0.0004\n",
      "Epoch [650/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 650 completed with average loss: 0.0004\n",
      "Epoch [651/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 651 completed with average loss: 0.0004\n",
      "Epoch [652/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 652 completed with average loss: 0.0004\n",
      "Epoch [653/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 653 completed with average loss: 0.0004\n",
      "Epoch [654/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 654 completed with average loss: 0.0004\n",
      "Epoch [655/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 655 completed with average loss: 0.0004\n",
      "Epoch [656/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 656 completed with average loss: 0.0004\n",
      "Epoch [657/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 657 completed with average loss: 0.0004\n",
      "Epoch [658/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 658 completed with average loss: 0.0004\n",
      "Epoch [659/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 659 completed with average loss: 0.0004\n",
      "Epoch [660/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 660 completed with average loss: 0.0004\n",
      "Epoch [661/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 661 completed with average loss: 0.0004\n",
      "Epoch [662/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 662 completed with average loss: 0.0004\n",
      "Epoch [663/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 663 completed with average loss: 0.0004\n",
      "Epoch [664/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 664 completed with average loss: 0.0004\n",
      "Epoch [665/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 665 completed with average loss: 0.0004\n",
      "Epoch [666/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 666 completed with average loss: 0.0004\n",
      "Epoch [667/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 667 completed with average loss: 0.0004\n",
      "Epoch [668/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 668 completed with average loss: 0.0004\n",
      "Epoch [669/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 669 completed with average loss: 0.0004\n",
      "Epoch [670/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 670 completed with average loss: 0.0004\n",
      "Epoch [671/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 671 completed with average loss: 0.0004\n",
      "Epoch [672/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 672 completed with average loss: 0.0004\n",
      "Epoch [673/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 673 completed with average loss: 0.0004\n",
      "Epoch [674/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 674 completed with average loss: 0.0004\n",
      "Epoch [675/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 675 completed with average loss: 0.0004\n",
      "Epoch [676/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 676 completed with average loss: 0.0004\n",
      "Epoch [677/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 677 completed with average loss: 0.0004\n",
      "Epoch [678/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 678 completed with average loss: 0.0004\n",
      "Epoch [679/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 679 completed with average loss: 0.0004\n",
      "Epoch [680/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 680 completed with average loss: 0.0004\n",
      "Epoch [681/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 681 completed with average loss: 0.0004\n",
      "Epoch [682/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 682 completed with average loss: 0.0004\n",
      "Epoch [683/1000], Step [0/1], Loss: 0.0004\n",
      "Epoch 683 completed with average loss: 0.0004\n",
      "Epoch [684/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 684 completed with average loss: 0.0003\n",
      "Epoch [685/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 685 completed with average loss: 0.0003\n",
      "Epoch [686/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 686 completed with average loss: 0.0003\n",
      "Epoch [687/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 687 completed with average loss: 0.0003\n",
      "Epoch [688/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 688 completed with average loss: 0.0003\n",
      "Epoch [689/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 689 completed with average loss: 0.0003\n",
      "Epoch [690/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 690 completed with average loss: 0.0003\n",
      "Epoch [691/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 691 completed with average loss: 0.0003\n",
      "Epoch [692/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 692 completed with average loss: 0.0003\n",
      "Epoch [693/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 693 completed with average loss: 0.0003\n",
      "Epoch [694/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 694 completed with average loss: 0.0003\n",
      "Epoch [695/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 695 completed with average loss: 0.0003\n",
      "Epoch [696/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 696 completed with average loss: 0.0003\n",
      "Epoch [697/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 697 completed with average loss: 0.0003\n",
      "Epoch [698/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 698 completed with average loss: 0.0003\n",
      "Epoch [699/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 699 completed with average loss: 0.0003\n",
      "Epoch [700/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 700 completed with average loss: 0.0003\n",
      "Epoch [701/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 701 completed with average loss: 0.0003\n",
      "Epoch [702/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 702 completed with average loss: 0.0003\n",
      "Epoch [703/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 703 completed with average loss: 0.0003\n",
      "Epoch [704/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 704 completed with average loss: 0.0003\n",
      "Epoch [705/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 705 completed with average loss: 0.0003\n",
      "Epoch [706/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 706 completed with average loss: 0.0003\n",
      "Epoch [707/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 707 completed with average loss: 0.0003\n",
      "Epoch [708/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 708 completed with average loss: 0.0003\n",
      "Epoch [709/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 709 completed with average loss: 0.0003\n",
      "Epoch [710/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 710 completed with average loss: 0.0003\n",
      "Epoch [711/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 711 completed with average loss: 0.0003\n",
      "Epoch [712/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 712 completed with average loss: 0.0003\n",
      "Epoch [713/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 713 completed with average loss: 0.0003\n",
      "Epoch [714/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 714 completed with average loss: 0.0003\n",
      "Epoch [715/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 715 completed with average loss: 0.0003\n",
      "Epoch [716/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 716 completed with average loss: 0.0003\n",
      "Epoch [717/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 717 completed with average loss: 0.0003\n",
      "Epoch [718/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 718 completed with average loss: 0.0003\n",
      "Epoch [719/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 719 completed with average loss: 0.0003\n",
      "Epoch [720/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 720 completed with average loss: 0.0003\n",
      "Epoch [721/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 721 completed with average loss: 0.0003\n",
      "Epoch [722/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 722 completed with average loss: 0.0003\n",
      "Epoch [723/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 723 completed with average loss: 0.0003\n",
      "Epoch [724/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 724 completed with average loss: 0.0003\n",
      "Epoch [725/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 725 completed with average loss: 0.0003\n",
      "Epoch [726/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 726 completed with average loss: 0.0003\n",
      "Epoch [727/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 727 completed with average loss: 0.0003\n",
      "Epoch [728/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 728 completed with average loss: 0.0003\n",
      "Epoch [729/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 729 completed with average loss: 0.0003\n",
      "Epoch [730/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 730 completed with average loss: 0.0003\n",
      "Epoch [731/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 731 completed with average loss: 0.0003\n",
      "Epoch [732/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 732 completed with average loss: 0.0003\n",
      "Epoch [733/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 733 completed with average loss: 0.0003\n",
      "Epoch [734/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 734 completed with average loss: 0.0003\n",
      "Epoch [735/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 735 completed with average loss: 0.0003\n",
      "Epoch [736/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 736 completed with average loss: 0.0003\n",
      "Epoch [737/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 737 completed with average loss: 0.0003\n",
      "Epoch [738/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 738 completed with average loss: 0.0003\n",
      "Epoch [739/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 739 completed with average loss: 0.0003\n",
      "Epoch [740/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 740 completed with average loss: 0.0003\n",
      "Epoch [741/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 741 completed with average loss: 0.0003\n",
      "Epoch [742/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 742 completed with average loss: 0.0003\n",
      "Epoch [743/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 743 completed with average loss: 0.0003\n",
      "Epoch [744/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 744 completed with average loss: 0.0003\n",
      "Epoch [745/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 745 completed with average loss: 0.0003\n",
      "Epoch [746/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 746 completed with average loss: 0.0003\n",
      "Epoch [747/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 747 completed with average loss: 0.0003\n",
      "Epoch [748/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 748 completed with average loss: 0.0003\n",
      "Epoch [749/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 749 completed with average loss: 0.0003\n",
      "Epoch [750/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 750 completed with average loss: 0.0003\n",
      "Epoch [751/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 751 completed with average loss: 0.0003\n",
      "Epoch [752/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 752 completed with average loss: 0.0003\n",
      "Epoch [753/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 753 completed with average loss: 0.0003\n",
      "Epoch [754/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 754 completed with average loss: 0.0003\n",
      "Epoch [755/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 755 completed with average loss: 0.0003\n",
      "Epoch [756/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 756 completed with average loss: 0.0003\n",
      "Epoch [757/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 757 completed with average loss: 0.0003\n",
      "Epoch [758/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 758 completed with average loss: 0.0003\n",
      "Epoch [759/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 759 completed with average loss: 0.0003\n",
      "Epoch [760/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 760 completed with average loss: 0.0003\n",
      "Epoch [761/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 761 completed with average loss: 0.0003\n",
      "Epoch [762/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 762 completed with average loss: 0.0003\n",
      "Epoch [763/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 763 completed with average loss: 0.0003\n",
      "Epoch [764/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 764 completed with average loss: 0.0003\n",
      "Epoch [765/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 765 completed with average loss: 0.0003\n",
      "Epoch [766/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 766 completed with average loss: 0.0003\n",
      "Epoch [767/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 767 completed with average loss: 0.0003\n",
      "Epoch [768/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 768 completed with average loss: 0.0003\n",
      "Epoch [769/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 769 completed with average loss: 0.0003\n",
      "Epoch [770/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 770 completed with average loss: 0.0003\n",
      "Epoch [771/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 771 completed with average loss: 0.0003\n",
      "Epoch [772/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 772 completed with average loss: 0.0003\n",
      "Epoch [773/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 773 completed with average loss: 0.0003\n",
      "Epoch [774/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 774 completed with average loss: 0.0003\n",
      "Epoch [775/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 775 completed with average loss: 0.0003\n",
      "Epoch [776/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 776 completed with average loss: 0.0003\n",
      "Epoch [777/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 777 completed with average loss: 0.0003\n",
      "Epoch [778/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 778 completed with average loss: 0.0003\n",
      "Epoch [779/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 779 completed with average loss: 0.0003\n",
      "Epoch [780/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 780 completed with average loss: 0.0003\n",
      "Epoch [781/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 781 completed with average loss: 0.0003\n",
      "Epoch [782/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 782 completed with average loss: 0.0003\n",
      "Epoch [783/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 783 completed with average loss: 0.0003\n",
      "Epoch [784/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 784 completed with average loss: 0.0003\n",
      "Epoch [785/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 785 completed with average loss: 0.0003\n",
      "Epoch [786/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 786 completed with average loss: 0.0003\n",
      "Epoch [787/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 787 completed with average loss: 0.0003\n",
      "Epoch [788/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 788 completed with average loss: 0.0003\n",
      "Epoch [789/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 789 completed with average loss: 0.0003\n",
      "Epoch [790/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 790 completed with average loss: 0.0003\n",
      "Epoch [791/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 791 completed with average loss: 0.0003\n",
      "Epoch [792/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 792 completed with average loss: 0.0003\n",
      "Epoch [793/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 793 completed with average loss: 0.0003\n",
      "Epoch [794/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 794 completed with average loss: 0.0003\n",
      "Epoch [795/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 795 completed with average loss: 0.0003\n",
      "Epoch [796/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 796 completed with average loss: 0.0003\n",
      "Epoch [797/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 797 completed with average loss: 0.0003\n",
      "Epoch [798/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 798 completed with average loss: 0.0003\n",
      "Epoch [799/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 799 completed with average loss: 0.0003\n",
      "Epoch [800/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 800 completed with average loss: 0.0003\n",
      "Epoch [801/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 801 completed with average loss: 0.0003\n",
      "Epoch [802/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 802 completed with average loss: 0.0003\n",
      "Epoch [803/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 803 completed with average loss: 0.0003\n",
      "Epoch [804/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 804 completed with average loss: 0.0003\n",
      "Epoch [805/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 805 completed with average loss: 0.0003\n",
      "Epoch [806/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 806 completed with average loss: 0.0003\n",
      "Epoch [807/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 807 completed with average loss: 0.0003\n",
      "Epoch [808/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 808 completed with average loss: 0.0003\n",
      "Epoch [809/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 809 completed with average loss: 0.0003\n",
      "Epoch [810/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 810 completed with average loss: 0.0003\n",
      "Epoch [811/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 811 completed with average loss: 0.0003\n",
      "Epoch [812/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 812 completed with average loss: 0.0003\n",
      "Epoch [813/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 813 completed with average loss: 0.0003\n",
      "Epoch [814/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 814 completed with average loss: 0.0003\n",
      "Epoch [815/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 815 completed with average loss: 0.0003\n",
      "Epoch [816/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 816 completed with average loss: 0.0003\n",
      "Epoch [817/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 817 completed with average loss: 0.0003\n",
      "Epoch [818/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 818 completed with average loss: 0.0003\n",
      "Epoch [819/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 819 completed with average loss: 0.0003\n",
      "Epoch [820/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 820 completed with average loss: 0.0003\n",
      "Epoch [821/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 821 completed with average loss: 0.0003\n",
      "Epoch [822/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 822 completed with average loss: 0.0003\n",
      "Epoch [823/1000], Step [0/1], Loss: 0.0003\n",
      "Epoch 823 completed with average loss: 0.0003\n",
      "Epoch [824/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 824 completed with average loss: 0.0002\n",
      "Epoch [825/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 825 completed with average loss: 0.0002\n",
      "Epoch [826/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 826 completed with average loss: 0.0002\n",
      "Epoch [827/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 827 completed with average loss: 0.0002\n",
      "Epoch [828/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 828 completed with average loss: 0.0002\n",
      "Epoch [829/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 829 completed with average loss: 0.0002\n",
      "Epoch [830/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 830 completed with average loss: 0.0002\n",
      "Epoch [831/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 831 completed with average loss: 0.0002\n",
      "Epoch [832/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 832 completed with average loss: 0.0002\n",
      "Epoch [833/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 833 completed with average loss: 0.0002\n",
      "Epoch [834/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 834 completed with average loss: 0.0002\n",
      "Epoch [835/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 835 completed with average loss: 0.0002\n",
      "Epoch [836/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 836 completed with average loss: 0.0002\n",
      "Epoch [837/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 837 completed with average loss: 0.0002\n",
      "Epoch [838/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 838 completed with average loss: 0.0002\n",
      "Epoch [839/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 839 completed with average loss: 0.0002\n",
      "Epoch [840/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 840 completed with average loss: 0.0002\n",
      "Epoch [841/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 841 completed with average loss: 0.0002\n",
      "Epoch [842/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 842 completed with average loss: 0.0002\n",
      "Epoch [843/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 843 completed with average loss: 0.0002\n",
      "Epoch [844/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 844 completed with average loss: 0.0002\n",
      "Epoch [845/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 845 completed with average loss: 0.0002\n",
      "Epoch [846/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 846 completed with average loss: 0.0002\n",
      "Epoch [847/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 847 completed with average loss: 0.0002\n",
      "Epoch [848/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 848 completed with average loss: 0.0002\n",
      "Epoch [849/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 849 completed with average loss: 0.0002\n",
      "Epoch [850/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 850 completed with average loss: 0.0002\n",
      "Epoch [851/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 851 completed with average loss: 0.0002\n",
      "Epoch [852/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 852 completed with average loss: 0.0002\n",
      "Epoch [853/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 853 completed with average loss: 0.0002\n",
      "Epoch [854/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 854 completed with average loss: 0.0002\n",
      "Epoch [855/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 855 completed with average loss: 0.0002\n",
      "Epoch [856/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 856 completed with average loss: 0.0002\n",
      "Epoch [857/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 857 completed with average loss: 0.0002\n",
      "Epoch [858/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 858 completed with average loss: 0.0002\n",
      "Epoch [859/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 859 completed with average loss: 0.0002\n",
      "Epoch [860/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 860 completed with average loss: 0.0002\n",
      "Epoch [861/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 861 completed with average loss: 0.0002\n",
      "Epoch [862/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 862 completed with average loss: 0.0002\n",
      "Epoch [863/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 863 completed with average loss: 0.0002\n",
      "Epoch [864/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 864 completed with average loss: 0.0002\n",
      "Epoch [865/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 865 completed with average loss: 0.0002\n",
      "Epoch [866/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 866 completed with average loss: 0.0002\n",
      "Epoch [867/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 867 completed with average loss: 0.0002\n",
      "Epoch [868/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 868 completed with average loss: 0.0002\n",
      "Epoch [869/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 869 completed with average loss: 0.0002\n",
      "Epoch [870/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 870 completed with average loss: 0.0002\n",
      "Epoch [871/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 871 completed with average loss: 0.0002\n",
      "Epoch [872/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 872 completed with average loss: 0.0002\n",
      "Epoch [873/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 873 completed with average loss: 0.0002\n",
      "Epoch [874/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 874 completed with average loss: 0.0002\n",
      "Epoch [875/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 875 completed with average loss: 0.0002\n",
      "Epoch [876/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 876 completed with average loss: 0.0002\n",
      "Epoch [877/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 877 completed with average loss: 0.0002\n",
      "Epoch [878/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 878 completed with average loss: 0.0002\n",
      "Epoch [879/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 879 completed with average loss: 0.0002\n",
      "Epoch [880/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 880 completed with average loss: 0.0002\n",
      "Epoch [881/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 881 completed with average loss: 0.0002\n",
      "Epoch [882/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 882 completed with average loss: 0.0002\n",
      "Epoch [883/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 883 completed with average loss: 0.0002\n",
      "Epoch [884/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 884 completed with average loss: 0.0002\n",
      "Epoch [885/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 885 completed with average loss: 0.0002\n",
      "Epoch [886/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 886 completed with average loss: 0.0002\n",
      "Epoch [887/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 887 completed with average loss: 0.0002\n",
      "Epoch [888/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 888 completed with average loss: 0.0002\n",
      "Epoch [889/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 889 completed with average loss: 0.0002\n",
      "Epoch [890/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 890 completed with average loss: 0.0002\n",
      "Epoch [891/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 891 completed with average loss: 0.0002\n",
      "Epoch [892/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 892 completed with average loss: 0.0002\n",
      "Epoch [893/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 893 completed with average loss: 0.0002\n",
      "Epoch [894/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 894 completed with average loss: 0.0002\n",
      "Epoch [895/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 895 completed with average loss: 0.0002\n",
      "Epoch [896/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 896 completed with average loss: 0.0002\n",
      "Epoch [897/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 897 completed with average loss: 0.0002\n",
      "Epoch [898/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 898 completed with average loss: 0.0002\n",
      "Epoch [899/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 899 completed with average loss: 0.0002\n",
      "Epoch [900/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 900 completed with average loss: 0.0002\n",
      "Epoch [901/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 901 completed with average loss: 0.0002\n",
      "Epoch [902/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 902 completed with average loss: 0.0002\n",
      "Epoch [903/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 903 completed with average loss: 0.0002\n",
      "Epoch [904/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 904 completed with average loss: 0.0002\n",
      "Epoch [905/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 905 completed with average loss: 0.0002\n",
      "Epoch [906/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 906 completed with average loss: 0.0002\n",
      "Epoch [907/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 907 completed with average loss: 0.0002\n",
      "Epoch [908/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 908 completed with average loss: 0.0002\n",
      "Epoch [909/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 909 completed with average loss: 0.0002\n",
      "Epoch [910/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 910 completed with average loss: 0.0002\n",
      "Epoch [911/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 911 completed with average loss: 0.0002\n",
      "Epoch [912/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 912 completed with average loss: 0.0002\n",
      "Epoch [913/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 913 completed with average loss: 0.0002\n",
      "Epoch [914/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 914 completed with average loss: 0.0002\n",
      "Epoch [915/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 915 completed with average loss: 0.0002\n",
      "Epoch [916/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 916 completed with average loss: 0.0002\n",
      "Epoch [917/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 917 completed with average loss: 0.0002\n",
      "Epoch [918/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 918 completed with average loss: 0.0002\n",
      "Epoch [919/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 919 completed with average loss: 0.0002\n",
      "Epoch [920/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 920 completed with average loss: 0.0002\n",
      "Epoch [921/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 921 completed with average loss: 0.0002\n",
      "Epoch [922/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 922 completed with average loss: 0.0002\n",
      "Epoch [923/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 923 completed with average loss: 0.0002\n",
      "Epoch [924/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 924 completed with average loss: 0.0002\n",
      "Epoch [925/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 925 completed with average loss: 0.0002\n",
      "Epoch [926/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 926 completed with average loss: 0.0002\n",
      "Epoch [927/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 927 completed with average loss: 0.0002\n",
      "Epoch [928/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 928 completed with average loss: 0.0002\n",
      "Epoch [929/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 929 completed with average loss: 0.0002\n",
      "Epoch [930/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 930 completed with average loss: 0.0002\n",
      "Epoch [931/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 931 completed with average loss: 0.0002\n",
      "Epoch [932/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 932 completed with average loss: 0.0002\n",
      "Epoch [933/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 933 completed with average loss: 0.0002\n",
      "Epoch [934/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 934 completed with average loss: 0.0002\n",
      "Epoch [935/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 935 completed with average loss: 0.0002\n",
      "Epoch [936/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 936 completed with average loss: 0.0002\n",
      "Epoch [937/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 937 completed with average loss: 0.0002\n",
      "Epoch [938/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 938 completed with average loss: 0.0002\n",
      "Epoch [939/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 939 completed with average loss: 0.0002\n",
      "Epoch [940/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 940 completed with average loss: 0.0002\n",
      "Epoch [941/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 941 completed with average loss: 0.0002\n",
      "Epoch [942/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 942 completed with average loss: 0.0002\n",
      "Epoch [943/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 943 completed with average loss: 0.0002\n",
      "Epoch [944/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 944 completed with average loss: 0.0002\n",
      "Epoch [945/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 945 completed with average loss: 0.0002\n",
      "Epoch [946/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 946 completed with average loss: 0.0002\n",
      "Epoch [947/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 947 completed with average loss: 0.0002\n",
      "Epoch [948/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 948 completed with average loss: 0.0002\n",
      "Epoch [949/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 949 completed with average loss: 0.0002\n",
      "Epoch [950/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 950 completed with average loss: 0.0002\n",
      "Epoch [951/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 951 completed with average loss: 0.0002\n",
      "Epoch [952/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 952 completed with average loss: 0.0002\n",
      "Epoch [953/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 953 completed with average loss: 0.0002\n",
      "Epoch [954/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 954 completed with average loss: 0.0002\n",
      "Epoch [955/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 955 completed with average loss: 0.0002\n",
      "Epoch [956/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 956 completed with average loss: 0.0002\n",
      "Epoch [957/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 957 completed with average loss: 0.0002\n",
      "Epoch [958/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 958 completed with average loss: 0.0002\n",
      "Epoch [959/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 959 completed with average loss: 0.0002\n",
      "Epoch [960/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 960 completed with average loss: 0.0002\n",
      "Epoch [961/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 961 completed with average loss: 0.0002\n",
      "Epoch [962/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 962 completed with average loss: 0.0002\n",
      "Epoch [963/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 963 completed with average loss: 0.0002\n",
      "Epoch [964/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 964 completed with average loss: 0.0002\n",
      "Epoch [965/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 965 completed with average loss: 0.0002\n",
      "Epoch [966/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 966 completed with average loss: 0.0002\n",
      "Epoch [967/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 967 completed with average loss: 0.0002\n",
      "Epoch [968/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 968 completed with average loss: 0.0002\n",
      "Epoch [969/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 969 completed with average loss: 0.0002\n",
      "Epoch [970/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 970 completed with average loss: 0.0002\n",
      "Epoch [971/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 971 completed with average loss: 0.0002\n",
      "Epoch [972/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 972 completed with average loss: 0.0002\n",
      "Epoch [973/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 973 completed with average loss: 0.0002\n",
      "Epoch [974/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 974 completed with average loss: 0.0002\n",
      "Epoch [975/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 975 completed with average loss: 0.0002\n",
      "Epoch [976/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 976 completed with average loss: 0.0002\n",
      "Epoch [977/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 977 completed with average loss: 0.0002\n",
      "Epoch [978/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 978 completed with average loss: 0.0002\n",
      "Epoch [979/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 979 completed with average loss: 0.0002\n",
      "Epoch [980/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 980 completed with average loss: 0.0002\n",
      "Epoch [981/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 981 completed with average loss: 0.0002\n",
      "Epoch [982/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 982 completed with average loss: 0.0002\n",
      "Epoch [983/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 983 completed with average loss: 0.0002\n",
      "Epoch [984/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 984 completed with average loss: 0.0002\n",
      "Epoch [985/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 985 completed with average loss: 0.0002\n",
      "Epoch [986/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 986 completed with average loss: 0.0002\n",
      "Epoch [987/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 987 completed with average loss: 0.0002\n",
      "Epoch [988/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 988 completed with average loss: 0.0002\n",
      "Epoch [989/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 989 completed with average loss: 0.0002\n",
      "Epoch [990/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 990 completed with average loss: 0.0002\n",
      "Epoch [991/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 991 completed with average loss: 0.0002\n",
      "Epoch [992/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 992 completed with average loss: 0.0002\n",
      "Epoch [993/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 993 completed with average loss: 0.0002\n",
      "Epoch [994/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 994 completed with average loss: 0.0002\n",
      "Epoch [995/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 995 completed with average loss: 0.0002\n",
      "Epoch [996/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 996 completed with average loss: 0.0002\n",
      "Epoch [997/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 997 completed with average loss: 0.0002\n",
      "Epoch [998/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 998 completed with average loss: 0.0002\n",
      "Epoch [999/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 999 completed with average loss: 0.0002\n",
      "Epoch [1000/1000], Step [0/1], Loss: 0.0002\n",
      "Epoch 1000 completed with average loss: 0.0002\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = hw1_helper.train(model, optimizer, train_dataloader, criterion, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0281\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b25910936434b37bbddfe85864eae66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Index:', max=1), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function aa598.hw1_helper.plot_data_regression(history, future, prediction, index, xlims=[-11, 5], ylims=[-2, 2])>"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    # prediction = model(history)         # Forward pass\n",
    "    prediction = model(future)\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_data)-1, step=1, description='Index:')\n",
    "xlims = [-11, 5]\n",
    "ylims = [-2,2]\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0770, -0.6118, -0.8512,  0.0705, -0.8305, -0.4340, -0.5566, -0.6198,\n",
      "         -0.2919, -0.5448, -0.4966, -0.8653, -0.7488, -0.3526, -0.3696],\n",
      "        [-0.3775, -0.2914, -0.3535,  0.0993, -0.3572, -0.0442, -0.0013, -0.1880,\n",
      "         -0.1340, -0.1110, -0.0654, -0.3138, -0.3377, -0.0248, -0.0714]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0039,  0.0504, -0.0044, -0.1418, -0.0355,  0.0686,  0.1401,  0.1881,\n",
      "          0.2192,  0.2382,  0.2486,  0.2529,  0.2531,  0.2503,  0.2455,  0.2395],\n",
      "        [ 0.0352,  0.0425,  0.0417,  0.0347,  0.0404,  0.0465,  0.0511,  0.0550,\n",
      "          0.0583,  0.0613,  0.0639,  0.0662,  0.0681,  0.0697,  0.0708,  0.0715]]), tensor([[ 0.0000e+00, -9.0001e-01, -1.0000e+00, -9.5578e-01, -7.4095e-01,\n",
      "         -5.6259e-01, -4.7597e-01, -3.4102e-01, -2.3189e-01, -1.4395e-01,\n",
      "         -5.5126e-01, -3.9765e-01, -2.3511e-01, -1.1261e-01, -2.0596e-02],\n",
      "        [ 0.0000e+00, -3.0000e-01, -3.0000e-01, -3.0000e-01, -1.6630e-01,\n",
      "         -8.1036e-02, -5.7633e-02, -1.3074e-02,  1.6389e-02,  3.5917e-02,\n",
      "         -5.4275e-02, -2.5513e-02,  5.7829e-04,  1.8761e-02,  3.1676e-02]]))\n"
     ]
    }
   ],
   "source": [
    "print(test_data[:, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
