{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 2 libs\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle # for the project\n",
    "\n",
    "from aa598.hw2_helper import simulate_dynamics\n",
    "import cvxpy as cp\n",
    "from cbfax.dynamics import *\n",
    "\n",
    "# Homework 1 libs\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import aa598.hw1_helper as hw1_helper\n",
    "\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "rc('text', usetex=False) # set to False if latex is not set up on your computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = DynamicallyExtendedSimpleCar() # robot dynamics\n",
    "human = DynamicallyExtendedSimpleCar() # human dynamics\n",
    "\n",
    "@jax.jit\n",
    "def obstacle_constraint(state, obstacle, radius):\n",
    "    return jnp.linalg.norm(state[:2] - obstacle[:2]) - radius\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_horizon = 25\n",
    "num_time_steps = 30\n",
    "num_sqp_iterations = 15\n",
    "dt = 0.1\n",
    "t = 0. # this doesn't affect anything, but a value is needed \n",
    "radius = 1. # minimum collision distance\n",
    "\n",
    "v_max = 1.5\n",
    "v_min = 0.\n",
    "acceleration_max = 1.0\n",
    "acceleration_min = -1.0\n",
    "steering_max = 0.3\n",
    "steering_min = -0.3\n",
    "\n",
    "human_control_prediction_noise_limit = 0.25\n",
    "human_control_prediction_variance = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = cp.Variable([planning_horizon+1, robot.state_dim])  # cvx variable for states\n",
    "us = cp.Variable([planning_horizon, robot.control_dim])  # cvx variable for controls\n",
    "slack = cp.Variable(1) # slack variable to make sure the problem is feasible\n",
    "As = [cp.Parameter([robot.state_dim, robot.state_dim]) for _ in range(planning_horizon)]  # parameters for linearized dynamics\n",
    "Bs = [cp.Parameter([robot.state_dim, robot.control_dim]) for _ in range(planning_horizon)] # parameters for linearized dynamics\n",
    "Cs = [cp.Parameter([robot.state_dim]) for _ in range(planning_horizon)] # parameters for linearized dynamics\n",
    "\n",
    "Gs = [cp.Parameter([robot.state_dim]) for _ in range(planning_horizon+1)] # parameters for linearized constraints\n",
    "hs = [cp.Parameter(1) for _ in range(planning_horizon+1)] # parameters for linearized constraints\n",
    "\n",
    "xs_previous = cp.Parameter([planning_horizon+1, robot.state_dim]) # parameter for previous solution\n",
    "us_previous = cp.Parameter([planning_horizon, robot.control_dim]) # parameter for previous solution\n",
    "initial_state = cp.Parameter([robot.state_dim]) # parameter for current robot state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = 0.2 # coefficient for control effort\n",
    "beta2 = 2. # coefficient for progress\n",
    "beta3 = 10. # coefficient for trust region\n",
    "slack_penalty = 1000. # coefficient for slack variable\n",
    "markup = 1.0\n",
    "\n",
    "objective = beta2 * (xs[-1,2]**2 + xs[-1,1]**2 - xs[-1,0]) + beta3 * (cp.sum_squares(xs - xs_previous) + cp.sum_squares(us - us_previous)) + slack_penalty * slack**2\n",
    "constraints = [xs[0] == initial_state, slack >= 0] # initial state and slack constraint\n",
    "for t in range(planning_horizon):\n",
    "    objective += beta1 * cp.sum_squares(us[t]) * markup**t\n",
    "    constraints += [xs[t+1] == As[t] @ xs[t] + Bs[t] @ us[t] + Cs[t]] # dynamics constraint\n",
    "    constraints += [xs[t,-1] <= v_max, xs[t,-1] >= v_min, us[t,0] <= acceleration_max, us[t,0] >= acceleration_min, us[t,1] <= steering_max, us[t,1] >= steering_min] # control limit constraints\n",
    "    constraints += [Gs[t] @ xs[t] + hs[t] >= -slack] # linearized collision avoidance constraint\n",
    "constraints += [xs[planning_horizon,-1] <= v_max, xs[planning_horizon,-1] >= v_min, Gs[planning_horizon] @ xs[planning_horizon] + hs[planning_horizon] >= 0] # constraints for last planning horizon step\n",
    "prob = cp.Problem(cp.Minimize(objective), constraints) # construct problem\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial states\n",
    "robot_state = jnp.array([-3.0, -0., 0., 1.])  # robot starting state\n",
    "human_state = jnp.array([-1., -2., jnp.pi/2, 1.]) # human starting state\n",
    "\n",
    "robot_trajectory = [robot_state] # list to collect robot's state as it replans\n",
    "human_trajectory = [human_state] # list to collect humans's state\n",
    "robot_control_list = []  # list to collect robot's constrols as it replans\n",
    "robot_trajectory_list = [] # list to collect robot's planned trajectories\n",
    "human_control_list = []\n",
    "\n",
    "# initial robot planned state and controls\n",
    "previous_controls = jnp.zeros([planning_horizon, robot.control_dim]) # initial guess for robot controls\n",
    "previous_states =  simulate_dynamics(robot, robot_state, previous_controls, dt) # initial guess for robot states\n",
    "xs_previous.value = np.array(previous_states) # set xs_previous parameter value\n",
    "us_previous.value = np.array(previous_controls) # set us_previous parameter value \n",
    "\n",
    "# jit the linearize dynamics and constraint functions to make it run faster\n",
    "linearize_dynamics = jax.jit(lambda states, controls, ti: jax.vmap(linearize, [None, 0, 0, None])(lambda s, c, t: robot.discrete_step(s, c, t, dt), states, controls, ti))\n",
    "linearize_obstacle = jax.jit(lambda states, controls, radius: jax.vmap(jax.grad(obstacle_constraint), [0, 0, None])(states, controls, radius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 0\n",
      "timestep: 1\n",
      "timestep: 2\n",
      "timestep: 3\n",
      "timestep: 4\n",
      "timestep: 5\n",
      "timestep: 6\n",
      "timestep: 7\n",
      "timestep: 8\n",
      "timestep: 9\n",
      "timestep: 10\n",
      "timestep: 11\n",
      "timestep: 12\n",
      "timestep: 13\n",
      "timestep: 14\n",
      "timestep: 15\n",
      "timestep: 16\n",
      "timestep: 17\n",
      "timestep: 18\n",
      "timestep: 19\n",
      "timestep: 20\n",
      "timestep: 21\n",
      "timestep: 22\n",
      "timestep: 23\n",
      "timestep: 24\n",
      "timestep: 25\n",
      "timestep: 26\n",
      "timestep: 27\n",
      "timestep: 28\n",
      "timestep: 29\n"
     ]
    }
   ],
   "source": [
    "solver = cp.CLARABEL\n",
    "\n",
    "for t in range(num_time_steps):\n",
    "    print(\"timestep: %i\"% t)\n",
    "    initial_state.value = np.array(robot_state)\n",
    "    # simulate human future trajectory, assuming some noisy behavior\n",
    "    noisy_human_control = jnp.clip(jnp.array(np.random.randn(planning_horizon, human.control_dim) * human_control_prediction_variance), -human_control_prediction_noise_limit, human_control_prediction_noise_limit)\n",
    "    human_future = simulate_dynamics(human, human_state, noisy_human_control, dt)\n",
    "    \n",
    "    for i in range(num_sqp_iterations):\n",
    "        # As_value, Bs_value, Cs_value = jax.vmap(linearize, [None, 0, 0, None])(lambda s, c, t: robot.discrete_step(s, c, t, dt), previous_states[:-1], previous_controls, t)\n",
    "        As_value, Bs_value, Cs_value = linearize_dynamics( previous_states[:-1], previous_controls, t)\n",
    "        # Gs_value = jax.vmap(jax.grad(obstacle_constraint), [0, 0, None])(previous_states, human_future, radius)\n",
    "        Gs_value = linearize_obstacle(previous_states, human_future, radius)\n",
    "        hs_value = jax.vmap(obstacle_constraint, [0, 0, None])(previous_states, human_future, radius) - jax.vmap(jnp.dot, [0, 0])(Gs_value, previous_states)\n",
    "\n",
    "        for i in range(planning_horizon):\n",
    "            As[i].value = np.array(As_value[i])\n",
    "            Bs[i].value = np.array(Bs_value[i])\n",
    "            Cs[i].value = np.array(Cs_value[i])\n",
    "            Gs[i].value = np.array(Gs_value[i])\n",
    "            hs[i].value = np.array(hs_value[i:i+1])\n",
    "        Gs[planning_horizon].value = np.array(Gs_value[planning_horizon])\n",
    "        hs[planning_horizon].value = np.array(hs_value[planning_horizon:planning_horizon+1])\n",
    "        \n",
    "        result = prob.solve(solver=solver)\n",
    "\n",
    "        # previous_states = xs.value\n",
    "        previous_controls = us.value\n",
    "        previous_states =  simulate_dynamics(robot, robot_state, previous_controls, dt)\n",
    "        xs_previous.value = np.array(previous_states)\n",
    "        us_previous.value = np.array(previous_controls)\n",
    "       \n",
    "    robot_control = previous_controls[0]\n",
    "    robot_control_list.append(robot_control)\n",
    "    # robot takes a step\n",
    "    robot_state = robot.discrete_step(robot_state, robot_control, 0., dt)\n",
    "    robot_trajectory.append(robot_state)\n",
    "    robot_trajectory_list.append(previous_states)\n",
    "\n",
    "    \n",
    "    human_random_control = jnp.clip(jnp.array(np.random.randn( human.control_dim) * human_control_prediction_variance), -human_control_prediction_noise_limit, human_control_prediction_noise_limit)\n",
    "    human_control_list.append(human_random_control)\n",
    "    # human states a step\n",
    "    human_state = human.discrete_step(human_state, human_random_control, 0., dt)\n",
    "    human_trajectory.append(human_state)\n",
    "\n",
    "human_traj = [human_trajectory]\n",
    "human_random_cont = (human_random_control)\n",
    "robot_trajectory = jnp.stack(robot_trajectory)\n",
    "human_trajectory = jnp.stack(human_trajectory)\n",
    "robot_controls = jnp.stack(robot_control_list)\n",
    "human_controls = jnp.stack(human_control_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #Project code\n",
    "# history_trajectory = jnp.array([robot_trajectory], [robot_control_list],[human_trajectory], [human_random_control])\n",
    "# with open('history.pickle', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(history_trajectory, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open('future.pickle', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(singleTrainData, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# # Training data\n",
    "# trajectoryTrainingData = {'xPosition' : robot_trajectory[:-5, 0], 'yPosition' : robot_trajectory[:-5, 1],\n",
    "#         'heading' : robot_trajectory[:-5, 2], 'velocity' : robot_trajectory[:-5, 3]}\n",
    "# with open('robot_trajectory_train_v3.pickle', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(trajectoryTrainingData, f, pickle.HIGHEST_PROTOCOL)\n",
    "# # Testing data\n",
    "# trajectoryTestingData = {'xPosition' : robot_trajectory[-5:, 0], 'yPosition' : robot_trajectory[-5:, 1],\n",
    "#         'heading' : robot_trajectory[-5:, 2], 'velocity' : robot_trajectory[-5:, 3]}\n",
    "# with open('robot_trajectory_test_v3.pickle', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(trajectoryTestingData, f, pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca056dce93745e899d75a7721a388c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=14, description='i', max=29), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting\n",
    "@interact(i=(0,num_time_steps-1))\n",
    "def plot(i):\n",
    "    fig, axs = plt.subplots(1,2, figsize=(18,8))\n",
    "    ax = axs[0]\n",
    "    robot_position = robot_trajectory[i, :2]\n",
    "    human_position = human_trajectory[i, :2]\n",
    "    circle1 = plt.Circle(robot_position, radius / 2, color='C0', alpha=0.4)\n",
    "    circle2 = plt.Circle(human_position, radius / 2, color='C1', alpha=0.4)\n",
    "    ax.add_patch(circle1)\n",
    "    ax.add_patch(circle2)\n",
    "    # ax.plot(human_samples[i,:,:,0].T, human_samples[i,:,:,1].T, \"o-\", alpha=0.1, markersize=2, color='C1')\n",
    "    ax.plot(robot_trajectory[:,0], robot_trajectory[:,1], \"o-\", markersize=3, color='C0')\n",
    "    ax.plot(robot_trajectory_list[i][:,0], robot_trajectory_list[i][:,1], \"o-\", markersize=3, color='C2', label=\"planned\")\n",
    "    print(robot_trajectory[i])\n",
    "\n",
    "    ax.plot(human_trajectory[:,0], human_trajectory[:,1], \"o-\", markersize=3, color='C1')\n",
    "    ax.scatter(robot_trajectory[i:i+1,0], robot_trajectory[i:i+1,1], s=30,  color='C0', label=\"Robot\")\n",
    "    ax.scatter(human_trajectory[i:i+1,0], human_trajectory[i:i+1,1], s=30,  color='C1', label=\"Human\")\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_xlim([-4,4])\n",
    "    ax.set_ylim([-3, 2])\n",
    "    ax.axis(\"equal\")\n",
    "\n",
    "    ax.set_title(\"heading=%.2f velocity=%.2f\"%(robot_trajectory[i,2], robot_trajectory[i,3]))\n",
    "    \n",
    "    ax = axs[1]\n",
    "    plt.plot(robot_controls)\n",
    "    plt.scatter([i], robot_controls[i:i+1, 0], label=\"Acceleration\")\n",
    "    plt.scatter([i], robot_controls[i:i+1, 1], label=\"Steering\")\n",
    "    ax.plot(robot_trajectory[:,-1], \"o-\", markersize=3, color='C0', label=\"Velocity\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.77215546 -0.8842893  -0.9601989  -0.9967745  -0.99251807]\n",
      " [-0.33736357 -0.6201609  -0.83809453 -0.96837014 -0.99736214]\n",
      " [-0.9241497  -0.9772044  -0.9993569  -0.98990667 -0.9491525 ]\n",
      " ...\n",
      " [ 0.8635387   0.84961313  0.8350685   0.8199155   0.80416495]\n",
      " [-0.78483945 -0.86998636 -0.934877   -0.9780004  -0.9983526 ]\n",
      " [ 0.9903264   0.98666096  0.8932651   0.7186327   0.47864532]]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/hw1/wave_data_train.pickle\", 'rb') as handle:\n",
    "    wave_data = pickle.load(handle)\n",
    "history = wave_data[\"history\"]\n",
    "future = wave_data[\"future\"]\n",
    "print((future))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "zero_row = jnp.zeros((1, 2))\n",
    "if (robot_controls[0] != 0).all():\n",
    "    robot_controls = jnp.vstack([zero_row, robot_controls])\n",
    "if (human_controls[0] != 0).all():\n",
    "    human_controls = jnp.vstack([zero_row, human_controls])\n",
    "\n",
    "totalMatrix = jnp.concatenate([robot_trajectory, robot_controls, human_trajectory, human_controls], axis = 1) \n",
    "# standardMatrix = jax.nn.standardize(totalMatrix)\n",
    "totalMatrix = np.array(totalMatrix)\n",
    "print(type(totalMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.5973402  -0.11299384 -0.06318497  0.5539429   0.23945051  0.07150198\n",
      " -1.101018    1.1584742   1.6394137   1.043159    0.25       -0.22132698]\n"
     ]
    }
   ],
   "source": [
    "print(totalMatrix[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84668077 0.28748375 0.35779326 0.26190037]\n",
      " [0.69095691 0.11451063 0.55191438 0.9498493 ]\n",
      " [0.43844422 0.67443648 0.57997547 0.41213325]\n",
      " [0.77084981 0.0320562  0.25998733 0.6083555 ]]\n"
     ]
    }
   ],
   "source": [
    "A = np.random.rand(4, 4)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.delete(A, [1, 2], axis=1)\n",
    "# \n",
    "# print(np.size(totalMatrix))\n",
    "# print(totalMatrix[:, 4:6])\n",
    "onlyHistoryData = np.delete(totalMatrix, [4, 6], axis = 1) # deleting the 4, 5 row because this is 'x'\n",
    "onlyTestData = (totalMatrix[:, 4:6])\n",
    "train_data_history = np.transpose(onlyHistoryData[:15, :])\n",
    "train_data_future = np.transpose(onlyHistoryData[:15, :])\n",
    "test_data_history = np.transpose(onlyTestData[15:, :])\n",
    "test_data_future = np.transpose(onlyTestData[15:, :])\n",
    "train_data = {'history' : train_data_history, 'future' : train_data_future}\n",
    "test_data = {'history' : test_data_history, 'future' : test_data_future}\n",
    "# print(len(train_data['history']))\n",
    "# print(train_data['future'])\n",
    "# print(len(train_data['history']))\n",
    "# print((test_data['future']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the training data\n",
    "with open('train.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(train_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "# This will be the testing data\n",
    "with open('test.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(test_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "# When training will have to leave out the testing robot controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.pickle\", 'rb') as handle:\n",
    "    train = pickle.load(handle)\n",
    "with open(\"test.pickle\", 'rb') as handle:\n",
    "    test = pickle.load(handle)\n",
    "# print((test.shape))\n",
    "# print((train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.arange(0,31)\n",
    "# print((totalMatrix))\n",
    "# plt.plot(t, standardMatrix[0:31, 0], color  = 'r') # Robot controls\n",
    "# plt.plot(t, standardMatrix[0:31, 6], color  = 'b') # Human controls\n",
    "plt.plot(t, totalMatrix[0:31, 0], color  = 'y') # Robot controls\n",
    "plt.plot(t, totalMatrix[0:31, 6], color  = 'g') # Human controls\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to poke around the data\n",
    "# THIS NEEDS TO BE REVIEWED\n",
    "test_data = hw1_helper.TrajectoryData(\"test\")\n",
    "train_data = hw1_helper.TrajectoryData(\"train\")\n",
    "\n",
    "\n",
    "# history_length = 3 # Number of backward steps considered to train\n",
    "# future_length = 3 # Number of forward considered to train\n",
    "# input_size = 10\n",
    "# output_size = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 10)\n",
      "<class 'dict'>\n",
      "(tensor([-0.0321, -0.0187]), tensor([-0.0321, -0.0187]))\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/hw1/wave_data_test.pickle\", 'rb') as handle:\n",
    "    test_wave_data = pickle.load(handle)\n",
    "print(test_wave_data['history'].shape)\n",
    "print(type(wave_data))\n",
    "print(test_data[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # HINT: Use Pytorch built-in functions for LSTM and Linear layers.\n",
    "        # HINT: batch dimension is dim=0\n",
    "\n",
    "        '''\n",
    "        looking at the code provided on pytorch\n",
    "\n",
    "        \n",
    "        nn.LSTM is looking for an input and an output without specifying that it is\n",
    "        being used as an encoder or a decoder so it can be reversed?\n",
    "\n",
    "        nn.Linear is a transformation that turn the output into a linearized equation where y = Ax + b\n",
    "            b is the bias\n",
    "            A is the weight\n",
    "            y is the output\n",
    "        '''\n",
    "        \n",
    "        # TODO: Define encoder LSTM.\n",
    "        self.encoder = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        ############################\n",
    "        \n",
    "        \n",
    "        # TODO: Define decoder LSTM\n",
    "        self.decoder = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        ############################\n",
    "        \n",
    "        \n",
    "        #TODO: Define linear project from hidden_dim to output_dim\n",
    "        # The output is a scalar but it needs to be an array [~,4]\n",
    "        self.projection = torch.nn.Linear(hidden_dim, output_dim, bias = False)\n",
    "        ############################\n",
    "        \n",
    "\n",
    "    def forward(self, x, t_max, y=None, prob=1.):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM model.\n",
    "        x: The input sequence [batch_size, seq_len, input_dim]\n",
    "        t_max: maximum time steps to unroll\n",
    "        y: The target sequence for teacher forcing (optional, used if teacher forcing is applied) [batch_size, t_max, output_dim]\n",
    "        prob: Probability to apply teacher forcing (0 to 1). 1 means 100% teacher forcing, \n",
    "        \"\"\"\n",
    "        \n",
    "        # making sure x and y is the appropriate size.\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(-1)\n",
    "        if y is not None and len(y.shape) == 2:\n",
    "            y = y.unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "        ys = [] # collect outputs\n",
    "        # TODO: Run input through encoder to get initial hidden state for decoder\n",
    "        _, h = self.encoder(x)\n",
    "        ############################\n",
    "        \n",
    "        \n",
    "        # TODO: initial state for decoder is last input state\n",
    "        y_input = x[:,-1]\n",
    "        # print('y_input = ', y_input)\n",
    "        y_input = y_input.unsqueeze(1)\n",
    "        ############################\n",
    "        \n",
    "\n",
    "        # TODO: unroll decoder \n",
    "        # TODO: if eval or no teacher forcing, use prediction from previous step\n",
    "        # TODO: if train and using teacher forcing, use prob to determine whether to use ground truth or previous prediction\n",
    "        \n",
    "        ############################\n",
    "        for i in range(t_max):\n",
    "            output, h = self.decoder(y_input, h)\n",
    "            output = self.projection(output)\n",
    "            # TODO: y_input = ...?\n",
    "            random = torch.rand([1, 1])\n",
    "            # if random >= 0.7:\n",
    "            #     y_input = output\n",
    "            # else:\n",
    "            #     y_input = y_input\n",
    "            y_input = output\n",
    "            ys.append(output.squeeze(1))\n",
    "        ys = torch.cat(ys, dim=1)\n",
    "    \n",
    "        \n",
    "        return ys # [batch_size, ts_max, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1\n",
    "future_length = 15\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "model = LSTM(input_size, output_size, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 10 # This is the number of data points in a array\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "prob = 0.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "def prob_schedule(i):\n",
    "    return 1 - jax.nn.sigmoid(20 * (i - 0.5)).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [0/1], Loss: 1.2488\n",
      "Epoch 1 completed with average loss: 1.2488\n",
      "Epoch [2/10], Step [0/1], Loss: 1.2485\n",
      "Epoch 2 completed with average loss: 1.2485\n",
      "Epoch [3/10], Step [0/1], Loss: 1.2481\n",
      "Epoch 3 completed with average loss: 1.2481\n",
      "Epoch [4/10], Step [0/1], Loss: 1.2478\n",
      "Epoch 4 completed with average loss: 1.2478\n",
      "Epoch [5/10], Step [0/1], Loss: 1.2475\n",
      "Epoch 5 completed with average loss: 1.2475\n",
      "Epoch [6/10], Step [0/1], Loss: 1.2472\n",
      "Epoch 6 completed with average loss: 1.2472\n",
      "Epoch [7/10], Step [0/1], Loss: 1.2468\n",
      "Epoch 7 completed with average loss: 1.2468\n",
      "Epoch [8/10], Step [0/1], Loss: 1.2465\n",
      "Epoch 8 completed with average loss: 1.2465\n",
      "Epoch [9/10], Step [0/1], Loss: 1.2462\n",
      "Epoch 9 completed with average loss: 1.2462\n",
      "Epoch [10/10], Step [0/1], Loss: 1.2459\n",
      "Epoch 10 completed with average loss: 1.2459\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# we use a slightly different training loop to account for teacher forcing\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    prob = prob_schedule((epoch + 1)/num_epochs)\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()        # Zero the gradients\n",
    "        output = model(data, future_length, target, prob)         # Forward pass\n",
    "        loss = criterion(output, target)  # Compute loss\n",
    "        loss.backward()              # Backpropagation\n",
    "        optimizer.step()             # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (15) must match the size of tensor b (16) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (history, future) \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[1;32m      5\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model(history, future_length)         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print out test loss\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:538\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/functional.py:3383\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3381\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3383\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (15) must match the size of tensor b (16) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history, future_length)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# try with different prediction horizons\n",
    "prediction_horizon = 16\n",
    "prediction = model(history, prediction_horizon)\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "test_dataloader = list(DataLoader(test_data, batch_size=1, shuffle=False))\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_dataloader)-1, step=1, description='Index:')\n",
    "xlims = [-16, prediction_horizon + 2]\n",
    "ylims = [-2,2]\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  tensor([-0.0933, -0.0766, -0.0716, -0.0694, -0.0686, -0.0683, -0.0682, -0.0682,\n",
      "        -0.0682, -0.0682, -0.0682, -0.0682, -0.0682, -0.0682, -0.0682],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "1  tensor([-0.0881, -0.0763, -0.0712, -0.0693, -0.0686, -0.0683, -0.0682, -0.0682,\n",
      "        -0.0682, -0.0682, -0.0682, -0.0682, -0.0682, -0.0682, -0.0682],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('0 ', prediction[0, :])\n",
    "print('1 ',prediction[1, :])\n",
    "# print('history', test['history'])\n",
    "# print('future', test['future'])\n",
    "# print('history', test['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[ 0.0000e+00, -9.0001e-01, -1.0000e+00, -9.5578e-01, -7.4095e-01,\n",
      "         -5.6259e-01, -4.7597e-01, -3.4102e-01, -2.3189e-01, -1.4395e-01,\n",
      "         -5.5126e-01, -3.9765e-01, -2.3511e-01, -1.1261e-01, -2.0596e-02],\n",
      "        [ 0.0000e+00, -3.0000e-01, -3.0000e-01, -3.0000e-01, -1.6630e-01,\n",
      "         -8.1036e-02, -5.7633e-02, -1.3074e-02,  1.6389e-02,  3.5917e-02,\n",
      "         -5.4275e-02, -2.5513e-02,  5.7829e-04,  1.8761e-02,  3.1676e-02]])\n",
      "<class 'numpy.ndarray'>\n",
      "[[-0.00599216 -0.01271572 -0.01582921 -0.01724304 -0.01788754 -0.01818348\n",
      "  -0.0183201  -0.01838337 -0.01841272 -0.01842635 -0.01843268 -0.01843562\n",
      "  -0.01843699 -0.01843762 -0.01843791]\n",
      " [-0.00701745 -0.01333736 -0.01612204 -0.01737714 -0.01794906 -0.01821185\n",
      "  -0.01833322 -0.01838946 -0.01841555 -0.01842766 -0.01843329 -0.0184359\n",
      "  -0.01843712 -0.01843768 -0.01843794]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1571e9040>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGhCAYAAABGRD9PAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuh0lEQVR4nO3df3BU9b3/8de6MTEhyYKg3kgiazWCQVNMNCiWXvGWwWGQXyHGa0vJBYnQe0eQWod8NQJONTVSgevcgKGOMuO90gIRZ+DqdRwKZUChbRqvGOydjiE/IA0mJdkQyibZnO8fW7asWWBXTrL7Cc/HzE7mfM5nz75zgrMvP5/POcdhWZYlAAAAA1wV7QIAAADCRXABAADGILgAAABjEFwAAIAxCC4AAMAYBBcAAGAMggsAADAGwQUAABiD4AIAAIwRc8Glu7tbPT090S4DAADEINuDy5EjR/Tkk09q5cqVmjFjhj7//PN+fdrb2/XUU0/pueee08MPP6x33303sG/atGmKj4+Xw+EIvFpbW+0uEwAAGCjOzoN5vV4VFRVp3759GjZsmHbu3Kl58+aptrZWDocj0O+JJ57QihUrNHHiRDU3NysrK0t33XWX3G63br75Zr3yyitBxx0xYkRYn9/X16cTJ04oJSUl6PMAAEDssixLnZ2duvHGG3XVVZcYU7FstGPHDuvRRx8NbPf29lpJSUnWZ599Fmg7efKkNXr06KD3TZs2zXrttdcsy7KskpKSb/z5jY2NliRevHjx4sWLl4GvxsbGS37X2zriUlNTo/T09MC20+lURkaGmpqadMcdd0iSPvvsM6WlpQW9z+12q6mpSZJ06tQpzZgxQ/v27dM//MM/6KWXXlJBQUHIz/N6vfJ6vYFt628Pum5sbFRqaqqdvxoAABggHo9HGRkZSklJuWRfW4NLa2urrrvuuqA2l8ultra2oD5JSUkX7JOYmKif/vSnSktL07PPPqvHHntM2dnZGjt2bL/PKysr05o1a/q1p6amElwAADBMOMs8bF2cG+qKoPj4eMXHx4fd59VXX9WECRN0ww03qKKiQsOHD9fu3btDfl5JSYk6OjoCr8bGRjt/HQAAEGNsHXEZOXKk2tvbg9q6urqCpobC6XNOfHy8brnlln4jNOckJCQoISHhsusGAABmsHXEJTMzU3V1dUFtLS0tys7ODurT0NAgn88XaKuvr1deXp7+8Ic/9DvmX/7yF33ve9+zs0wAAGAoW4PL7NmzVV1drebmZknSoUOHNH36dDkcDhUWFqqhoUG33nqr7rzzTu3atUuSf7SlpaVFDz74oL744gsdOHAgcLydO3cqPz9ft956q51lAgAAQ9k6VTRq1Ci98847+pd/+RdNnDhRXq9Xr776qjo6OvTRRx/pz3/+s2666SZt27ZNjz/+uD7++GN1d3frjTfeUFxcnGbOnKl//Md/1LXXXqvMzEzdcccdeumll+wsEQAAGMxhnbuGeAjweDxyuVzq6OjgqiIAAAwRyfd3zD2rCAAA4EIILgAAwBi2rnEBAGAo8vmk/ful5mYpLU2aPFlyOqNd1ZWJ4AIAwEVUVUnLlkl/ezKNJCk9XdqwQZo7N3p1XamYKgIA4AKqqqR584JDiyQdP+5vr6qKTl1XMoILAAAh+Hz+kZZQ196ea1u+3N8Pg4fgAgBACPv39x9pOZ9lSY2N/n4YPKxxAQAghL/dBN62frHKtIXHBBcAAEII8ezfy+oXi0xceMxUEQAAIUye7P8SdzhC73c4pIwMfz8TmbrwmOACAEAITqd/5EHqH17Oba9fH9vTKhdi8sJjggsAABcwd660fbs0enRwe3q6vz1Wp1MuxeSFx6xxAQDgIubOlWbNMmsB66WYvPCY4AIAwCU4ndIDD0S7CvuYvPCYqSIAAK4wJi88JrgAAHCFMXnhMcEFAIArkKkLj1njAgDAFcrEhccEFwAArmCmLTxmqggAABiDERcAAAaIaQ8wNAHBBQCAAWDiAwxNwFQRACDqfD5p717pnXf8P2PxGTmRMPUBhiYguAAAoqqqSnK7pSlTpMce8/90u839cjf5AYYmILgAAKJmKI5MmPwAQxMQXAAAUTFURyZMfoChCQguAICoGKojEyY/wNAEBBcAQFQM1ZEJkx9gaAKCCwAgKobqyITJDzA0AcEFABAVQ3lkwtQHGJqAG9ABAKLi3MjEvHn+kHL+It2hMDJh4gMMTUBwAQBEzbmRiVB3mF2/3vyRCdMeYGgCggsAIKoYmUAkCC4AgKhjZALhYnEuAAAwBsEFAAAYg+ACAACMQXABAADGYHEuACCqfD6uKDJBrPydbA8uR44cUWVlpZKSknTkyBG9/PLLGj9+fFCf9vZ2rVmzRsOGDdOnn36qhQsXas6cOZKkxsZGvfLKK0pJSVFNTY1WrlypySbeNhEAcElVVaHv4bJhg/n3cBlKYurvZNno7NmzVm5urnX69GnLsizr3XfftcaNG2f19fUF9XvkkUesTz75xLIsyzpx4oQ1fPhwq66uzrIsy7r//vutpqYmy7Is6w9/+IM1cuRIq7OzM6zP7+josCRZHR0dNv1GAICBsmOHZTkcluW/Z+7fXw6H/7VjR7QrhGUNzt8pku9vW9e47N69W5mZmRo2bJgk6eGHH1ZDQ4M+//zzQJ+vvvpKBw4c0MSJEyVJaWlpmjhxonbt2qXq6mr5fD6N/tvDHSZMmKBRo0Zp7969dpYJAIgyn8//f/Dn3+b/nHNty5f7+yF6YvHvZGtwqampUXp6emDb6XQqIyNDTeeNLX322WdK+9qjPt1ut5qamvq9//x9oXi9Xnk8nqAXACD27d8fPO3wdZYlNTb6+yF6YvHvZGtwaW1tVVJSUlCby+VSW1tbWH3Cef/5ysrK5HK5Aq+MjAybfhMAwEBqbra3HwZGLP6dbA0u3d3d6unpCWqLj49XfHx8WH3Cef/5SkpK1NHREXg1Njba9JsAAAbS1wbeL7sfBkYs/p1svapo5MiRam9vD2rr6uoKmhq6WJ9w3n++hIQEJSQk2FE6AGAQTZ7svyrl+PHQ6yccDv9+LiqNrsmTfEq/rkfHv0qQJUe//dH4O9k64pKZmam6urqgtpaWFmVnZwf1aWhokO+8lTz19fXKy8sL+f5z+wAAQ4fT6b+UVvJ/+Z3v3Pb69dzPJaqqquS8xa0NXz0myZJDfUG7o/V3sjW4zJ49W9XV1Wr+22TXoUOHNH36dDkcDhUWFqqhoUG33nqr7rzzTu3atUuSf0SlpaVFDz74oL773e/qr3/9q2pqaiRJTU1Nuv7663XbbbfZWSYAIAbMnStt3y797ULSgPR0fzv3cYmiqipp3jypqUlz9a62a55G63hQl2j9nRyWFWqQ7pvbs2ePysvLNXHiRHm9Xj377LPq6OjQt7/9bb3//vvKy8tTU1OTHn/8cU2YMEHd3d164oknNHbsWEn+q45+/OMfa+LEiTpz5oyefvrpC04VfZ3H45HL5VJHR4dSU1Pt/LUAAAMkVu7Iir/x+SS3u9/lRD5dpf2arGbdqLTrejW56R054+35Q0Xy/W17cIkmggsAAJdp715pypRL9/v1r6UHHrDlIyP5/uYhiwAA4O9i8Rro8/CQRQAwHFMtsFUsXgN9HkZcAMBgVVX+5QhTpkiPPeb/6Xb724FvZNIkadSoC+93OKSMjKhdq05wAQBDnXfhR5Djx/3thBdErKpKuuUWqbU19P4YuFad4AIABorFh9/BcBdKwueLgWvVWeMCAAaK5OF3Nl34gaHsYkn4nOuuk/70J+kCj+EZLIy4AICBYvzCD5jmUklYkr76Sjp4cHDquQiCCwAYKMYv/IBpfvSj8PrFQBJmqggADMRDCmGbq666+BTR+WIgCTPiAgAG4iGFsMXVV4cfWq67LiaSMMEFAAzFQwpxWRoapN7e8Pt///sxkYSZKgIAg82dK82axZ1z8Q2MHx9Z/1mzBqaOCBFcAMBwTieXPOMbOHMm/L4uV0xME0lMFQEAcGVKSgq/7+uvx8wwHiMuADBAePghYtrnn0tjxly639SpUmHhwNcTJoILAAyAqir/jUjPv6dXerr/SiAWzSIm3HSTFBd38QW6Dof04YeDV1MYmCoCAJvx8EMYo6fHH15CiYuT+voGt54wEFwAwEY8/BDG6emR6uul5GT/zeiSk/3bPT3RriwkggsA2CiShx8CMeOmm6TOTn+i7uz0b8coggsA2IiHHwIDi+ACADbi4YfAwCK4AICNzj388OvPDzrH4ZAyMmLmXl6AcQguAGAjHn4IDCyCCwDYjIcfAgOHG9ABwADg4YfAwCC4AMAA4eGHgP2YKgIAAMYguAAAAGMQXAAAgDEILgAAwBgEFwAAYAyCCwAAMAbBBQAAGIPgAgAAjEFwAQAAxiC4AAAAYxBcAACAMQguAADAGDEXXHw+n7xer/r6+qJdCgAAiDG2Ph26u7tbq1atUlxcnGprazV16lQtWbIkZN/Kykp9+eWXam1tVXx8vDZs2KCrr75ab7/9toqKioL6vvLKK3r66aftLBUAABjI1uBSWlqqnJwcFRYW6vTp08rKylJ2drYmTZoU1O/9999XTU2NKioqJElz5szR2rVrVVJSIsuy9NZbb2n8+PGB/hkZGXaWCQAADGXbVFFfX582b96smTNnSpKSk5M1ffp0VVVV9eu7ceNGzZo1K7BdWFioHTt2BLbvu+8+3X333YHXDTfcYFeZAADAYLYFl2PHjsnpdCoxMTHQ5na71dTU1K9vTU2N0tPTL9hv7dq1uvHGGzV8+HD96Ec/ktfrDfmZXq9XHo8n6AUAAIYu24JLa2urkpKSgtpcLpfa2tou2ff8fsnJybrvvvt05MgRvfHGG3rzzTf14osvhvzMsrIyuVyuwIspJSB6fD5p717pnXf8P32+aFcEYCiKaI1LZWWlKisrQ+5LTU1VT09PUFt8fLzi4+P79e3u7g7qe36/efPmBdrz8/O1b98+7dy5Uy+88EK/45SUlGjFihWBbY/HQ3gBoqCqSlq2TDp/gDU9XdqwQZo7N3p1ARh6IgouxcXFKi4uDrnv6NGjys3NDWrr6upSWlpav74jR45Ue3v7JftJUlZWlg4fPhxyX0JCghISEsKsHsBAqKqS5s2TLCu4/fhxf/v27YQXAPaxbapozJgx6u3tVUtLS6Ctvr5eeXl5/fpmZmaqrq6uX7/m5uag90vSyZMn9dBDD9lVJgAb+Xz+kZavhxbp723Llw/utBFTVsDQZltwSUpKUn5+vrZu3SrJf5VRdXW1CgoKJEnl5eXasmWLJGn+/PmBfpL/8uilS5dq+PDhWrt2baD91KlT+vjjj4OmgwDEjv37g6eHvs6ypMZGf7/BUFUlud3SlCnSY4/5f7rd/nYAQ4Ot93HZtGmTioqKVF9fL4fDobKyMo0YMUKSdPDgQbndbi1YsEDFxcX605/+pAULFmjMmDG65557NHnyZEn+6Z+cnBxNmDBBo0aN0ltvvaXU1FQ7ywRgk+Zme/tdDqasgCuDw7JCDfKayePxyOVyqaOjg7ADDIK9e/2jGpfy619LDzwwcHX4fP6RlQuN/jgc/sXCdXWS0zlwdQD4ZiL5/o65ZxUBMMfkyf5A4HCE3u9wSBkZ/n4DKdamrAAMHIILgG/M6fRf8iz1Dy/nttevH/hRjliasgIwsAguAC7L3Ln+9SOjRwe3p6cP3rqSC9xN4Rv3AxC7WOMCwBY+n38qprnZHxAmTx689STn1rgcPx760mzWuACxLZLvb1uvKgJw5XI6B3YB7qU+e8MG/9VDDkdweBnMKSsAA4+pIgBDQixMWQEYeIy4ABgy5s6VZs2K3pQVgIFHcAEwpERzygrAwGOqCAAAGIPgAgAAjEFwAQAAxiC4AAAAYxBcAACAMQguAADAGAQXAABgDIILAAAwBsEFAAAYg+ACAACMQXABAADG4FlFACLm8/EgQwDRQXABEJGqKmnZMqmp6e9t6enShg3+pzMDwEBiqghA2KqqpHnzgkOLJB0/7m+vqopOXQCuHAQXAGHx+fwjLZbVf9+5tuXL/f0AYKAQXACEZf/+/iMt57MsqbHR3w8ABgrBBUBYmpvt7QcA3wTBBUBY0tLs7QcA3wTBBUBYJk/2Xz3kcITe73BIGRn+fgAwUAguAMLidPoveZb6h5dz2+vXcz8XAAOL4AIgbHPnStu3S6NHB7enp/vbuY8LgIHGDegARGTuXGnWLO6cCyA6CC4AIuZ0Sg88EO0qAFyJmCoCAADGILgAAABjEFwAAIAxCC4AAMAYBBcAAGAMggsAADAGwQUAABgj6sGloaEh2iUAAABD2HoDuu7ubq1atUpxcXGqra3V1KlTtWTJkn79LMvSr3/9a1VUVGj//v1qaWkJ2l9ZWakvv/xSra2tio+P14YNG3T11VfbWSoAADCQrcGltLRUOTk5Kiws1OnTp5WVlaXs7GxNmjQpqN/OnTvV2tqqqVOnau/evUH73n//fdXU1KiiokKSNGfOHK1du1YlJSV2lgoAAAxk21RRX1+fNm/erJkzZ0qSkpOTNX36dFVVVfXrO2fOHC1evFgJCQn99m3cuFGzZs0KbBcWFmrHjh12lQkAAAxm24jLsWPH5HQ6lZiYGGhzu92qqamJ6Dg1NTVKT08POkZTU1PIvl6vV16vN7Dt8XgiKxoAABjFthGX1tZWJSUlBbW5XC61tbVd1nEudoyysjK5XK7AKyMjI/LCAQCAMSIacamsrFRlZWXIfampqerp6Qlqi4+PV3x8fEQFdXd3Bx3nYscoKSnRihUrAtsej4fwAgDAEBZRcCkuLlZxcXHIfUePHlVubm5QW1dXl9LS0iIqaOTIkWpvbw/rGAkJCSHXyQAAgKHJtqmiMWPGqLe3N+jS5vr6euXl5UV0nMzMTNXV1V3WMQAAwNBkW3BJSkpSfn6+tm7dKsl/lVF1dbUKCgokSeXl5dqyZUvQe3w+X7/jzJ8/P3AMyX959NKlS+0qEwAAGMzW+7hs2rRJRUVFqq+vl8PhUFlZmUaMGCFJOnjwoNxutxYsWCBJWrdunXbt2qVTp07phRde0IwZM5STk6Pi4mL96U9/0oIFCzRmzBjdc889mjx5sp1lAgAAQzksy7KiXYRdPB6PXC6XOjo6lJqaGu1yAABAGCL5/o76s4oAAADCRXABAADGILgAAABjEFwAAIAxCC4AAMAYBBcAAGAMggsAADAGwQUAABiD4AIAAIxBcAEAAMYguAAAAGMQXAAAgDEILgAAwBgEFwAAYAyCCwAAMAbBBQAAGIPgAgAAjEFwAQAAxiC4AAAAYxBcAACAMQguAADAGAQXAABgDIILAAAwBsEFAAAYg+ACAACMQXABAADGILgAAABjEFwAAIAxCC4AAMAYBBcAAGAMggsAADAGwQUAABiD4AIAAIxBcAEAAMYguAAAAGMQXAAAgDEILgAAwBgEFwAAYIyoB5eGhoag7b6+Pp09e1Z9fX1RqggAAMQqW4NLd3e3SkpKVFpaqvz8fG3atClkP8uytGfPHs2bN0/33HNP0L79+/crMTFRTqdTDodDDodD//Zv/2ZnmQAAwFBxdh6stLRUOTk5Kiws1OnTp5WVlaXs7GxNmjQpqN/OnTvV2tqqqVOnau/evUH7LMtSWVmZvve97wXabrjhBjvLBAAAhrItuPT19Wnz5s06fvy4JCk5OVnTp09XVVVVv+AyZ84cSdJbb70V8lg5OTm6++677SoNAAAMEbZNFR07dkxOp1OJiYmBNrfbraampoiP9fbbb+tb3/qWkpOT9eijj+rUqVMh+3m9Xnk8nqAXAAAYumwLLq2trUpKSgpqc7lcamtri+g411xzjcaPH6/Dhw/rvffe0549e7Rs2bKQfcvKyuRyuQKvjIyMb1w/AACIfRFNFVVWVqqysjLkvtTUVPX09AS1xcfHKz4+PqKC7r33Xt17772SpH/6p3/SypUrtXr16pB9S0pKtGLFisC2x+MhvAAAMIRFFFyKi4tVXFwcct/Ro0eVm5sb1NbV1aW0tLRvXp2krKysfiM55yQkJCghIeGyjg8AAMxh21TRmDFj1Nvbq5aWlkBbfX298vLywj7GmTNn9Mc//jGo7eTJk3rooYfsKhMAABjMtuCSlJSk/Px8bd26VZL/KqPq6moVFBRIksrLy7Vly5ag9/h8vn7HqKioCNx87uzZs9qxY4deeOEFu8oEAAAGs/U+Lps2bVJRUZHq6+vlcDhUVlamESNGSJIOHjwot9utBQsWSJLWrVunXbt26dSpU3rhhRc0Y8YM5eTkaNy4cZowYYKys7N13XXX6dVXX9VNN91kZ5kAAMBQDsuyrGgXYRePxyOXy6WOjg6lpqZGuxwAABCGSL6/o/6sIgAAgHARXAAAgDEILgAAwBgEFwAAYAyCCwAAMIatl0MDpvD5pP37peZmKS1NmjxZcjqjXRUA4FIILrjiVFVJy5ZJ5z+4PD1d2rBBmjs3enUBAC6NqSJcUaqqpHnzgkOLJB0/7m+vqopOXQCA8BBccMXw+fwjLaFuuXiubflyfz8AQGwiuOCKsX9//5GW81mW1Njo7wcAiE0EF1wxmpvt7QcAGHwEF1wx0tLs7QcAGHwEF1wxJk/2Xz3kcITe73BIGRn+fgCA2ERwwRXD6fRf8iz1Dy/nttev534uABDLCC64osydK23fLo0eHdyenu5v5z4uABDbuAEdrjhz50qzZnHnXAAwEcEFVySnU3rggWhXAQCIFFNFAADAGAQXAABgDIILAAAwBsEFAAAYg+ACAACMQXABAADGILgAAABjEFwAAIAxCC4AAMAYBBcAAGAMggsAADAGwQUAABiD4AIAAIxBcAEAAMYguAAAAGMQXAAAgDEILgAAwBgEFwAAYAyCCwAAMAbBBQAAGIPgAgAAjEFwAQAAxoiz82Dd3d1atWqV4uLiVFtbq6lTp2rJkiX9+n3xxRdaunSpDh06pJSUFC1atEgvvviiHA6HJOmll15SV1eX6urqlJmZqTVr1thZJgAAMJStwaW0tFQ5OTkqLCzU6dOnlZWVpezsbE2aNCmo3+rVq7VgwQJVVFRo9+7deuaZZ3TLLbdo0aJF2rhxo5xOp1588UX5fD5NnDhR48aN0z//8z/bWSoAADCQbVNFfX192rx5s2bOnClJSk5O1vTp01VVVdWv77hx41RUVKTbb79dTz/9tKZMmaJ9+/ZJkjZu3KjZs2dLkpxOp+bNm6cdO3bYVSYAADCYbcHl2LFjcjqdSkxMDLS53W41NTX167t69eqg7ZSUFKWlpam7u1u1tbVKT0+/5DEkyev1yuPxBL0AAMDQZVtwaW1tVVJSUlCby+VSW1vbRd935swZHT58WAsXLlR7e7t8Pl/QcS52jLKyMrlcrsArIyPj8n8RAAAQsyJa41JZWanKysqQ+1JTU9XT0xPUFh8fr/j4+Ise8/nnn1dpaanGjh0bGFnp6ekJvO9ixygpKdGKFSsC2x6Ph/ACAMAQFlFwKS4uVnFxcch9R48eVW5ublBbV1eX0tLSLni8bdu2KSUlRUuXLpUkjRw5UpLU3t6u66+//pLHSEhIUEJCQiS/AgAAMJhtU0VjxoxRb2+vWlpaAm319fXKy8sL2X/fvn2qra3VqlWrAm2JiYkaPXq06urqwjoGAAC4stgWXJKSkpSfn6+tW7dK8l9lVF1drYKCAklSeXm5tmzZIkmqqanRunXrNG3aNH3yySc6cOCAPvjgA/X29mr+/PmBY0jSnj17tHDhQrvKBAAABnNYlmXZdbCOjg4VFRXp5ptvlsPhUEFBge69915J0uzZs+V2u7V69WqNHTtWJ0+e7Pf+trY2JScna/HixRo2bJhSU1P1ne98RzNmzAjr8z0ej1wulzo6OpSammrXrwUAAAZQJN/ftgaXy9Hb26u4uMu7Hx7BBQAA80Ty/R0zzyq63NACAACGvpgJLgAAAJdCcAEAAMYguAAAAGMQXAAAgDEILgAAwBgEFwAAYAyCCwAAMAbBBQAAGIPgAgAAjEFwAQAAxiC4AAAAYxBcAACAMQguAADAGAQXAABgDIILAAAwBsEFAAAYg+ACAACMQXABAADGILgAAABjEFwAAIAxCC4AAMAYBBcAAGAMggsAADAGwQUAABiD4AIAAIxBcAEAAMYguAAAAGMQXAAAgDEILgAAwBgEFwAAYAyCCwAAMAbBBQAAGIPgAgAAjEFwAQAAxiC4AAAAYxBcAACAMQguAADAGDEXXHw+n7xer/r6+qJdCgAAiDG2Bpfu7m6VlJSotLRU+fn52rRpU8h+X3zxhaZMmaKkpCTdcMMN+n//7//JsixJ0ttvv61rrrlGTqdTDodDDodDa9eutbNMAABgqDg7D1ZaWqqcnBwVFhbq9OnTysrKUnZ2tiZNmhTUb/Xq1VqwYIEqKiq0e/duPfPMM7rlllu0aNEiWZalt956S+PHjw/0z8jIsLNMAABgKNuCS19fnzZv3qzjx49LkpKTkzV9+nRVVVX1Cy7jxo1TUVGRJOn222/X+++/r3379mnRokWSpPvuu0+33XabXaUBAIAhwrapomPHjsnpdCoxMTHQ5na71dTU1K/v6tWrg7ZTUlKUlpYW2F67dq1uvPFGDR8+XD/60Y/k9XpDfqbX65XH4wl6AQCAocu24NLa2qqkpKSgNpfLpba2tou+78yZMzp8+LAWLlwoyT9Sc9999+nIkSN644039Oabb+rFF18M+d6ysjK5XK7AiyklAACGNod1blVsGCorK1VZWRlyX2pqqr744gudOHEi0PbGG2+oqqpKu3fvvuAxn376ad1yyy1aunRpyP1PPvmk9u7dq//93//tt8/r9QaNxng8HmVkZKijo0Opqanh/loAACCKPB6PXC5XWN/fEa1xKS4uVnFxcch9R48eVW5ublBbV1dX0BTQ123btk0pKSkXDC2SlJWVpcOHD4fcl5CQoISEhDAqBwAAQ4FtU0VjxoxRb2+vWlpaAm319fXKy8sL2X/fvn2qra3VqlWrAm3Nzc1B75ekkydP6qGHHrKrTAAAYDDbgktSUpLy8/O1detWSf6rjKqrq1VQUCBJKi8v15YtWyRJNTU1WrdunaZNm6ZPPvlEBw4c0AcffKDk5OSge7acOnVKH3/8sVasWGFXmQAAwGC23sdl06ZNKioqUn19vRwOh8rKyjRixAhJ0sGDB+V2uzVr1ixNmzZNJ0+e1HvvvRf0/ra2NiUkJCgnJ0cTJkzQqFGj9NZbb7FeBQAASIpwce5A6u3tVVzc5eWoSBb3AACA2BDJ93fMPKvockMLAAAY+mImuAAAAFwKwQUAABiD4AIAAIxBcAEAAMYguAAAAGMQXAAAgDEILgAAwBgEFwAAYAyCCwAAMAbBBQAAGIPgAgAAjEFwAQAAxiC4AAAAYxBcAACAMQguAADAGAQXAABgDIILAAAwBsEFAAAYg+ACAACMQXABAADGILgAAABjEFwAAIAxCC4AAMAYBBcAAGAMggsAADAGwQUAABiD4AIAAIxBcAEAAMYguAAAAGMQXAAAgDEILgAAwBgEFwAAYAyCCwAAMAbBBQAAGIPgAgAAjEFwAQAAxiC4AAAAY8RFuwAT+HzS/v1Sc7OUliZNniw5nbFzvGh9BgAAg83W4NLd3a1Vq1YpLi5OtbW1mjp1qpYsWdKvX0tLi4qLi/XRRx8pOTlZS5cu1erVqwP7X3rpJXV1damurk6ZmZlas2aNnWVGpKpKWrZMamr6e1t6urRhgzR3bvSPF63PAAAgKiwbPfPMM9bWrVsty7Kszs5OKyMjwzpw4EC/fqWlpdY777xjffXVV9Yvf/lLKy4uztqzZ49lWZZVUVFh/exnP7Msy7J6e3ut3Nxc67/+67/C+vyOjg5LktXR0WHL77Njh2U5HJYlBb8cDv9rx47oHi9anwEAgJ0i+f52WJZl2RGA+vr6NGrUKB0/flyJiYmSpCVLlig5OVlr164N6vub3/xG3/3udwPbd999t8rLy/Xggw8qOztb27Zt09ixYyVJP/vZz/S73/1O27dvv2QNHo9HLpdLHR0dSk1Nvazfx+eT3O7gUYvzORz+UYy6uvCmYOw+XrQ+AwAAu0Xy/W3b4txjx47J6XQGQoskud1uNYX4Fj0/tPzxj3/UXXfdpSlTpqi7u1u1tbVKT0+/5DEkyev1yuPxBL3ssn//hQOA5B/HaGz094vG8aL1GQAARJNtwaW1tVVJSUlBbS6XS21tbSH7//a3v9WCBQs0e/ZsdXZ26v/+7//U3t4un88XdJyLHaOsrEwulyvwysjIsOvXUXNzbPeL1mcAABBNES3OraysVGVlZch9qamp6unpCWqLj49XfHx8yP533XWXtmzZot7eXs2ZM0cLFiwITAf19PQE3nexY5SUlGjFihWBbY/HY1t4SUuL7X7R+gwAAKIpouBSXFys4uLikPuOHj2q3NzcoLauri6lXeBbMi4uLvDzscce0+LFizVy5EhJUnt7u66//vpLHiMhIUEJCQmR/AphmzzZvx7k+HH/FMvXnVsvMnlydI4Xrc8AACCabJsqGjNmjHp7e9XS0hJoq6+vV15eXlC/v/zlL/rNb34T1Hb27Fnl5eUpMTFRo0ePVl1d3UWPMRicTv/lw5L/C/9857bXrw9/kavdx4vWZwAAEE22BZekpCTl5+dr69atkvxXGVVXV6ugoECSVF5eri1btujaa6/VK6+8Iq/XK0ny+Xzatm2bfv7zn0uS5s+fHziGJO3Zs0cLFy60q8yIzJ0rbd8ujR4d3J6e7m+P9J4odh8vWp8BAEC02HY5tCR1dHSoqKhIN998sxwOhwoKCnTvvfdKkmbPni23263169dr5cqV+p//+R/dc889GjVqlB577DHdcccdkvw3sVu8eLGGDRum1NRUfec739GMGTPC+nw7L4c+H3fOBQBg4ETy/W1rcIm2gQouAABg4ETlPi4AAAADjeACAACMQXABAADGILgAAABjEFwAAIAxCC4AAMAYBBcAAGAMggsAADAGwQUAABgjoqdDx7pzNwH2eDxRrgQAAITr3Pd2ODfzH1LBpbOzU5KUkZER5UoAAECkOjs75XK5LtpnSD2rqK+vTydOnFBKSoocDoetx/Z4PMrIyFBjYyPPQRpAnOfBwXkeHJznwcF5HjwDda4ty1JnZ6duvPFGXXXVxVexDKkRl6uuukrp6ekD+hmpqan8hzEIOM+Dg/M8ODjPg4PzPHgG4lxfaqTlHBbnAgAAYxBcAACAMQguYUpISNCqVauUkJAQ7VKGNM7z4OA8Dw7O8+DgPA+eWDjXQ2pxLgAAGNoYcQEAAMYguAAAAGMQXAAAgDEILmFoaGiIdglA1PT29srr9Ua7jCHtxIkTqqysjHYZgBFYnHsBXq9X27dv12uvvab09HRt3779gn2rqqp04MABdXd3q62tTRs3bgz7RjqQuru7tWrVKsXFxam2tlZTp07VkiVLQvZdt26dPB6PTpw4odbWVr3xxhsaPnz44BZsqEjOs+S/k+W///u/q6GhQYsXL9a4ceMGsVpzRXqeJWnKlCnq7OzU7373u0Gq0nzhnucvvvhCS5cu1aFDh5SSkqJFixbpxRdftP3u6kPJkSNHVFlZqaSkJB05ckQvv/yyxo8fH9Snvb1da9as0bBhw/Tpp59q4cKFmjNnzuAUaCGkl19+2frwww+tH/7wh1Z+fv4F+3366afWzJkzA9vLli2ziouLB6PEIeOZZ56xtm7dalmWZXV2dloZGRnWgQMH+vX71a9+Za1cuTKwvXTpUuu5554btDpNF+55tizL+utf/2r94Ac/sN57773BLHFIiOQ8W5Zl/fKXv7QeeeQRKzc3d7BKHBLCPc+FhYXWm2++adXW1lqvvPKK5XA4rF/84heDXa4xzp49a+Xm5lqnT5+2LMuy3n33XWvcuHFWX19fUL9HHnnE+uSTTyzLsqwTJ05Yw4cPt+rq6galRqaKLuCZZ57R1KlTL5nKf/GLX2j69OmB7UcffVQ7duwY6PKGjL6+Pm3evFkzZ86UJCUnJ2v69Omqqqrq13fv3r267rrrAtt33HHHoNVpukjOsyT94Ac/0P333x/oj/BEep47Ojp0/Phx3X777YNZpvEiOc/jxo1TUVGRbr/9dj399NOaMmWK9u3bN9glG2P37t3KzMzUsGHDJEkPP/ywGhoa9Pnnnwf6fPXVVzpw4IAmTpwoSUpLS9PEiRO1a9euQamR4HKZampqgp6P5Ha71dbWprNnz0axKnMcO3ZMTqdTiYmJgTa3262mpqZ+fbOysrR27Vp9/PHH6u3t1e9//3stX758EKs1VyTned++fTp48KAWL148mCUOCZGcZ0n6j//4Dz3++OODVd6QEcl5Xr16ddB2SkqK0tLSBrpEY339O83pdCojIyPo3H722Wf9zuHF/p3bjeBymVpbW5WUlBTYPre2pa2tLVolGeXr50/yn8NQ52/JkiWaMWOGHnjgAY0fP14PPvigRo4cOVilGi2S8/yf//mfuvvuu1VaWqrbbrtN999/vw4dOjRYpRotkvP8+eef64YbblBKSspglTdkRHKez3fmzBkdPnxYCxcuHMjyjBbOuf2m598uQ+rp0JGorKy84Cr+a6+9Vh9++GFYx+nu7lZPT09gOz4+PugnLn6uU1NTg86f5D93oc7fl19+qZSUFP35z3/W66+/rscff1x1dXV67rnnBqRu09h1nn//+98rLi5OFRUVWrNmjZYvX65HHnlE9fX1A1K3aew6z5WVlfr5z38+IDUOBXad5/M9//zzKi0t1dixY22rc6j5+nea1P/chtNnIF2xwaW4uFjFxcWXfZyRI0eqvb09sN3V1aX4+Hhde+21l33soeJi5/ro0aPKzc0Nauvq6go5lLto0SJt2bJFI0aM0MqVK3XjjTfqJz/5CcHlb+w6z11dXSouLg4MFz///POqqKjQV199FbTG6Eplx3mura3Vxo0b9frrr0vyX3JuWZauueYaHT58WNnZ2QNTvEHs+vd8zrZt25SSkqKlS5faWudQ8/XvNKn/uQ2nz0BiqugyZWZmqq6uLrBdX1+v3NxcOZ3OKFZljjFjxqi3t1ctLS2Btvr6euXl5fXr+9vf/lajRo0KbN9zzz26+uqrB6VO00Vynm+66SZ1dnYGtlNTU3X11VcrOTl5UGo1WbjnOSsrS93d3Tp79qzOnj2r5557TvPnz9fZs2cJLWGI5N+z5F+3VVtbq1WrVg1Wicb6+neaJLW0tAT9u8zMzFRDQ4N8Pl+g7WLn324El0s4/w9zzpYtW1ReXi5Jmj9/vrZv3x7o99///d/613/910Gt0WRJSUnKz8/X1q1bJfmvFqiurlZBQYEkqby8XFu2bJEk5eTk6IMPPgi8d+fOnXryyScHv2gDRXKev//97wed548++kjf//73gxZCIrRIzvP5urq6+g2948IiOc81NTVat26dpk2bpk8++UQHDhzQBx98oN7e3qjVH8tmz56t6upqNTc3S5IOHTqk6dOny+FwqLCwUA0NDbr11lt15513Bq4i6urqUktLix588MFBqZEb0F1AU1OTfvWrX+m1115Tb2+vli1bph/+8Ie6/vrrtXz5ch07dkw7d+6UJL366qv6+OOPdccddygpKUk/+clPolu8YTo6OlRUVKSbb75ZDodDBQUFuvfeeyX5/yNyu91av369vvzySz311FPKyspSYmKiEhMT9eMf/1hXXUX+Dke457mvr08//elPVV9fr29961s6e/asSkpK+i3GQ2jhnmfJv4h/69atqqiokMfj0VNPPaX58+czJReGcM7z6tWrNXbsWJ08ebLf+9va2pjSv4A9e/aovLxcEydOlNfr1bPPPquOjg59+9vf1vvvv6+8vDw1NTXp8ccf14QJE9Td3a0nnnhi0NYOEVwAAFeU3t5excVdsUs8jUdwAQAAxmCMHQAAGIPgAgAAjEFwAQAAxiC4AAAAYxBcAACAMQguAADAGAQXAABgDIILAAAwBsEFAAAYg+ACAACMQXABAADGILgAAABj/H+nFZ3jFO8BygAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.linspace(0,16, 16)\n",
    "# plt.plot(t, prediction[:, 0].detach().numpy())\n",
    "print(type(prediction))\n",
    "# print(prediction[:, :])\n",
    "predict = prediction.detach().numpy()\n",
    "# future = future.detach().numpy()\n",
    "print(future)\n",
    "print(type(predict))\n",
    "print(predict)\n",
    "plt.scatter(predict[0, :], predict[1, :], marker = 'o',  color = 'r')\n",
    "plt.scatter(future[0, :], future[1, :], marker = 'o', color = 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainXPosition' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrainXPosition\u001b[49m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m j \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(testXPosition), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(t, trainXPosition, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainXPosition' is not defined"
     ]
    }
   ],
   "source": [
    "t = jnp.arange(0, len(trainXPosition), 1)\n",
    "j = jnp.arange(0, len(testXPosition), 1)\n",
    "plt.scatter(t, trainXPosition, color='b')\n",
    "plt.scatter(j+len(trainXPosition), testXPosition , color = 'k')\n",
    "# plt.scatter(j+len(trainXPosition), prediction)\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # plotting\n",
    "# @interact(i=(0,num_time_steps-1))\n",
    "# def plot(i):\n",
    "#     fig, axs = plt.subplots(1,2, figsize=(18,8))\n",
    "#     ax = axs[0]\n",
    "#     robot_position = train_data[i]\n",
    "#     circle1 = plt.Circle(robot_position, radius / 2, color='C0', alpha=0.4)\n",
    "#     ax.add_patch(circle1)\n",
    "#     ax.add_patch(circle2)\n",
    "#     # ax.plot(human_samples[i,:,:,0].T, human_samples[i,:,:,1].T, \"o-\", alpha=0.1, markersize=2, color='C1')\n",
    "#     ax.plot(train_data[0], train_data[1], \"o-\", markersize=3, color='C0')\n",
    "#     ax.plot(train_data[i][0], train_data[i][1], \"o-\", markersize=3, color='C2', label=\"planned\")\n",
    "#     ax.scatter(train_data[i:i+1], train_data[i:i+1], s=30,  color='C0', label=\"Robot\")\n",
    "#     ax.grid()\n",
    "#     ax.legend()\n",
    "\n",
    "#     ax.set_xlim([-4,4])\n",
    "#     ax.set_ylim([-3, 2])\n",
    "#     ax.axis(\"equal\")\n",
    "\n",
    "#     ax.set_title(\"heading=%.2f velocity=%.2f\"%(robot_trajectory[i,2], robot_trajectory[i,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple MLP model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, history_length, future_length, hidden_size=32):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # TODO: construct MLP network\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(history_length, 20),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(20, 40),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(40, 25),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(25, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, future_length),\n",
    "        )\n",
    "        #############################\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 4\n",
    "history_length = 15\n",
    "future_length = 15\n",
    "\n",
    "model = MLP(history_length, future_length, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Step [0/1], Loss: 1.2241\n",
      "Epoch 1 completed with average loss: 1.2241\n",
      "Epoch [2/1000], Step [0/1], Loss: 1.2212\n",
      "Epoch 2 completed with average loss: 1.2212\n",
      "Epoch [3/1000], Step [0/1], Loss: 1.2182\n",
      "Epoch 3 completed with average loss: 1.2182\n",
      "Epoch [4/1000], Step [0/1], Loss: 1.2151\n",
      "Epoch 4 completed with average loss: 1.2151\n",
      "Epoch [5/1000], Step [0/1], Loss: 1.2121\n",
      "Epoch 5 completed with average loss: 1.2121\n",
      "Epoch [6/1000], Step [0/1], Loss: 1.2092\n",
      "Epoch 6 completed with average loss: 1.2092\n",
      "Epoch [7/1000], Step [0/1], Loss: 1.2062\n",
      "Epoch 7 completed with average loss: 1.2062\n",
      "Epoch [8/1000], Step [0/1], Loss: 1.2032\n",
      "Epoch 8 completed with average loss: 1.2032\n",
      "Epoch [9/1000], Step [0/1], Loss: 1.2000\n",
      "Epoch 9 completed with average loss: 1.2000\n",
      "Epoch [10/1000], Step [0/1], Loss: 1.1968\n",
      "Epoch 10 completed with average loss: 1.1968\n",
      "Epoch [11/1000], Step [0/1], Loss: 1.1933\n",
      "Epoch 11 completed with average loss: 1.1933\n",
      "Epoch [12/1000], Step [0/1], Loss: 1.1903\n",
      "Epoch 12 completed with average loss: 1.1903\n",
      "Epoch [13/1000], Step [0/1], Loss: 1.1874\n",
      "Epoch 13 completed with average loss: 1.1874\n",
      "Epoch [14/1000], Step [0/1], Loss: 1.1844\n",
      "Epoch 14 completed with average loss: 1.1844\n",
      "Epoch [15/1000], Step [0/1], Loss: 1.1813\n",
      "Epoch 15 completed with average loss: 1.1813\n",
      "Epoch [16/1000], Step [0/1], Loss: 1.1779\n",
      "Epoch 16 completed with average loss: 1.1779\n",
      "Epoch [17/1000], Step [0/1], Loss: 1.1744\n",
      "Epoch 17 completed with average loss: 1.1744\n",
      "Epoch [18/1000], Step [0/1], Loss: 1.1705\n",
      "Epoch 18 completed with average loss: 1.1705\n",
      "Epoch [19/1000], Step [0/1], Loss: 1.1664\n",
      "Epoch 19 completed with average loss: 1.1664\n",
      "Epoch [20/1000], Step [0/1], Loss: 1.1620\n",
      "Epoch 20 completed with average loss: 1.1620\n",
      "Epoch [21/1000], Step [0/1], Loss: 1.1574\n",
      "Epoch 21 completed with average loss: 1.1574\n",
      "Epoch [22/1000], Step [0/1], Loss: 1.1523\n",
      "Epoch 22 completed with average loss: 1.1523\n",
      "Epoch [23/1000], Step [0/1], Loss: 1.1469\n",
      "Epoch 23 completed with average loss: 1.1469\n",
      "Epoch [24/1000], Step [0/1], Loss: 1.1411\n",
      "Epoch 24 completed with average loss: 1.1411\n",
      "Epoch [25/1000], Step [0/1], Loss: 1.1348\n",
      "Epoch 25 completed with average loss: 1.1348\n",
      "Epoch [26/1000], Step [0/1], Loss: 1.1282\n",
      "Epoch 26 completed with average loss: 1.1282\n",
      "Epoch [27/1000], Step [0/1], Loss: 1.1209\n",
      "Epoch 27 completed with average loss: 1.1209\n",
      "Epoch [28/1000], Step [0/1], Loss: 1.1131\n",
      "Epoch 28 completed with average loss: 1.1131\n",
      "Epoch [29/1000], Step [0/1], Loss: 1.1046\n",
      "Epoch 29 completed with average loss: 1.1046\n",
      "Epoch [30/1000], Step [0/1], Loss: 1.0955\n",
      "Epoch 30 completed with average loss: 1.0955\n",
      "Epoch [31/1000], Step [0/1], Loss: 1.0857\n",
      "Epoch 31 completed with average loss: 1.0857\n",
      "Epoch [32/1000], Step [0/1], Loss: 1.0751\n",
      "Epoch 32 completed with average loss: 1.0751\n",
      "Epoch [33/1000], Step [0/1], Loss: 1.0639\n",
      "Epoch 33 completed with average loss: 1.0639\n",
      "Epoch [34/1000], Step [0/1], Loss: 1.0519\n",
      "Epoch 34 completed with average loss: 1.0519\n",
      "Epoch [35/1000], Step [0/1], Loss: 1.0391\n",
      "Epoch 35 completed with average loss: 1.0391\n",
      "Epoch [36/1000], Step [0/1], Loss: 1.0253\n",
      "Epoch 36 completed with average loss: 1.0253\n",
      "Epoch [37/1000], Step [0/1], Loss: 1.0104\n",
      "Epoch 37 completed with average loss: 1.0104\n",
      "Epoch [38/1000], Step [0/1], Loss: 0.9944\n",
      "Epoch 38 completed with average loss: 0.9944\n",
      "Epoch [39/1000], Step [0/1], Loss: 0.9776\n",
      "Epoch 39 completed with average loss: 0.9776\n",
      "Epoch [40/1000], Step [0/1], Loss: 0.9600\n",
      "Epoch 40 completed with average loss: 0.9600\n",
      "Epoch [41/1000], Step [0/1], Loss: 0.9413\n",
      "Epoch 41 completed with average loss: 0.9413\n",
      "Epoch [42/1000], Step [0/1], Loss: 0.9214\n",
      "Epoch 42 completed with average loss: 0.9214\n",
      "Epoch [43/1000], Step [0/1], Loss: 0.9003\n",
      "Epoch 43 completed with average loss: 0.9003\n",
      "Epoch [44/1000], Step [0/1], Loss: 0.8779\n",
      "Epoch 44 completed with average loss: 0.8779\n",
      "Epoch [45/1000], Step [0/1], Loss: 0.8545\n",
      "Epoch 45 completed with average loss: 0.8545\n",
      "Epoch [46/1000], Step [0/1], Loss: 0.8298\n",
      "Epoch 46 completed with average loss: 0.8298\n",
      "Epoch [47/1000], Step [0/1], Loss: 0.8041\n",
      "Epoch 47 completed with average loss: 0.8041\n",
      "Epoch [48/1000], Step [0/1], Loss: 0.7776\n",
      "Epoch 48 completed with average loss: 0.7776\n",
      "Epoch [49/1000], Step [0/1], Loss: 0.7503\n",
      "Epoch 49 completed with average loss: 0.7503\n",
      "Epoch [50/1000], Step [0/1], Loss: 0.7225\n",
      "Epoch 50 completed with average loss: 0.7225\n",
      "Epoch [51/1000], Step [0/1], Loss: 0.6943\n",
      "Epoch 51 completed with average loss: 0.6943\n",
      "Epoch [52/1000], Step [0/1], Loss: 0.6660\n",
      "Epoch 52 completed with average loss: 0.6660\n",
      "Epoch [53/1000], Step [0/1], Loss: 0.6380\n",
      "Epoch 53 completed with average loss: 0.6380\n",
      "Epoch [54/1000], Step [0/1], Loss: 0.6105\n",
      "Epoch 54 completed with average loss: 0.6105\n",
      "Epoch [55/1000], Step [0/1], Loss: 0.5838\n",
      "Epoch 55 completed with average loss: 0.5838\n",
      "Epoch [56/1000], Step [0/1], Loss: 0.5584\n",
      "Epoch 56 completed with average loss: 0.5584\n",
      "Epoch [57/1000], Step [0/1], Loss: 0.5343\n",
      "Epoch 57 completed with average loss: 0.5343\n",
      "Epoch [58/1000], Step [0/1], Loss: 0.5118\n",
      "Epoch 58 completed with average loss: 0.5118\n",
      "Epoch [59/1000], Step [0/1], Loss: 0.4907\n",
      "Epoch 59 completed with average loss: 0.4907\n",
      "Epoch [60/1000], Step [0/1], Loss: 0.4711\n",
      "Epoch 60 completed with average loss: 0.4711\n",
      "Epoch [61/1000], Step [0/1], Loss: 0.4528\n",
      "Epoch 61 completed with average loss: 0.4528\n",
      "Epoch [62/1000], Step [0/1], Loss: 0.4356\n",
      "Epoch 62 completed with average loss: 0.4356\n",
      "Epoch [63/1000], Step [0/1], Loss: 0.4191\n",
      "Epoch 63 completed with average loss: 0.4191\n",
      "Epoch [64/1000], Step [0/1], Loss: 0.4032\n",
      "Epoch 64 completed with average loss: 0.4032\n",
      "Epoch [65/1000], Step [0/1], Loss: 0.3876\n",
      "Epoch 65 completed with average loss: 0.3876\n",
      "Epoch [66/1000], Step [0/1], Loss: 0.3721\n",
      "Epoch 66 completed with average loss: 0.3721\n",
      "Epoch [67/1000], Step [0/1], Loss: 0.3565\n",
      "Epoch 67 completed with average loss: 0.3565\n",
      "Epoch [68/1000], Step [0/1], Loss: 0.3408\n",
      "Epoch 68 completed with average loss: 0.3408\n",
      "Epoch [69/1000], Step [0/1], Loss: 0.3251\n",
      "Epoch 69 completed with average loss: 0.3251\n",
      "Epoch [70/1000], Step [0/1], Loss: 0.3096\n",
      "Epoch 70 completed with average loss: 0.3096\n",
      "Epoch [71/1000], Step [0/1], Loss: 0.2942\n",
      "Epoch 71 completed with average loss: 0.2942\n",
      "Epoch [72/1000], Step [0/1], Loss: 0.2792\n",
      "Epoch 72 completed with average loss: 0.2792\n",
      "Epoch [73/1000], Step [0/1], Loss: 0.2647\n",
      "Epoch 73 completed with average loss: 0.2647\n",
      "Epoch [74/1000], Step [0/1], Loss: 0.2506\n",
      "Epoch 74 completed with average loss: 0.2506\n",
      "Epoch [75/1000], Step [0/1], Loss: 0.2373\n",
      "Epoch 75 completed with average loss: 0.2373\n",
      "Epoch [76/1000], Step [0/1], Loss: 0.2246\n",
      "Epoch 76 completed with average loss: 0.2246\n",
      "Epoch [77/1000], Step [0/1], Loss: 0.2129\n",
      "Epoch 77 completed with average loss: 0.2129\n",
      "Epoch [78/1000], Step [0/1], Loss: 0.2022\n",
      "Epoch 78 completed with average loss: 0.2022\n",
      "Epoch [79/1000], Step [0/1], Loss: 0.1915\n",
      "Epoch 79 completed with average loss: 0.1915\n",
      "Epoch [80/1000], Step [0/1], Loss: 0.1809\n",
      "Epoch 80 completed with average loss: 0.1809\n",
      "Epoch [81/1000], Step [0/1], Loss: 0.1709\n",
      "Epoch 81 completed with average loss: 0.1709\n",
      "Epoch [82/1000], Step [0/1], Loss: 0.1612\n",
      "Epoch 82 completed with average loss: 0.1612\n",
      "Epoch [83/1000], Step [0/1], Loss: 0.1520\n",
      "Epoch 83 completed with average loss: 0.1520\n",
      "Epoch [84/1000], Step [0/1], Loss: 0.1433\n",
      "Epoch 84 completed with average loss: 0.1433\n",
      "Epoch [85/1000], Step [0/1], Loss: 0.1349\n",
      "Epoch 85 completed with average loss: 0.1349\n",
      "Epoch [86/1000], Step [0/1], Loss: 0.1270\n",
      "Epoch 86 completed with average loss: 0.1270\n",
      "Epoch [87/1000], Step [0/1], Loss: 0.1195\n",
      "Epoch 87 completed with average loss: 0.1195\n",
      "Epoch [88/1000], Step [0/1], Loss: 0.1124\n",
      "Epoch 88 completed with average loss: 0.1124\n",
      "Epoch [89/1000], Step [0/1], Loss: 0.1056\n",
      "Epoch 89 completed with average loss: 0.1056\n",
      "Epoch [90/1000], Step [0/1], Loss: 0.0992\n",
      "Epoch 90 completed with average loss: 0.0992\n",
      "Epoch [91/1000], Step [0/1], Loss: 0.0931\n",
      "Epoch 91 completed with average loss: 0.0931\n",
      "Epoch [92/1000], Step [0/1], Loss: 0.0874\n",
      "Epoch 92 completed with average loss: 0.0874\n",
      "Epoch [93/1000], Step [0/1], Loss: 0.0820\n",
      "Epoch 93 completed with average loss: 0.0820\n",
      "Epoch [94/1000], Step [0/1], Loss: 0.0769\n",
      "Epoch 94 completed with average loss: 0.0769\n",
      "Epoch [95/1000], Step [0/1], Loss: 0.0720\n",
      "Epoch 95 completed with average loss: 0.0720\n",
      "Epoch [96/1000], Step [0/1], Loss: 0.0675\n",
      "Epoch 96 completed with average loss: 0.0675\n",
      "Epoch [97/1000], Step [0/1], Loss: 0.0633\n",
      "Epoch 97 completed with average loss: 0.0633\n",
      "Epoch [98/1000], Step [0/1], Loss: 0.0593\n",
      "Epoch 98 completed with average loss: 0.0593\n",
      "Epoch [99/1000], Step [0/1], Loss: 0.0556\n",
      "Epoch 99 completed with average loss: 0.0556\n",
      "Epoch [100/1000], Step [0/1], Loss: 0.0523\n",
      "Epoch 100 completed with average loss: 0.0523\n",
      "Epoch [101/1000], Step [0/1], Loss: 0.0491\n",
      "Epoch 101 completed with average loss: 0.0491\n",
      "Epoch [102/1000], Step [0/1], Loss: 0.0461\n",
      "Epoch 102 completed with average loss: 0.0461\n",
      "Epoch [103/1000], Step [0/1], Loss: 0.0434\n",
      "Epoch 103 completed with average loss: 0.0434\n",
      "Epoch [104/1000], Step [0/1], Loss: 0.0408\n",
      "Epoch 104 completed with average loss: 0.0408\n",
      "Epoch [105/1000], Step [0/1], Loss: 0.0385\n",
      "Epoch 105 completed with average loss: 0.0385\n",
      "Epoch [106/1000], Step [0/1], Loss: 0.0364\n",
      "Epoch 106 completed with average loss: 0.0364\n",
      "Epoch [107/1000], Step [0/1], Loss: 0.0345\n",
      "Epoch 107 completed with average loss: 0.0345\n",
      "Epoch [108/1000], Step [0/1], Loss: 0.0330\n",
      "Epoch 108 completed with average loss: 0.0330\n",
      "Epoch [109/1000], Step [0/1], Loss: 0.0312\n",
      "Epoch 109 completed with average loss: 0.0312\n",
      "Epoch [110/1000], Step [0/1], Loss: 0.0296\n",
      "Epoch 110 completed with average loss: 0.0296\n",
      "Epoch [111/1000], Step [0/1], Loss: 0.0281\n",
      "Epoch 111 completed with average loss: 0.0281\n",
      "Epoch [112/1000], Step [0/1], Loss: 0.0266\n",
      "Epoch 112 completed with average loss: 0.0266\n",
      "Epoch [113/1000], Step [0/1], Loss: 0.0253\n",
      "Epoch 113 completed with average loss: 0.0253\n",
      "Epoch [114/1000], Step [0/1], Loss: 0.0242\n",
      "Epoch 114 completed with average loss: 0.0242\n",
      "Epoch [115/1000], Step [0/1], Loss: 0.0231\n",
      "Epoch 115 completed with average loss: 0.0231\n",
      "Epoch [116/1000], Step [0/1], Loss: 0.0222\n",
      "Epoch 116 completed with average loss: 0.0222\n",
      "Epoch [117/1000], Step [0/1], Loss: 0.0213\n",
      "Epoch 117 completed with average loss: 0.0213\n",
      "Epoch [118/1000], Step [0/1], Loss: 0.0206\n",
      "Epoch 118 completed with average loss: 0.0206\n",
      "Epoch [119/1000], Step [0/1], Loss: 0.0199\n",
      "Epoch 119 completed with average loss: 0.0199\n",
      "Epoch [120/1000], Step [0/1], Loss: 0.0192\n",
      "Epoch 120 completed with average loss: 0.0192\n",
      "Epoch [121/1000], Step [0/1], Loss: 0.0187\n",
      "Epoch 121 completed with average loss: 0.0187\n",
      "Epoch [122/1000], Step [0/1], Loss: 0.0182\n",
      "Epoch 122 completed with average loss: 0.0182\n",
      "Epoch [123/1000], Step [0/1], Loss: 0.0178\n",
      "Epoch 123 completed with average loss: 0.0178\n",
      "Epoch [124/1000], Step [0/1], Loss: 0.0174\n",
      "Epoch 124 completed with average loss: 0.0174\n",
      "Epoch [125/1000], Step [0/1], Loss: 0.0171\n",
      "Epoch 125 completed with average loss: 0.0171\n",
      "Epoch [126/1000], Step [0/1], Loss: 0.0168\n",
      "Epoch 126 completed with average loss: 0.0168\n",
      "Epoch [127/1000], Step [0/1], Loss: 0.0166\n",
      "Epoch 127 completed with average loss: 0.0166\n",
      "Epoch [128/1000], Step [0/1], Loss: 0.0163\n",
      "Epoch 128 completed with average loss: 0.0163\n",
      "Epoch [129/1000], Step [0/1], Loss: 0.0161\n",
      "Epoch 129 completed with average loss: 0.0161\n",
      "Epoch [130/1000], Step [0/1], Loss: 0.0159\n",
      "Epoch 130 completed with average loss: 0.0159\n",
      "Epoch [131/1000], Step [0/1], Loss: 0.0158\n",
      "Epoch 131 completed with average loss: 0.0158\n",
      "Epoch [132/1000], Step [0/1], Loss: 0.0156\n",
      "Epoch 132 completed with average loss: 0.0156\n",
      "Epoch [133/1000], Step [0/1], Loss: 0.0155\n",
      "Epoch 133 completed with average loss: 0.0155\n",
      "Epoch [134/1000], Step [0/1], Loss: 0.0153\n",
      "Epoch 134 completed with average loss: 0.0153\n",
      "Epoch [135/1000], Step [0/1], Loss: 0.0152\n",
      "Epoch 135 completed with average loss: 0.0152\n",
      "Epoch [136/1000], Step [0/1], Loss: 0.0151\n",
      "Epoch 136 completed with average loss: 0.0151\n",
      "Epoch [137/1000], Step [0/1], Loss: 0.0150\n",
      "Epoch 137 completed with average loss: 0.0150\n",
      "Epoch [138/1000], Step [0/1], Loss: 0.0149\n",
      "Epoch 138 completed with average loss: 0.0149\n",
      "Epoch [139/1000], Step [0/1], Loss: 0.0148\n",
      "Epoch 139 completed with average loss: 0.0148\n",
      "Epoch [140/1000], Step [0/1], Loss: 0.0147\n",
      "Epoch 140 completed with average loss: 0.0147\n",
      "Epoch [141/1000], Step [0/1], Loss: 0.0146\n",
      "Epoch 141 completed with average loss: 0.0146\n",
      "Epoch [142/1000], Step [0/1], Loss: 0.0145\n",
      "Epoch 142 completed with average loss: 0.0145\n",
      "Epoch [143/1000], Step [0/1], Loss: 0.0144\n",
      "Epoch 143 completed with average loss: 0.0144\n",
      "Epoch [144/1000], Step [0/1], Loss: 0.0143\n",
      "Epoch 144 completed with average loss: 0.0143\n",
      "Epoch [145/1000], Step [0/1], Loss: 0.0142\n",
      "Epoch 145 completed with average loss: 0.0142\n",
      "Epoch [146/1000], Step [0/1], Loss: 0.0141\n",
      "Epoch 146 completed with average loss: 0.0141\n",
      "Epoch [147/1000], Step [0/1], Loss: 0.0140\n",
      "Epoch 147 completed with average loss: 0.0140\n",
      "Epoch [148/1000], Step [0/1], Loss: 0.0139\n",
      "Epoch 148 completed with average loss: 0.0139\n",
      "Epoch [149/1000], Step [0/1], Loss: 0.0138\n",
      "Epoch 149 completed with average loss: 0.0138\n",
      "Epoch [150/1000], Step [0/1], Loss: 0.0137\n",
      "Epoch 150 completed with average loss: 0.0137\n",
      "Epoch [151/1000], Step [0/1], Loss: 0.0137\n",
      "Epoch 151 completed with average loss: 0.0137\n",
      "Epoch [152/1000], Step [0/1], Loss: 0.0136\n",
      "Epoch 152 completed with average loss: 0.0136\n",
      "Epoch [153/1000], Step [0/1], Loss: 0.0135\n",
      "Epoch 153 completed with average loss: 0.0135\n",
      "Epoch [154/1000], Step [0/1], Loss: 0.0134\n",
      "Epoch 154 completed with average loss: 0.0134\n",
      "Epoch [155/1000], Step [0/1], Loss: 0.0134\n",
      "Epoch 155 completed with average loss: 0.0134\n",
      "Epoch [156/1000], Step [0/1], Loss: 0.0133\n",
      "Epoch 156 completed with average loss: 0.0133\n",
      "Epoch [157/1000], Step [0/1], Loss: 0.0132\n",
      "Epoch 157 completed with average loss: 0.0132\n",
      "Epoch [158/1000], Step [0/1], Loss: 0.0132\n",
      "Epoch 158 completed with average loss: 0.0132\n",
      "Epoch [159/1000], Step [0/1], Loss: 0.0131\n",
      "Epoch 159 completed with average loss: 0.0131\n",
      "Epoch [160/1000], Step [0/1], Loss: 0.0131\n",
      "Epoch 160 completed with average loss: 0.0131\n",
      "Epoch [161/1000], Step [0/1], Loss: 0.0130\n",
      "Epoch 161 completed with average loss: 0.0130\n",
      "Epoch [162/1000], Step [0/1], Loss: 0.0129\n",
      "Epoch 162 completed with average loss: 0.0129\n",
      "Epoch [163/1000], Step [0/1], Loss: 0.0129\n",
      "Epoch 163 completed with average loss: 0.0129\n",
      "Epoch [164/1000], Step [0/1], Loss: 0.0128\n",
      "Epoch 164 completed with average loss: 0.0128\n",
      "Epoch [165/1000], Step [0/1], Loss: 0.0128\n",
      "Epoch 165 completed with average loss: 0.0128\n",
      "Epoch [166/1000], Step [0/1], Loss: 0.0127\n",
      "Epoch 166 completed with average loss: 0.0127\n",
      "Epoch [167/1000], Step [0/1], Loss: 0.0126\n",
      "Epoch 167 completed with average loss: 0.0126\n",
      "Epoch [168/1000], Step [0/1], Loss: 0.0126\n",
      "Epoch 168 completed with average loss: 0.0126\n",
      "Epoch [169/1000], Step [0/1], Loss: 0.0125\n",
      "Epoch 169 completed with average loss: 0.0125\n",
      "Epoch [170/1000], Step [0/1], Loss: 0.0125\n",
      "Epoch 170 completed with average loss: 0.0125\n",
      "Epoch [171/1000], Step [0/1], Loss: 0.0124\n",
      "Epoch 171 completed with average loss: 0.0124\n",
      "Epoch [172/1000], Step [0/1], Loss: 0.0124\n",
      "Epoch 172 completed with average loss: 0.0124\n",
      "Epoch [173/1000], Step [0/1], Loss: 0.0123\n",
      "Epoch 173 completed with average loss: 0.0123\n",
      "Epoch [174/1000], Step [0/1], Loss: 0.0122\n",
      "Epoch 174 completed with average loss: 0.0122\n",
      "Epoch [175/1000], Step [0/1], Loss: 0.0122\n",
      "Epoch 175 completed with average loss: 0.0122\n",
      "Epoch [176/1000], Step [0/1], Loss: 0.0121\n",
      "Epoch 176 completed with average loss: 0.0121\n",
      "Epoch [177/1000], Step [0/1], Loss: 0.0121\n",
      "Epoch 177 completed with average loss: 0.0121\n",
      "Epoch [178/1000], Step [0/1], Loss: 0.0120\n",
      "Epoch 178 completed with average loss: 0.0120\n",
      "Epoch [179/1000], Step [0/1], Loss: 0.0120\n",
      "Epoch 179 completed with average loss: 0.0120\n",
      "Epoch [180/1000], Step [0/1], Loss: 0.0119\n",
      "Epoch 180 completed with average loss: 0.0119\n",
      "Epoch [181/1000], Step [0/1], Loss: 0.0119\n",
      "Epoch 181 completed with average loss: 0.0119\n",
      "Epoch [182/1000], Step [0/1], Loss: 0.0118\n",
      "Epoch 182 completed with average loss: 0.0118\n",
      "Epoch [183/1000], Step [0/1], Loss: 0.0118\n",
      "Epoch 183 completed with average loss: 0.0118\n",
      "Epoch [184/1000], Step [0/1], Loss: 0.0117\n",
      "Epoch 184 completed with average loss: 0.0117\n",
      "Epoch [185/1000], Step [0/1], Loss: 0.0116\n",
      "Epoch 185 completed with average loss: 0.0116\n",
      "Epoch [186/1000], Step [0/1], Loss: 0.0116\n",
      "Epoch 186 completed with average loss: 0.0116\n",
      "Epoch [187/1000], Step [0/1], Loss: 0.0115\n",
      "Epoch 187 completed with average loss: 0.0115\n",
      "Epoch [188/1000], Step [0/1], Loss: 0.0115\n",
      "Epoch 188 completed with average loss: 0.0115\n",
      "Epoch [189/1000], Step [0/1], Loss: 0.0114\n",
      "Epoch 189 completed with average loss: 0.0114\n",
      "Epoch [190/1000], Step [0/1], Loss: 0.0114\n",
      "Epoch 190 completed with average loss: 0.0114\n",
      "Epoch [191/1000], Step [0/1], Loss: 0.0113\n",
      "Epoch 191 completed with average loss: 0.0113\n",
      "Epoch [192/1000], Step [0/1], Loss: 0.0113\n",
      "Epoch 192 completed with average loss: 0.0113\n",
      "Epoch [193/1000], Step [0/1], Loss: 0.0112\n",
      "Epoch 193 completed with average loss: 0.0112\n",
      "Epoch [194/1000], Step [0/1], Loss: 0.0112\n",
      "Epoch 194 completed with average loss: 0.0112\n",
      "Epoch [195/1000], Step [0/1], Loss: 0.0111\n",
      "Epoch 195 completed with average loss: 0.0111\n",
      "Epoch [196/1000], Step [0/1], Loss: 0.0111\n",
      "Epoch 196 completed with average loss: 0.0111\n",
      "Epoch [197/1000], Step [0/1], Loss: 0.0110\n",
      "Epoch 197 completed with average loss: 0.0110\n",
      "Epoch [198/1000], Step [0/1], Loss: 0.0110\n",
      "Epoch 198 completed with average loss: 0.0110\n",
      "Epoch [199/1000], Step [0/1], Loss: 0.0109\n",
      "Epoch 199 completed with average loss: 0.0109\n",
      "Epoch [200/1000], Step [0/1], Loss: 0.0108\n",
      "Epoch 200 completed with average loss: 0.0108\n",
      "Epoch [201/1000], Step [0/1], Loss: 0.0108\n",
      "Epoch 201 completed with average loss: 0.0108\n",
      "Epoch [202/1000], Step [0/1], Loss: 0.0107\n",
      "Epoch 202 completed with average loss: 0.0107\n",
      "Epoch [203/1000], Step [0/1], Loss: 0.0107\n",
      "Epoch 203 completed with average loss: 0.0107\n",
      "Epoch [204/1000], Step [0/1], Loss: 0.0106\n",
      "Epoch 204 completed with average loss: 0.0106\n",
      "Epoch [205/1000], Step [0/1], Loss: 0.0106\n",
      "Epoch 205 completed with average loss: 0.0106\n",
      "Epoch [206/1000], Step [0/1], Loss: 0.0105\n",
      "Epoch 206 completed with average loss: 0.0105\n",
      "Epoch [207/1000], Step [0/1], Loss: 0.0105\n",
      "Epoch 207 completed with average loss: 0.0105\n",
      "Epoch [208/1000], Step [0/1], Loss: 0.0104\n",
      "Epoch 208 completed with average loss: 0.0104\n",
      "Epoch [209/1000], Step [0/1], Loss: 0.0104\n",
      "Epoch 209 completed with average loss: 0.0104\n",
      "Epoch [210/1000], Step [0/1], Loss: 0.0103\n",
      "Epoch 210 completed with average loss: 0.0103\n",
      "Epoch [211/1000], Step [0/1], Loss: 0.0103\n",
      "Epoch 211 completed with average loss: 0.0103\n",
      "Epoch [212/1000], Step [0/1], Loss: 0.0102\n",
      "Epoch 212 completed with average loss: 0.0102\n",
      "Epoch [213/1000], Step [0/1], Loss: 0.0102\n",
      "Epoch 213 completed with average loss: 0.0102\n",
      "Epoch [214/1000], Step [0/1], Loss: 0.0102\n",
      "Epoch 214 completed with average loss: 0.0102\n",
      "Epoch [215/1000], Step [0/1], Loss: 0.0101\n",
      "Epoch 215 completed with average loss: 0.0101\n",
      "Epoch [216/1000], Step [0/1], Loss: 0.0101\n",
      "Epoch 216 completed with average loss: 0.0101\n",
      "Epoch [217/1000], Step [0/1], Loss: 0.0100\n",
      "Epoch 217 completed with average loss: 0.0100\n",
      "Epoch [218/1000], Step [0/1], Loss: 0.0100\n",
      "Epoch 218 completed with average loss: 0.0100\n",
      "Epoch [219/1000], Step [0/1], Loss: 0.0099\n",
      "Epoch 219 completed with average loss: 0.0099\n",
      "Epoch [220/1000], Step [0/1], Loss: 0.0099\n",
      "Epoch 220 completed with average loss: 0.0099\n",
      "Epoch [221/1000], Step [0/1], Loss: 0.0098\n",
      "Epoch 221 completed with average loss: 0.0098\n",
      "Epoch [222/1000], Step [0/1], Loss: 0.0098\n",
      "Epoch 222 completed with average loss: 0.0098\n",
      "Epoch [223/1000], Step [0/1], Loss: 0.0097\n",
      "Epoch 223 completed with average loss: 0.0097\n",
      "Epoch [224/1000], Step [0/1], Loss: 0.0097\n",
      "Epoch 224 completed with average loss: 0.0097\n",
      "Epoch [225/1000], Step [0/1], Loss: 0.0097\n",
      "Epoch 225 completed with average loss: 0.0097\n",
      "Epoch [226/1000], Step [0/1], Loss: 0.0096\n",
      "Epoch 226 completed with average loss: 0.0096\n",
      "Epoch [227/1000], Step [0/1], Loss: 0.0096\n",
      "Epoch 227 completed with average loss: 0.0096\n",
      "Epoch [228/1000], Step [0/1], Loss: 0.0095\n",
      "Epoch 228 completed with average loss: 0.0095\n",
      "Epoch [229/1000], Step [0/1], Loss: 0.0094\n",
      "Epoch 229 completed with average loss: 0.0094\n",
      "Epoch [230/1000], Step [0/1], Loss: 0.0094\n",
      "Epoch 230 completed with average loss: 0.0094\n",
      "Epoch [231/1000], Step [0/1], Loss: 0.0093\n",
      "Epoch 231 completed with average loss: 0.0093\n",
      "Epoch [232/1000], Step [0/1], Loss: 0.0093\n",
      "Epoch 232 completed with average loss: 0.0093\n",
      "Epoch [233/1000], Step [0/1], Loss: 0.0092\n",
      "Epoch 233 completed with average loss: 0.0092\n",
      "Epoch [234/1000], Step [0/1], Loss: 0.0092\n",
      "Epoch 234 completed with average loss: 0.0092\n",
      "Epoch [235/1000], Step [0/1], Loss: 0.0092\n",
      "Epoch 235 completed with average loss: 0.0092\n",
      "Epoch [236/1000], Step [0/1], Loss: 0.0091\n",
      "Epoch 236 completed with average loss: 0.0091\n",
      "Epoch [237/1000], Step [0/1], Loss: 0.0090\n",
      "Epoch 237 completed with average loss: 0.0090\n",
      "Epoch [238/1000], Step [0/1], Loss: 0.0090\n",
      "Epoch 238 completed with average loss: 0.0090\n",
      "Epoch [239/1000], Step [0/1], Loss: 0.0090\n",
      "Epoch 239 completed with average loss: 0.0090\n",
      "Epoch [240/1000], Step [0/1], Loss: 0.0089\n",
      "Epoch 240 completed with average loss: 0.0089\n",
      "Epoch [241/1000], Step [0/1], Loss: 0.0089\n",
      "Epoch 241 completed with average loss: 0.0089\n",
      "Epoch [242/1000], Step [0/1], Loss: 0.0088\n",
      "Epoch 242 completed with average loss: 0.0088\n",
      "Epoch [243/1000], Step [0/1], Loss: 0.0088\n",
      "Epoch 243 completed with average loss: 0.0088\n",
      "Epoch [244/1000], Step [0/1], Loss: 0.0087\n",
      "Epoch 244 completed with average loss: 0.0087\n",
      "Epoch [245/1000], Step [0/1], Loss: 0.0087\n",
      "Epoch 245 completed with average loss: 0.0087\n",
      "Epoch [246/1000], Step [0/1], Loss: 0.0086\n",
      "Epoch 246 completed with average loss: 0.0086\n",
      "Epoch [247/1000], Step [0/1], Loss: 0.0086\n",
      "Epoch 247 completed with average loss: 0.0086\n",
      "Epoch [248/1000], Step [0/1], Loss: 0.0085\n",
      "Epoch 248 completed with average loss: 0.0085\n",
      "Epoch [249/1000], Step [0/1], Loss: 0.0085\n",
      "Epoch 249 completed with average loss: 0.0085\n",
      "Epoch [250/1000], Step [0/1], Loss: 0.0084\n",
      "Epoch 250 completed with average loss: 0.0084\n",
      "Epoch [251/1000], Step [0/1], Loss: 0.0084\n",
      "Epoch 251 completed with average loss: 0.0084\n",
      "Epoch [252/1000], Step [0/1], Loss: 0.0084\n",
      "Epoch 252 completed with average loss: 0.0084\n",
      "Epoch [253/1000], Step [0/1], Loss: 0.0083\n",
      "Epoch 253 completed with average loss: 0.0083\n",
      "Epoch [254/1000], Step [0/1], Loss: 0.0083\n",
      "Epoch 254 completed with average loss: 0.0083\n",
      "Epoch [255/1000], Step [0/1], Loss: 0.0082\n",
      "Epoch 255 completed with average loss: 0.0082\n",
      "Epoch [256/1000], Step [0/1], Loss: 0.0082\n",
      "Epoch 256 completed with average loss: 0.0082\n",
      "Epoch [257/1000], Step [0/1], Loss: 0.0081\n",
      "Epoch 257 completed with average loss: 0.0081\n",
      "Epoch [258/1000], Step [0/1], Loss: 0.0081\n",
      "Epoch 258 completed with average loss: 0.0081\n",
      "Epoch [259/1000], Step [0/1], Loss: 0.0080\n",
      "Epoch 259 completed with average loss: 0.0080\n",
      "Epoch [260/1000], Step [0/1], Loss: 0.0080\n",
      "Epoch 260 completed with average loss: 0.0080\n",
      "Epoch [261/1000], Step [0/1], Loss: 0.0080\n",
      "Epoch 261 completed with average loss: 0.0080\n",
      "Epoch [262/1000], Step [0/1], Loss: 0.0079\n",
      "Epoch 262 completed with average loss: 0.0079\n",
      "Epoch [263/1000], Step [0/1], Loss: 0.0079\n",
      "Epoch 263 completed with average loss: 0.0079\n",
      "Epoch [264/1000], Step [0/1], Loss: 0.0078\n",
      "Epoch 264 completed with average loss: 0.0078\n",
      "Epoch [265/1000], Step [0/1], Loss: 0.0078\n",
      "Epoch 265 completed with average loss: 0.0078\n",
      "Epoch [266/1000], Step [0/1], Loss: 0.0078\n",
      "Epoch 266 completed with average loss: 0.0078\n",
      "Epoch [267/1000], Step [0/1], Loss: 0.0077\n",
      "Epoch 267 completed with average loss: 0.0077\n",
      "Epoch [268/1000], Step [0/1], Loss: 0.0077\n",
      "Epoch 268 completed with average loss: 0.0077\n",
      "Epoch [269/1000], Step [0/1], Loss: 0.0076\n",
      "Epoch 269 completed with average loss: 0.0076\n",
      "Epoch [270/1000], Step [0/1], Loss: 0.0076\n",
      "Epoch 270 completed with average loss: 0.0076\n",
      "Epoch [271/1000], Step [0/1], Loss: 0.0076\n",
      "Epoch 271 completed with average loss: 0.0076\n",
      "Epoch [272/1000], Step [0/1], Loss: 0.0075\n",
      "Epoch 272 completed with average loss: 0.0075\n",
      "Epoch [273/1000], Step [0/1], Loss: 0.0075\n",
      "Epoch 273 completed with average loss: 0.0075\n",
      "Epoch [274/1000], Step [0/1], Loss: 0.0075\n",
      "Epoch 274 completed with average loss: 0.0075\n",
      "Epoch [275/1000], Step [0/1], Loss: 0.0074\n",
      "Epoch 275 completed with average loss: 0.0074\n",
      "Epoch [276/1000], Step [0/1], Loss: 0.0074\n",
      "Epoch 276 completed with average loss: 0.0074\n",
      "Epoch [277/1000], Step [0/1], Loss: 0.0074\n",
      "Epoch 277 completed with average loss: 0.0074\n",
      "Epoch [278/1000], Step [0/1], Loss: 0.0073\n",
      "Epoch 278 completed with average loss: 0.0073\n",
      "Epoch [279/1000], Step [0/1], Loss: 0.0073\n",
      "Epoch 279 completed with average loss: 0.0073\n",
      "Epoch [280/1000], Step [0/1], Loss: 0.0073\n",
      "Epoch 280 completed with average loss: 0.0073\n",
      "Epoch [281/1000], Step [0/1], Loss: 0.0072\n",
      "Epoch 281 completed with average loss: 0.0072\n",
      "Epoch [282/1000], Step [0/1], Loss: 0.0072\n",
      "Epoch 282 completed with average loss: 0.0072\n",
      "Epoch [283/1000], Step [0/1], Loss: 0.0072\n",
      "Epoch 283 completed with average loss: 0.0072\n",
      "Epoch [284/1000], Step [0/1], Loss: 0.0071\n",
      "Epoch 284 completed with average loss: 0.0071\n",
      "Epoch [285/1000], Step [0/1], Loss: 0.0071\n",
      "Epoch 285 completed with average loss: 0.0071\n",
      "Epoch [286/1000], Step [0/1], Loss: 0.0071\n",
      "Epoch 286 completed with average loss: 0.0071\n",
      "Epoch [287/1000], Step [0/1], Loss: 0.0070\n",
      "Epoch 287 completed with average loss: 0.0070\n",
      "Epoch [288/1000], Step [0/1], Loss: 0.0070\n",
      "Epoch 288 completed with average loss: 0.0070\n",
      "Epoch [289/1000], Step [0/1], Loss: 0.0070\n",
      "Epoch 289 completed with average loss: 0.0070\n",
      "Epoch [290/1000], Step [0/1], Loss: 0.0069\n",
      "Epoch 290 completed with average loss: 0.0069\n",
      "Epoch [291/1000], Step [0/1], Loss: 0.0069\n",
      "Epoch 291 completed with average loss: 0.0069\n",
      "Epoch [292/1000], Step [0/1], Loss: 0.0069\n",
      "Epoch 292 completed with average loss: 0.0069\n",
      "Epoch [293/1000], Step [0/1], Loss: 0.0068\n",
      "Epoch 293 completed with average loss: 0.0068\n",
      "Epoch [294/1000], Step [0/1], Loss: 0.0068\n",
      "Epoch 294 completed with average loss: 0.0068\n",
      "Epoch [295/1000], Step [0/1], Loss: 0.0068\n",
      "Epoch 295 completed with average loss: 0.0068\n",
      "Epoch [296/1000], Step [0/1], Loss: 0.0068\n",
      "Epoch 296 completed with average loss: 0.0068\n",
      "Epoch [297/1000], Step [0/1], Loss: 0.0067\n",
      "Epoch 297 completed with average loss: 0.0067\n",
      "Epoch [298/1000], Step [0/1], Loss: 0.0067\n",
      "Epoch 298 completed with average loss: 0.0067\n",
      "Epoch [299/1000], Step [0/1], Loss: 0.0067\n",
      "Epoch 299 completed with average loss: 0.0067\n",
      "Epoch [300/1000], Step [0/1], Loss: 0.0066\n",
      "Epoch 300 completed with average loss: 0.0066\n",
      "Epoch [301/1000], Step [0/1], Loss: 0.0066\n",
      "Epoch 301 completed with average loss: 0.0066\n",
      "Epoch [302/1000], Step [0/1], Loss: 0.0066\n",
      "Epoch 302 completed with average loss: 0.0066\n",
      "Epoch [303/1000], Step [0/1], Loss: 0.0066\n",
      "Epoch 303 completed with average loss: 0.0066\n",
      "Epoch [304/1000], Step [0/1], Loss: 0.0065\n",
      "Epoch 304 completed with average loss: 0.0065\n",
      "Epoch [305/1000], Step [0/1], Loss: 0.0065\n",
      "Epoch 305 completed with average loss: 0.0065\n",
      "Epoch [306/1000], Step [0/1], Loss: 0.0065\n",
      "Epoch 306 completed with average loss: 0.0065\n",
      "Epoch [307/1000], Step [0/1], Loss: 0.0065\n",
      "Epoch 307 completed with average loss: 0.0065\n",
      "Epoch [308/1000], Step [0/1], Loss: 0.0064\n",
      "Epoch 308 completed with average loss: 0.0064\n",
      "Epoch [309/1000], Step [0/1], Loss: 0.0064\n",
      "Epoch 309 completed with average loss: 0.0064\n",
      "Epoch [310/1000], Step [0/1], Loss: 0.0064\n",
      "Epoch 310 completed with average loss: 0.0064\n",
      "Epoch [311/1000], Step [0/1], Loss: 0.0064\n",
      "Epoch 311 completed with average loss: 0.0064\n",
      "Epoch [312/1000], Step [0/1], Loss: 0.0063\n",
      "Epoch 312 completed with average loss: 0.0063\n",
      "Epoch [313/1000], Step [0/1], Loss: 0.0063\n",
      "Epoch 313 completed with average loss: 0.0063\n",
      "Epoch [314/1000], Step [0/1], Loss: 0.0063\n",
      "Epoch 314 completed with average loss: 0.0063\n",
      "Epoch [315/1000], Step [0/1], Loss: 0.0063\n",
      "Epoch 315 completed with average loss: 0.0063\n",
      "Epoch [316/1000], Step [0/1], Loss: 0.0062\n",
      "Epoch 316 completed with average loss: 0.0062\n",
      "Epoch [317/1000], Step [0/1], Loss: 0.0062\n",
      "Epoch 317 completed with average loss: 0.0062\n",
      "Epoch [318/1000], Step [0/1], Loss: 0.0062\n",
      "Epoch 318 completed with average loss: 0.0062\n",
      "Epoch [319/1000], Step [0/1], Loss: 0.0062\n",
      "Epoch 319 completed with average loss: 0.0062\n",
      "Epoch [320/1000], Step [0/1], Loss: 0.0061\n",
      "Epoch 320 completed with average loss: 0.0061\n",
      "Epoch [321/1000], Step [0/1], Loss: 0.0061\n",
      "Epoch 321 completed with average loss: 0.0061\n",
      "Epoch [322/1000], Step [0/1], Loss: 0.0061\n",
      "Epoch 322 completed with average loss: 0.0061\n",
      "Epoch [323/1000], Step [0/1], Loss: 0.0061\n",
      "Epoch 323 completed with average loss: 0.0061\n",
      "Epoch [324/1000], Step [0/1], Loss: 0.0060\n",
      "Epoch 324 completed with average loss: 0.0060\n",
      "Epoch [325/1000], Step [0/1], Loss: 0.0060\n",
      "Epoch 325 completed with average loss: 0.0060\n",
      "Epoch [326/1000], Step [0/1], Loss: 0.0060\n",
      "Epoch 326 completed with average loss: 0.0060\n",
      "Epoch [327/1000], Step [0/1], Loss: 0.0060\n",
      "Epoch 327 completed with average loss: 0.0060\n",
      "Epoch [328/1000], Step [0/1], Loss: 0.0060\n",
      "Epoch 328 completed with average loss: 0.0060\n",
      "Epoch [329/1000], Step [0/1], Loss: 0.0059\n",
      "Epoch 329 completed with average loss: 0.0059\n",
      "Epoch [330/1000], Step [0/1], Loss: 0.0059\n",
      "Epoch 330 completed with average loss: 0.0059\n",
      "Epoch [331/1000], Step [0/1], Loss: 0.0059\n",
      "Epoch 331 completed with average loss: 0.0059\n",
      "Epoch [332/1000], Step [0/1], Loss: 0.0059\n",
      "Epoch 332 completed with average loss: 0.0059\n",
      "Epoch [333/1000], Step [0/1], Loss: 0.0058\n",
      "Epoch 333 completed with average loss: 0.0058\n",
      "Epoch [334/1000], Step [0/1], Loss: 0.0058\n",
      "Epoch 334 completed with average loss: 0.0058\n",
      "Epoch [335/1000], Step [0/1], Loss: 0.0058\n",
      "Epoch 335 completed with average loss: 0.0058\n",
      "Epoch [336/1000], Step [0/1], Loss: 0.0058\n",
      "Epoch 336 completed with average loss: 0.0058\n",
      "Epoch [337/1000], Step [0/1], Loss: 0.0058\n",
      "Epoch 337 completed with average loss: 0.0058\n",
      "Epoch [338/1000], Step [0/1], Loss: 0.0057\n",
      "Epoch 338 completed with average loss: 0.0057\n",
      "Epoch [339/1000], Step [0/1], Loss: 0.0057\n",
      "Epoch 339 completed with average loss: 0.0057\n",
      "Epoch [340/1000], Step [0/1], Loss: 0.0057\n",
      "Epoch 340 completed with average loss: 0.0057\n",
      "Epoch [341/1000], Step [0/1], Loss: 0.0057\n",
      "Epoch 341 completed with average loss: 0.0057\n",
      "Epoch [342/1000], Step [0/1], Loss: 0.0057\n",
      "Epoch 342 completed with average loss: 0.0057\n",
      "Epoch [343/1000], Step [0/1], Loss: 0.0056\n",
      "Epoch 343 completed with average loss: 0.0056\n",
      "Epoch [344/1000], Step [0/1], Loss: 0.0056\n",
      "Epoch 344 completed with average loss: 0.0056\n",
      "Epoch [345/1000], Step [0/1], Loss: 0.0056\n",
      "Epoch 345 completed with average loss: 0.0056\n",
      "Epoch [346/1000], Step [0/1], Loss: 0.0056\n",
      "Epoch 346 completed with average loss: 0.0056\n",
      "Epoch [347/1000], Step [0/1], Loss: 0.0055\n",
      "Epoch 347 completed with average loss: 0.0055\n",
      "Epoch [348/1000], Step [0/1], Loss: 0.0055\n",
      "Epoch 348 completed with average loss: 0.0055\n",
      "Epoch [349/1000], Step [0/1], Loss: 0.0055\n",
      "Epoch 349 completed with average loss: 0.0055\n",
      "Epoch [350/1000], Step [0/1], Loss: 0.0055\n",
      "Epoch 350 completed with average loss: 0.0055\n",
      "Epoch [351/1000], Step [0/1], Loss: 0.0055\n",
      "Epoch 351 completed with average loss: 0.0055\n",
      "Epoch [352/1000], Step [0/1], Loss: 0.0055\n",
      "Epoch 352 completed with average loss: 0.0055\n",
      "Epoch [353/1000], Step [0/1], Loss: 0.0054\n",
      "Epoch 353 completed with average loss: 0.0054\n",
      "Epoch [354/1000], Step [0/1], Loss: 0.0054\n",
      "Epoch 354 completed with average loss: 0.0054\n",
      "Epoch [355/1000], Step [0/1], Loss: 0.0054\n",
      "Epoch 355 completed with average loss: 0.0054\n",
      "Epoch [356/1000], Step [0/1], Loss: 0.0054\n",
      "Epoch 356 completed with average loss: 0.0054\n",
      "Epoch [357/1000], Step [0/1], Loss: 0.0054\n",
      "Epoch 357 completed with average loss: 0.0054\n",
      "Epoch [358/1000], Step [0/1], Loss: 0.0053\n",
      "Epoch 358 completed with average loss: 0.0053\n",
      "Epoch [359/1000], Step [0/1], Loss: 0.0053\n",
      "Epoch 359 completed with average loss: 0.0053\n",
      "Epoch [360/1000], Step [0/1], Loss: 0.0053\n",
      "Epoch 360 completed with average loss: 0.0053\n",
      "Epoch [361/1000], Step [0/1], Loss: 0.0053\n",
      "Epoch 361 completed with average loss: 0.0053\n",
      "Epoch [362/1000], Step [0/1], Loss: 0.0053\n",
      "Epoch 362 completed with average loss: 0.0053\n",
      "Epoch [363/1000], Step [0/1], Loss: 0.0052\n",
      "Epoch 363 completed with average loss: 0.0052\n",
      "Epoch [364/1000], Step [0/1], Loss: 0.0052\n",
      "Epoch 364 completed with average loss: 0.0052\n",
      "Epoch [365/1000], Step [0/1], Loss: 0.0052\n",
      "Epoch 365 completed with average loss: 0.0052\n",
      "Epoch [366/1000], Step [0/1], Loss: 0.0052\n",
      "Epoch 366 completed with average loss: 0.0052\n",
      "Epoch [367/1000], Step [0/1], Loss: 0.0052\n",
      "Epoch 367 completed with average loss: 0.0052\n",
      "Epoch [368/1000], Step [0/1], Loss: 0.0052\n",
      "Epoch 368 completed with average loss: 0.0052\n",
      "Epoch [369/1000], Step [0/1], Loss: 0.0051\n",
      "Epoch 369 completed with average loss: 0.0051\n",
      "Epoch [370/1000], Step [0/1], Loss: 0.0051\n",
      "Epoch 370 completed with average loss: 0.0051\n",
      "Epoch [371/1000], Step [0/1], Loss: 0.0051\n",
      "Epoch 371 completed with average loss: 0.0051\n",
      "Epoch [372/1000], Step [0/1], Loss: 0.0051\n",
      "Epoch 372 completed with average loss: 0.0051\n",
      "Epoch [373/1000], Step [0/1], Loss: 0.0051\n",
      "Epoch 373 completed with average loss: 0.0051\n",
      "Epoch [374/1000], Step [0/1], Loss: 0.0051\n",
      "Epoch 374 completed with average loss: 0.0051\n",
      "Epoch [375/1000], Step [0/1], Loss: 0.0050\n",
      "Epoch 375 completed with average loss: 0.0050\n",
      "Epoch [376/1000], Step [0/1], Loss: 0.0050\n",
      "Epoch 376 completed with average loss: 0.0050\n",
      "Epoch [377/1000], Step [0/1], Loss: 0.0050\n",
      "Epoch 377 completed with average loss: 0.0050\n",
      "Epoch [378/1000], Step [0/1], Loss: 0.0050\n",
      "Epoch 378 completed with average loss: 0.0050\n",
      "Epoch [379/1000], Step [0/1], Loss: 0.0050\n",
      "Epoch 379 completed with average loss: 0.0050\n",
      "Epoch [380/1000], Step [0/1], Loss: 0.0050\n",
      "Epoch 380 completed with average loss: 0.0050\n",
      "Epoch [381/1000], Step [0/1], Loss: 0.0049\n",
      "Epoch 381 completed with average loss: 0.0049\n",
      "Epoch [382/1000], Step [0/1], Loss: 0.0049\n",
      "Epoch 382 completed with average loss: 0.0049\n",
      "Epoch [383/1000], Step [0/1], Loss: 0.0049\n",
      "Epoch 383 completed with average loss: 0.0049\n",
      "Epoch [384/1000], Step [0/1], Loss: 0.0049\n",
      "Epoch 384 completed with average loss: 0.0049\n",
      "Epoch [385/1000], Step [0/1], Loss: 0.0049\n",
      "Epoch 385 completed with average loss: 0.0049\n",
      "Epoch [386/1000], Step [0/1], Loss: 0.0049\n",
      "Epoch 386 completed with average loss: 0.0049\n",
      "Epoch [387/1000], Step [0/1], Loss: 0.0049\n",
      "Epoch 387 completed with average loss: 0.0049\n",
      "Epoch [388/1000], Step [0/1], Loss: 0.0048\n",
      "Epoch 388 completed with average loss: 0.0048\n",
      "Epoch [389/1000], Step [0/1], Loss: 0.0048\n",
      "Epoch 389 completed with average loss: 0.0048\n",
      "Epoch [390/1000], Step [0/1], Loss: 0.0048\n",
      "Epoch 390 completed with average loss: 0.0048\n",
      "Epoch [391/1000], Step [0/1], Loss: 0.0048\n",
      "Epoch 391 completed with average loss: 0.0048\n",
      "Epoch [392/1000], Step [0/1], Loss: 0.0048\n",
      "Epoch 392 completed with average loss: 0.0048\n",
      "Epoch [393/1000], Step [0/1], Loss: 0.0048\n",
      "Epoch 393 completed with average loss: 0.0048\n",
      "Epoch [394/1000], Step [0/1], Loss: 0.0048\n",
      "Epoch 394 completed with average loss: 0.0048\n",
      "Epoch [395/1000], Step [0/1], Loss: 0.0047\n",
      "Epoch 395 completed with average loss: 0.0047\n",
      "Epoch [396/1000], Step [0/1], Loss: 0.0047\n",
      "Epoch 396 completed with average loss: 0.0047\n",
      "Epoch [397/1000], Step [0/1], Loss: 0.0047\n",
      "Epoch 397 completed with average loss: 0.0047\n",
      "Epoch [398/1000], Step [0/1], Loss: 0.0047\n",
      "Epoch 398 completed with average loss: 0.0047\n",
      "Epoch [399/1000], Step [0/1], Loss: 0.0047\n",
      "Epoch 399 completed with average loss: 0.0047\n",
      "Epoch [400/1000], Step [0/1], Loss: 0.0047\n",
      "Epoch 400 completed with average loss: 0.0047\n",
      "Epoch [401/1000], Step [0/1], Loss: 0.0047\n",
      "Epoch 401 completed with average loss: 0.0047\n",
      "Epoch [402/1000], Step [0/1], Loss: 0.0046\n",
      "Epoch 402 completed with average loss: 0.0046\n",
      "Epoch [403/1000], Step [0/1], Loss: 0.0046\n",
      "Epoch 403 completed with average loss: 0.0046\n",
      "Epoch [404/1000], Step [0/1], Loss: 0.0046\n",
      "Epoch 404 completed with average loss: 0.0046\n",
      "Epoch [405/1000], Step [0/1], Loss: 0.0046\n",
      "Epoch 405 completed with average loss: 0.0046\n",
      "Epoch [406/1000], Step [0/1], Loss: 0.0046\n",
      "Epoch 406 completed with average loss: 0.0046\n",
      "Epoch [407/1000], Step [0/1], Loss: 0.0046\n",
      "Epoch 407 completed with average loss: 0.0046\n",
      "Epoch [408/1000], Step [0/1], Loss: 0.0046\n",
      "Epoch 408 completed with average loss: 0.0046\n",
      "Epoch [409/1000], Step [0/1], Loss: 0.0046\n",
      "Epoch 409 completed with average loss: 0.0046\n",
      "Epoch [410/1000], Step [0/1], Loss: 0.0045\n",
      "Epoch 410 completed with average loss: 0.0045\n",
      "Epoch [411/1000], Step [0/1], Loss: 0.0045\n",
      "Epoch 411 completed with average loss: 0.0045\n",
      "Epoch [412/1000], Step [0/1], Loss: 0.0045\n",
      "Epoch 412 completed with average loss: 0.0045\n",
      "Epoch [413/1000], Step [0/1], Loss: 0.0045\n",
      "Epoch 413 completed with average loss: 0.0045\n",
      "Epoch [414/1000], Step [0/1], Loss: 0.0045\n",
      "Epoch 414 completed with average loss: 0.0045\n",
      "Epoch [415/1000], Step [0/1], Loss: 0.0045\n",
      "Epoch 415 completed with average loss: 0.0045\n",
      "Epoch [416/1000], Step [0/1], Loss: 0.0045\n",
      "Epoch 416 completed with average loss: 0.0045\n",
      "Epoch [417/1000], Step [0/1], Loss: 0.0045\n",
      "Epoch 417 completed with average loss: 0.0045\n",
      "Epoch [418/1000], Step [0/1], Loss: 0.0044\n",
      "Epoch 418 completed with average loss: 0.0044\n",
      "Epoch [419/1000], Step [0/1], Loss: 0.0044\n",
      "Epoch 419 completed with average loss: 0.0044\n",
      "Epoch [420/1000], Step [0/1], Loss: 0.0044\n",
      "Epoch 420 completed with average loss: 0.0044\n",
      "Epoch [421/1000], Step [0/1], Loss: 0.0044\n",
      "Epoch 421 completed with average loss: 0.0044\n",
      "Epoch [422/1000], Step [0/1], Loss: 0.0044\n",
      "Epoch 422 completed with average loss: 0.0044\n",
      "Epoch [423/1000], Step [0/1], Loss: 0.0044\n",
      "Epoch 423 completed with average loss: 0.0044\n",
      "Epoch [424/1000], Step [0/1], Loss: 0.0044\n",
      "Epoch 424 completed with average loss: 0.0044\n",
      "Epoch [425/1000], Step [0/1], Loss: 0.0044\n",
      "Epoch 425 completed with average loss: 0.0044\n",
      "Epoch [426/1000], Step [0/1], Loss: 0.0044\n",
      "Epoch 426 completed with average loss: 0.0044\n",
      "Epoch [427/1000], Step [0/1], Loss: 0.0043\n",
      "Epoch 427 completed with average loss: 0.0043\n",
      "Epoch [428/1000], Step [0/1], Loss: 0.0043\n",
      "Epoch 428 completed with average loss: 0.0043\n",
      "Epoch [429/1000], Step [0/1], Loss: 0.0043\n",
      "Epoch 429 completed with average loss: 0.0043\n",
      "Epoch [430/1000], Step [0/1], Loss: 0.0043\n",
      "Epoch 430 completed with average loss: 0.0043\n",
      "Epoch [431/1000], Step [0/1], Loss: 0.0043\n",
      "Epoch 431 completed with average loss: 0.0043\n",
      "Epoch [432/1000], Step [0/1], Loss: 0.0043\n",
      "Epoch 432 completed with average loss: 0.0043\n",
      "Epoch [433/1000], Step [0/1], Loss: 0.0043\n",
      "Epoch 433 completed with average loss: 0.0043\n",
      "Epoch [434/1000], Step [0/1], Loss: 0.0043\n",
      "Epoch 434 completed with average loss: 0.0043\n",
      "Epoch [435/1000], Step [0/1], Loss: 0.0043\n",
      "Epoch 435 completed with average loss: 0.0043\n",
      "Epoch [436/1000], Step [0/1], Loss: 0.0042\n",
      "Epoch 436 completed with average loss: 0.0042\n",
      "Epoch [437/1000], Step [0/1], Loss: 0.0042\n",
      "Epoch 437 completed with average loss: 0.0042\n",
      "Epoch [438/1000], Step [0/1], Loss: 0.0042\n",
      "Epoch 438 completed with average loss: 0.0042\n",
      "Epoch [439/1000], Step [0/1], Loss: 0.0042\n",
      "Epoch 439 completed with average loss: 0.0042\n",
      "Epoch [440/1000], Step [0/1], Loss: 0.0042\n",
      "Epoch 440 completed with average loss: 0.0042\n",
      "Epoch [441/1000], Step [0/1], Loss: 0.0042\n",
      "Epoch 441 completed with average loss: 0.0042\n",
      "Epoch [442/1000], Step [0/1], Loss: 0.0042\n",
      "Epoch 442 completed with average loss: 0.0042\n",
      "Epoch [443/1000], Step [0/1], Loss: 0.0042\n",
      "Epoch 443 completed with average loss: 0.0042\n",
      "Epoch [444/1000], Step [0/1], Loss: 0.0042\n",
      "Epoch 444 completed with average loss: 0.0042\n",
      "Epoch [445/1000], Step [0/1], Loss: 0.0041\n",
      "Epoch 445 completed with average loss: 0.0041\n",
      "Epoch [446/1000], Step [0/1], Loss: 0.0041\n",
      "Epoch 446 completed with average loss: 0.0041\n",
      "Epoch [447/1000], Step [0/1], Loss: 0.0041\n",
      "Epoch 447 completed with average loss: 0.0041\n",
      "Epoch [448/1000], Step [0/1], Loss: 0.0041\n",
      "Epoch 448 completed with average loss: 0.0041\n",
      "Epoch [449/1000], Step [0/1], Loss: 0.0041\n",
      "Epoch 449 completed with average loss: 0.0041\n",
      "Epoch [450/1000], Step [0/1], Loss: 0.0041\n",
      "Epoch 450 completed with average loss: 0.0041\n",
      "Epoch [451/1000], Step [0/1], Loss: 0.0041\n",
      "Epoch 451 completed with average loss: 0.0041\n",
      "Epoch [452/1000], Step [0/1], Loss: 0.0041\n",
      "Epoch 452 completed with average loss: 0.0041\n",
      "Epoch [453/1000], Step [0/1], Loss: 0.0041\n",
      "Epoch 453 completed with average loss: 0.0041\n",
      "Epoch [454/1000], Step [0/1], Loss: 0.0040\n",
      "Epoch 454 completed with average loss: 0.0040\n",
      "Epoch [455/1000], Step [0/1], Loss: 0.0040\n",
      "Epoch 455 completed with average loss: 0.0040\n",
      "Epoch [456/1000], Step [0/1], Loss: 0.0040\n",
      "Epoch 456 completed with average loss: 0.0040\n",
      "Epoch [457/1000], Step [0/1], Loss: 0.0040\n",
      "Epoch 457 completed with average loss: 0.0040\n",
      "Epoch [458/1000], Step [0/1], Loss: 0.0040\n",
      "Epoch 458 completed with average loss: 0.0040\n",
      "Epoch [459/1000], Step [0/1], Loss: 0.0040\n",
      "Epoch 459 completed with average loss: 0.0040\n",
      "Epoch [460/1000], Step [0/1], Loss: 0.0039\n",
      "Epoch 460 completed with average loss: 0.0039\n",
      "Epoch [461/1000], Step [0/1], Loss: 0.0039\n",
      "Epoch 461 completed with average loss: 0.0039\n",
      "Epoch [462/1000], Step [0/1], Loss: 0.0039\n",
      "Epoch 462 completed with average loss: 0.0039\n",
      "Epoch [463/1000], Step [0/1], Loss: 0.0039\n",
      "Epoch 463 completed with average loss: 0.0039\n",
      "Epoch [464/1000], Step [0/1], Loss: 0.0039\n",
      "Epoch 464 completed with average loss: 0.0039\n",
      "Epoch [465/1000], Step [0/1], Loss: 0.0039\n",
      "Epoch 465 completed with average loss: 0.0039\n",
      "Epoch [466/1000], Step [0/1], Loss: 0.0039\n",
      "Epoch 466 completed with average loss: 0.0039\n",
      "Epoch [467/1000], Step [0/1], Loss: 0.0039\n",
      "Epoch 467 completed with average loss: 0.0039\n",
      "Epoch [468/1000], Step [0/1], Loss: 0.0039\n",
      "Epoch 468 completed with average loss: 0.0039\n",
      "Epoch [469/1000], Step [0/1], Loss: 0.0038\n",
      "Epoch 469 completed with average loss: 0.0038\n",
      "Epoch [470/1000], Step [0/1], Loss: 0.0038\n",
      "Epoch 470 completed with average loss: 0.0038\n",
      "Epoch [471/1000], Step [0/1], Loss: 0.0038\n",
      "Epoch 471 completed with average loss: 0.0038\n",
      "Epoch [472/1000], Step [0/1], Loss: 0.0038\n",
      "Epoch 472 completed with average loss: 0.0038\n",
      "Epoch [473/1000], Step [0/1], Loss: 0.0038\n",
      "Epoch 473 completed with average loss: 0.0038\n",
      "Epoch [474/1000], Step [0/1], Loss: 0.0038\n",
      "Epoch 474 completed with average loss: 0.0038\n",
      "Epoch [475/1000], Step [0/1], Loss: 0.0038\n",
      "Epoch 475 completed with average loss: 0.0038\n",
      "Epoch [476/1000], Step [0/1], Loss: 0.0038\n",
      "Epoch 476 completed with average loss: 0.0038\n",
      "Epoch [477/1000], Step [0/1], Loss: 0.0038\n",
      "Epoch 477 completed with average loss: 0.0038\n",
      "Epoch [478/1000], Step [0/1], Loss: 0.0038\n",
      "Epoch 478 completed with average loss: 0.0038\n",
      "Epoch [479/1000], Step [0/1], Loss: 0.0037\n",
      "Epoch 479 completed with average loss: 0.0037\n",
      "Epoch [480/1000], Step [0/1], Loss: 0.0037\n",
      "Epoch 480 completed with average loss: 0.0037\n",
      "Epoch [481/1000], Step [0/1], Loss: 0.0037\n",
      "Epoch 481 completed with average loss: 0.0037\n",
      "Epoch [482/1000], Step [0/1], Loss: 0.0037\n",
      "Epoch 482 completed with average loss: 0.0037\n",
      "Epoch [483/1000], Step [0/1], Loss: 0.0037\n",
      "Epoch 483 completed with average loss: 0.0037\n",
      "Epoch [484/1000], Step [0/1], Loss: 0.0037\n",
      "Epoch 484 completed with average loss: 0.0037\n",
      "Epoch [485/1000], Step [0/1], Loss: 0.0037\n",
      "Epoch 485 completed with average loss: 0.0037\n",
      "Epoch [486/1000], Step [0/1], Loss: 0.0037\n",
      "Epoch 486 completed with average loss: 0.0037\n",
      "Epoch [487/1000], Step [0/1], Loss: 0.0037\n",
      "Epoch 487 completed with average loss: 0.0037\n",
      "Epoch [488/1000], Step [0/1], Loss: 0.0037\n",
      "Epoch 488 completed with average loss: 0.0037\n",
      "Epoch [489/1000], Step [0/1], Loss: 0.0036\n",
      "Epoch 489 completed with average loss: 0.0036\n",
      "Epoch [490/1000], Step [0/1], Loss: 0.0036\n",
      "Epoch 490 completed with average loss: 0.0036\n",
      "Epoch [491/1000], Step [0/1], Loss: 0.0036\n",
      "Epoch 491 completed with average loss: 0.0036\n",
      "Epoch [492/1000], Step [0/1], Loss: 0.0036\n",
      "Epoch 492 completed with average loss: 0.0036\n",
      "Epoch [493/1000], Step [0/1], Loss: 0.0036\n",
      "Epoch 493 completed with average loss: 0.0036\n",
      "Epoch [494/1000], Step [0/1], Loss: 0.0036\n",
      "Epoch 494 completed with average loss: 0.0036\n",
      "Epoch [495/1000], Step [0/1], Loss: 0.0036\n",
      "Epoch 495 completed with average loss: 0.0036\n",
      "Epoch [496/1000], Step [0/1], Loss: 0.0036\n",
      "Epoch 496 completed with average loss: 0.0036\n",
      "Epoch [497/1000], Step [0/1], Loss: 0.0036\n",
      "Epoch 497 completed with average loss: 0.0036\n",
      "Epoch [498/1000], Step [0/1], Loss: 0.0036\n",
      "Epoch 498 completed with average loss: 0.0036\n",
      "Epoch [499/1000], Step [0/1], Loss: 0.0036\n",
      "Epoch 499 completed with average loss: 0.0036\n",
      "Epoch [500/1000], Step [0/1], Loss: 0.0035\n",
      "Epoch 500 completed with average loss: 0.0035\n",
      "Epoch [501/1000], Step [0/1], Loss: 0.0035\n",
      "Epoch 501 completed with average loss: 0.0035\n",
      "Epoch [502/1000], Step [0/1], Loss: 0.0035\n",
      "Epoch 502 completed with average loss: 0.0035\n",
      "Epoch [503/1000], Step [0/1], Loss: 0.0035\n",
      "Epoch 503 completed with average loss: 0.0035\n",
      "Epoch [504/1000], Step [0/1], Loss: 0.0035\n",
      "Epoch 504 completed with average loss: 0.0035\n",
      "Epoch [505/1000], Step [0/1], Loss: 0.0035\n",
      "Epoch 505 completed with average loss: 0.0035\n",
      "Epoch [506/1000], Step [0/1], Loss: 0.0035\n",
      "Epoch 506 completed with average loss: 0.0035\n",
      "Epoch [507/1000], Step [0/1], Loss: 0.0035\n",
      "Epoch 507 completed with average loss: 0.0035\n",
      "Epoch [508/1000], Step [0/1], Loss: 0.0035\n",
      "Epoch 508 completed with average loss: 0.0035\n",
      "Epoch [509/1000], Step [0/1], Loss: 0.0035\n",
      "Epoch 509 completed with average loss: 0.0035\n",
      "Epoch [510/1000], Step [0/1], Loss: 0.0035\n",
      "Epoch 510 completed with average loss: 0.0035\n",
      "Epoch [511/1000], Step [0/1], Loss: 0.0035\n",
      "Epoch 511 completed with average loss: 0.0035\n",
      "Epoch [512/1000], Step [0/1], Loss: 0.0035\n",
      "Epoch 512 completed with average loss: 0.0035\n",
      "Epoch [513/1000], Step [0/1], Loss: 0.0034\n",
      "Epoch 513 completed with average loss: 0.0034\n",
      "Epoch [514/1000], Step [0/1], Loss: 0.0034\n",
      "Epoch 514 completed with average loss: 0.0034\n",
      "Epoch [515/1000], Step [0/1], Loss: 0.0034\n",
      "Epoch 515 completed with average loss: 0.0034\n",
      "Epoch [516/1000], Step [0/1], Loss: 0.0034\n",
      "Epoch 516 completed with average loss: 0.0034\n",
      "Epoch [517/1000], Step [0/1], Loss: 0.0034\n",
      "Epoch 517 completed with average loss: 0.0034\n",
      "Epoch [518/1000], Step [0/1], Loss: 0.0034\n",
      "Epoch 518 completed with average loss: 0.0034\n",
      "Epoch [519/1000], Step [0/1], Loss: 0.0034\n",
      "Epoch 519 completed with average loss: 0.0034\n",
      "Epoch [520/1000], Step [0/1], Loss: 0.0034\n",
      "Epoch 520 completed with average loss: 0.0034\n",
      "Epoch [521/1000], Step [0/1], Loss: 0.0034\n",
      "Epoch 521 completed with average loss: 0.0034\n",
      "Epoch [522/1000], Step [0/1], Loss: 0.0034\n",
      "Epoch 522 completed with average loss: 0.0034\n",
      "Epoch [523/1000], Step [0/1], Loss: 0.0034\n",
      "Epoch 523 completed with average loss: 0.0034\n",
      "Epoch [524/1000], Step [0/1], Loss: 0.0034\n",
      "Epoch 524 completed with average loss: 0.0034\n",
      "Epoch [525/1000], Step [0/1], Loss: 0.0034\n",
      "Epoch 525 completed with average loss: 0.0034\n",
      "Epoch [526/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 526 completed with average loss: 0.0033\n",
      "Epoch [527/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 527 completed with average loss: 0.0033\n",
      "Epoch [528/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 528 completed with average loss: 0.0033\n",
      "Epoch [529/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 529 completed with average loss: 0.0033\n",
      "Epoch [530/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 530 completed with average loss: 0.0033\n",
      "Epoch [531/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 531 completed with average loss: 0.0033\n",
      "Epoch [532/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 532 completed with average loss: 0.0033\n",
      "Epoch [533/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 533 completed with average loss: 0.0033\n",
      "Epoch [534/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 534 completed with average loss: 0.0033\n",
      "Epoch [535/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 535 completed with average loss: 0.0033\n",
      "Epoch [536/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 536 completed with average loss: 0.0033\n",
      "Epoch [537/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 537 completed with average loss: 0.0033\n",
      "Epoch [538/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 538 completed with average loss: 0.0033\n",
      "Epoch [539/1000], Step [0/1], Loss: 0.0033\n",
      "Epoch 539 completed with average loss: 0.0033\n",
      "Epoch [540/1000], Step [0/1], Loss: 0.0032\n",
      "Epoch 540 completed with average loss: 0.0032\n",
      "Epoch [541/1000], Step [0/1], Loss: 0.0032\n",
      "Epoch 541 completed with average loss: 0.0032\n",
      "Epoch [542/1000], Step [0/1], Loss: 0.0032\n",
      "Epoch 542 completed with average loss: 0.0032\n",
      "Epoch [543/1000], Step [0/1], Loss: 0.0032\n",
      "Epoch 543 completed with average loss: 0.0032\n",
      "Epoch [544/1000], Step [0/1], Loss: 0.0032\n",
      "Epoch 544 completed with average loss: 0.0032\n",
      "Epoch [545/1000], Step [0/1], Loss: 0.0032\n",
      "Epoch 545 completed with average loss: 0.0032\n",
      "Epoch [546/1000], Step [0/1], Loss: 0.0032\n",
      "Epoch 546 completed with average loss: 0.0032\n",
      "Epoch [547/1000], Step [0/1], Loss: 0.0032\n",
      "Epoch 547 completed with average loss: 0.0032\n",
      "Epoch [548/1000], Step [0/1], Loss: 0.0032\n",
      "Epoch 548 completed with average loss: 0.0032\n",
      "Epoch [549/1000], Step [0/1], Loss: 0.0032\n",
      "Epoch 549 completed with average loss: 0.0032\n",
      "Epoch [550/1000], Step [0/1], Loss: 0.0032\n",
      "Epoch 550 completed with average loss: 0.0032\n",
      "Epoch [551/1000], Step [0/1], Loss: 0.0032\n",
      "Epoch 551 completed with average loss: 0.0032\n",
      "Epoch [552/1000], Step [0/1], Loss: 0.0032\n",
      "Epoch 552 completed with average loss: 0.0032\n",
      "Epoch [553/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 553 completed with average loss: 0.0031\n",
      "Epoch [554/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 554 completed with average loss: 0.0031\n",
      "Epoch [555/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 555 completed with average loss: 0.0031\n",
      "Epoch [556/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 556 completed with average loss: 0.0031\n",
      "Epoch [557/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 557 completed with average loss: 0.0031\n",
      "Epoch [558/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 558 completed with average loss: 0.0031\n",
      "Epoch [559/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 559 completed with average loss: 0.0031\n",
      "Epoch [560/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 560 completed with average loss: 0.0031\n",
      "Epoch [561/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 561 completed with average loss: 0.0031\n",
      "Epoch [562/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 562 completed with average loss: 0.0031\n",
      "Epoch [563/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 563 completed with average loss: 0.0031\n",
      "Epoch [564/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 564 completed with average loss: 0.0031\n",
      "Epoch [565/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 565 completed with average loss: 0.0031\n",
      "Epoch [566/1000], Step [0/1], Loss: 0.0031\n",
      "Epoch 566 completed with average loss: 0.0031\n",
      "Epoch [567/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 567 completed with average loss: 0.0030\n",
      "Epoch [568/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 568 completed with average loss: 0.0030\n",
      "Epoch [569/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 569 completed with average loss: 0.0030\n",
      "Epoch [570/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 570 completed with average loss: 0.0030\n",
      "Epoch [571/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 571 completed with average loss: 0.0030\n",
      "Epoch [572/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 572 completed with average loss: 0.0030\n",
      "Epoch [573/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 573 completed with average loss: 0.0030\n",
      "Epoch [574/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 574 completed with average loss: 0.0030\n",
      "Epoch [575/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 575 completed with average loss: 0.0030\n",
      "Epoch [576/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 576 completed with average loss: 0.0030\n",
      "Epoch [577/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 577 completed with average loss: 0.0030\n",
      "Epoch [578/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 578 completed with average loss: 0.0030\n",
      "Epoch [579/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 579 completed with average loss: 0.0030\n",
      "Epoch [580/1000], Step [0/1], Loss: 0.0030\n",
      "Epoch 580 completed with average loss: 0.0030\n",
      "Epoch [581/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 581 completed with average loss: 0.0029\n",
      "Epoch [582/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 582 completed with average loss: 0.0029\n",
      "Epoch [583/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 583 completed with average loss: 0.0029\n",
      "Epoch [584/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 584 completed with average loss: 0.0029\n",
      "Epoch [585/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 585 completed with average loss: 0.0029\n",
      "Epoch [586/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 586 completed with average loss: 0.0029\n",
      "Epoch [587/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 587 completed with average loss: 0.0029\n",
      "Epoch [588/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 588 completed with average loss: 0.0029\n",
      "Epoch [589/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 589 completed with average loss: 0.0029\n",
      "Epoch [590/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 590 completed with average loss: 0.0029\n",
      "Epoch [591/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 591 completed with average loss: 0.0029\n",
      "Epoch [592/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 592 completed with average loss: 0.0029\n",
      "Epoch [593/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 593 completed with average loss: 0.0029\n",
      "Epoch [594/1000], Step [0/1], Loss: 0.0029\n",
      "Epoch 594 completed with average loss: 0.0029\n",
      "Epoch [595/1000], Step [0/1], Loss: 0.0028\n",
      "Epoch 595 completed with average loss: 0.0028\n",
      "Epoch [596/1000], Step [0/1], Loss: 0.0028\n",
      "Epoch 596 completed with average loss: 0.0028\n",
      "Epoch [597/1000], Step [0/1], Loss: 0.0028\n",
      "Epoch 597 completed with average loss: 0.0028\n",
      "Epoch [598/1000], Step [0/1], Loss: 0.0028\n",
      "Epoch 598 completed with average loss: 0.0028\n",
      "Epoch [599/1000], Step [0/1], Loss: 0.0028\n",
      "Epoch 599 completed with average loss: 0.0028\n",
      "Epoch [600/1000], Step [0/1], Loss: 0.0028\n",
      "Epoch 600 completed with average loss: 0.0028\n",
      "Epoch [601/1000], Step [0/1], Loss: 0.0028\n",
      "Epoch 601 completed with average loss: 0.0028\n",
      "Epoch [602/1000], Step [0/1], Loss: 0.0028\n",
      "Epoch 602 completed with average loss: 0.0028\n",
      "Epoch [603/1000], Step [0/1], Loss: 0.0028\n",
      "Epoch 603 completed with average loss: 0.0028\n",
      "Epoch [604/1000], Step [0/1], Loss: 0.0028\n",
      "Epoch 604 completed with average loss: 0.0028\n",
      "Epoch [605/1000], Step [0/1], Loss: 0.0028\n",
      "Epoch 605 completed with average loss: 0.0028\n",
      "Epoch [606/1000], Step [0/1], Loss: 0.0028\n",
      "Epoch 606 completed with average loss: 0.0028\n",
      "Epoch [607/1000], Step [0/1], Loss: 0.0028\n",
      "Epoch 607 completed with average loss: 0.0028\n",
      "Epoch [608/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 608 completed with average loss: 0.0027\n",
      "Epoch [609/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 609 completed with average loss: 0.0027\n",
      "Epoch [610/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 610 completed with average loss: 0.0027\n",
      "Epoch [611/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 611 completed with average loss: 0.0027\n",
      "Epoch [612/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 612 completed with average loss: 0.0027\n",
      "Epoch [613/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 613 completed with average loss: 0.0027\n",
      "Epoch [614/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 614 completed with average loss: 0.0027\n",
      "Epoch [615/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 615 completed with average loss: 0.0027\n",
      "Epoch [616/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 616 completed with average loss: 0.0027\n",
      "Epoch [617/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 617 completed with average loss: 0.0027\n",
      "Epoch [618/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 618 completed with average loss: 0.0027\n",
      "Epoch [619/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 619 completed with average loss: 0.0027\n",
      "Epoch [620/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 620 completed with average loss: 0.0027\n",
      "Epoch [621/1000], Step [0/1], Loss: 0.0027\n",
      "Epoch 621 completed with average loss: 0.0027\n",
      "Epoch [622/1000], Step [0/1], Loss: 0.0026\n",
      "Epoch 622 completed with average loss: 0.0026\n",
      "Epoch [623/1000], Step [0/1], Loss: 0.0026\n",
      "Epoch 623 completed with average loss: 0.0026\n",
      "Epoch [624/1000], Step [0/1], Loss: 0.0026\n",
      "Epoch 624 completed with average loss: 0.0026\n",
      "Epoch [625/1000], Step [0/1], Loss: 0.0026\n",
      "Epoch 625 completed with average loss: 0.0026\n",
      "Epoch [626/1000], Step [0/1], Loss: 0.0026\n",
      "Epoch 626 completed with average loss: 0.0026\n",
      "Epoch [627/1000], Step [0/1], Loss: 0.0026\n",
      "Epoch 627 completed with average loss: 0.0026\n",
      "Epoch [628/1000], Step [0/1], Loss: 0.0026\n",
      "Epoch 628 completed with average loss: 0.0026\n",
      "Epoch [629/1000], Step [0/1], Loss: 0.0026\n",
      "Epoch 629 completed with average loss: 0.0026\n",
      "Epoch [630/1000], Step [0/1], Loss: 0.0026\n",
      "Epoch 630 completed with average loss: 0.0026\n",
      "Epoch [631/1000], Step [0/1], Loss: 0.0026\n",
      "Epoch 631 completed with average loss: 0.0026\n",
      "Epoch [632/1000], Step [0/1], Loss: 0.0026\n",
      "Epoch 632 completed with average loss: 0.0026\n",
      "Epoch [633/1000], Step [0/1], Loss: 0.0026\n",
      "Epoch 633 completed with average loss: 0.0026\n",
      "Epoch [634/1000], Step [0/1], Loss: 0.0026\n",
      "Epoch 634 completed with average loss: 0.0026\n",
      "Epoch [635/1000], Step [0/1], Loss: 0.0025\n",
      "Epoch 635 completed with average loss: 0.0025\n",
      "Epoch [636/1000], Step [0/1], Loss: 0.0025\n",
      "Epoch 636 completed with average loss: 0.0025\n",
      "Epoch [637/1000], Step [0/1], Loss: 0.0025\n",
      "Epoch 637 completed with average loss: 0.0025\n",
      "Epoch [638/1000], Step [0/1], Loss: 0.0025\n",
      "Epoch 638 completed with average loss: 0.0025\n",
      "Epoch [639/1000], Step [0/1], Loss: 0.0025\n",
      "Epoch 639 completed with average loss: 0.0025\n",
      "Epoch [640/1000], Step [0/1], Loss: 0.0025\n",
      "Epoch 640 completed with average loss: 0.0025\n",
      "Epoch [641/1000], Step [0/1], Loss: 0.0025\n",
      "Epoch 641 completed with average loss: 0.0025\n",
      "Epoch [642/1000], Step [0/1], Loss: 0.0025\n",
      "Epoch 642 completed with average loss: 0.0025\n",
      "Epoch [643/1000], Step [0/1], Loss: 0.0025\n",
      "Epoch 643 completed with average loss: 0.0025\n",
      "Epoch [644/1000], Step [0/1], Loss: 0.0025\n",
      "Epoch 644 completed with average loss: 0.0025\n",
      "Epoch [645/1000], Step [0/1], Loss: 0.0025\n",
      "Epoch 645 completed with average loss: 0.0025\n",
      "Epoch [646/1000], Step [0/1], Loss: 0.0025\n",
      "Epoch 646 completed with average loss: 0.0025\n",
      "Epoch [647/1000], Step [0/1], Loss: 0.0024\n",
      "Epoch 647 completed with average loss: 0.0024\n",
      "Epoch [648/1000], Step [0/1], Loss: 0.0024\n",
      "Epoch 648 completed with average loss: 0.0024\n",
      "Epoch [649/1000], Step [0/1], Loss: 0.0024\n",
      "Epoch 649 completed with average loss: 0.0024\n",
      "Epoch [650/1000], Step [0/1], Loss: 0.0024\n",
      "Epoch 650 completed with average loss: 0.0024\n",
      "Epoch [651/1000], Step [0/1], Loss: 0.0024\n",
      "Epoch 651 completed with average loss: 0.0024\n",
      "Epoch [652/1000], Step [0/1], Loss: 0.0024\n",
      "Epoch 652 completed with average loss: 0.0024\n",
      "Epoch [653/1000], Step [0/1], Loss: 0.0024\n",
      "Epoch 653 completed with average loss: 0.0024\n",
      "Epoch [654/1000], Step [0/1], Loss: 0.0024\n",
      "Epoch 654 completed with average loss: 0.0024\n",
      "Epoch [655/1000], Step [0/1], Loss: 0.0024\n",
      "Epoch 655 completed with average loss: 0.0024\n",
      "Epoch [656/1000], Step [0/1], Loss: 0.0024\n",
      "Epoch 656 completed with average loss: 0.0024\n",
      "Epoch [657/1000], Step [0/1], Loss: 0.0024\n",
      "Epoch 657 completed with average loss: 0.0024\n",
      "Epoch [658/1000], Step [0/1], Loss: 0.0024\n",
      "Epoch 658 completed with average loss: 0.0024\n",
      "Epoch [659/1000], Step [0/1], Loss: 0.0024\n",
      "Epoch 659 completed with average loss: 0.0024\n",
      "Epoch [660/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 660 completed with average loss: 0.0023\n",
      "Epoch [661/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 661 completed with average loss: 0.0023\n",
      "Epoch [662/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 662 completed with average loss: 0.0023\n",
      "Epoch [663/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 663 completed with average loss: 0.0023\n",
      "Epoch [664/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 664 completed with average loss: 0.0023\n",
      "Epoch [665/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 665 completed with average loss: 0.0023\n",
      "Epoch [666/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 666 completed with average loss: 0.0023\n",
      "Epoch [667/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 667 completed with average loss: 0.0023\n",
      "Epoch [668/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 668 completed with average loss: 0.0023\n",
      "Epoch [669/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 669 completed with average loss: 0.0023\n",
      "Epoch [670/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 670 completed with average loss: 0.0023\n",
      "Epoch [671/1000], Step [0/1], Loss: 0.0023\n",
      "Epoch 671 completed with average loss: 0.0023\n",
      "Epoch [672/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 672 completed with average loss: 0.0022\n",
      "Epoch [673/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 673 completed with average loss: 0.0022\n",
      "Epoch [674/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 674 completed with average loss: 0.0022\n",
      "Epoch [675/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 675 completed with average loss: 0.0022\n",
      "Epoch [676/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 676 completed with average loss: 0.0022\n",
      "Epoch [677/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 677 completed with average loss: 0.0022\n",
      "Epoch [678/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 678 completed with average loss: 0.0022\n",
      "Epoch [679/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 679 completed with average loss: 0.0022\n",
      "Epoch [680/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 680 completed with average loss: 0.0022\n",
      "Epoch [681/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 681 completed with average loss: 0.0022\n",
      "Epoch [682/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 682 completed with average loss: 0.0022\n",
      "Epoch [683/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 683 completed with average loss: 0.0022\n",
      "Epoch [684/1000], Step [0/1], Loss: 0.0022\n",
      "Epoch 684 completed with average loss: 0.0022\n",
      "Epoch [685/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 685 completed with average loss: 0.0021\n",
      "Epoch [686/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 686 completed with average loss: 0.0021\n",
      "Epoch [687/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 687 completed with average loss: 0.0021\n",
      "Epoch [688/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 688 completed with average loss: 0.0021\n",
      "Epoch [689/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 689 completed with average loss: 0.0021\n",
      "Epoch [690/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 690 completed with average loss: 0.0021\n",
      "Epoch [691/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 691 completed with average loss: 0.0021\n",
      "Epoch [692/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 692 completed with average loss: 0.0021\n",
      "Epoch [693/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 693 completed with average loss: 0.0021\n",
      "Epoch [694/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 694 completed with average loss: 0.0021\n",
      "Epoch [695/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 695 completed with average loss: 0.0021\n",
      "Epoch [696/1000], Step [0/1], Loss: 0.0021\n",
      "Epoch 696 completed with average loss: 0.0021\n",
      "Epoch [697/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 697 completed with average loss: 0.0020\n",
      "Epoch [698/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 698 completed with average loss: 0.0020\n",
      "Epoch [699/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 699 completed with average loss: 0.0020\n",
      "Epoch [700/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 700 completed with average loss: 0.0020\n",
      "Epoch [701/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 701 completed with average loss: 0.0020\n",
      "Epoch [702/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 702 completed with average loss: 0.0020\n",
      "Epoch [703/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 703 completed with average loss: 0.0020\n",
      "Epoch [704/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 704 completed with average loss: 0.0020\n",
      "Epoch [705/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 705 completed with average loss: 0.0020\n",
      "Epoch [706/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 706 completed with average loss: 0.0020\n",
      "Epoch [707/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 707 completed with average loss: 0.0020\n",
      "Epoch [708/1000], Step [0/1], Loss: 0.0020\n",
      "Epoch 708 completed with average loss: 0.0020\n",
      "Epoch [709/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 709 completed with average loss: 0.0019\n",
      "Epoch [710/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 710 completed with average loss: 0.0019\n",
      "Epoch [711/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 711 completed with average loss: 0.0019\n",
      "Epoch [712/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 712 completed with average loss: 0.0019\n",
      "Epoch [713/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 713 completed with average loss: 0.0019\n",
      "Epoch [714/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 714 completed with average loss: 0.0019\n",
      "Epoch [715/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 715 completed with average loss: 0.0019\n",
      "Epoch [716/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 716 completed with average loss: 0.0019\n",
      "Epoch [717/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 717 completed with average loss: 0.0019\n",
      "Epoch [718/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 718 completed with average loss: 0.0019\n",
      "Epoch [719/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 719 completed with average loss: 0.0019\n",
      "Epoch [720/1000], Step [0/1], Loss: 0.0019\n",
      "Epoch 720 completed with average loss: 0.0019\n",
      "Epoch [721/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 721 completed with average loss: 0.0018\n",
      "Epoch [722/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 722 completed with average loss: 0.0018\n",
      "Epoch [723/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 723 completed with average loss: 0.0018\n",
      "Epoch [724/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 724 completed with average loss: 0.0018\n",
      "Epoch [725/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 725 completed with average loss: 0.0018\n",
      "Epoch [726/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 726 completed with average loss: 0.0018\n",
      "Epoch [727/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 727 completed with average loss: 0.0018\n",
      "Epoch [728/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 728 completed with average loss: 0.0018\n",
      "Epoch [729/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 729 completed with average loss: 0.0018\n",
      "Epoch [730/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 730 completed with average loss: 0.0018\n",
      "Epoch [731/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 731 completed with average loss: 0.0018\n",
      "Epoch [732/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 732 completed with average loss: 0.0018\n",
      "Epoch [733/1000], Step [0/1], Loss: 0.0018\n",
      "Epoch 733 completed with average loss: 0.0018\n",
      "Epoch [734/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 734 completed with average loss: 0.0017\n",
      "Epoch [735/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 735 completed with average loss: 0.0017\n",
      "Epoch [736/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 736 completed with average loss: 0.0017\n",
      "Epoch [737/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 737 completed with average loss: 0.0017\n",
      "Epoch [738/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 738 completed with average loss: 0.0017\n",
      "Epoch [739/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 739 completed with average loss: 0.0017\n",
      "Epoch [740/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 740 completed with average loss: 0.0017\n",
      "Epoch [741/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 741 completed with average loss: 0.0017\n",
      "Epoch [742/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 742 completed with average loss: 0.0017\n",
      "Epoch [743/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 743 completed with average loss: 0.0017\n",
      "Epoch [744/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 744 completed with average loss: 0.0017\n",
      "Epoch [745/1000], Step [0/1], Loss: 0.0017\n",
      "Epoch 745 completed with average loss: 0.0017\n",
      "Epoch [746/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 746 completed with average loss: 0.0016\n",
      "Epoch [747/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 747 completed with average loss: 0.0016\n",
      "Epoch [748/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 748 completed with average loss: 0.0016\n",
      "Epoch [749/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 749 completed with average loss: 0.0016\n",
      "Epoch [750/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 750 completed with average loss: 0.0016\n",
      "Epoch [751/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 751 completed with average loss: 0.0016\n",
      "Epoch [752/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 752 completed with average loss: 0.0016\n",
      "Epoch [753/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 753 completed with average loss: 0.0016\n",
      "Epoch [754/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 754 completed with average loss: 0.0016\n",
      "Epoch [755/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 755 completed with average loss: 0.0016\n",
      "Epoch [756/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 756 completed with average loss: 0.0016\n",
      "Epoch [757/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 757 completed with average loss: 0.0016\n",
      "Epoch [758/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 758 completed with average loss: 0.0016\n",
      "Epoch [759/1000], Step [0/1], Loss: 0.0016\n",
      "Epoch 759 completed with average loss: 0.0016\n",
      "Epoch [760/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 760 completed with average loss: 0.0015\n",
      "Epoch [761/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 761 completed with average loss: 0.0015\n",
      "Epoch [762/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 762 completed with average loss: 0.0015\n",
      "Epoch [763/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 763 completed with average loss: 0.0015\n",
      "Epoch [764/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 764 completed with average loss: 0.0015\n",
      "Epoch [765/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 765 completed with average loss: 0.0015\n",
      "Epoch [766/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 766 completed with average loss: 0.0015\n",
      "Epoch [767/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 767 completed with average loss: 0.0015\n",
      "Epoch [768/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 768 completed with average loss: 0.0015\n",
      "Epoch [769/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 769 completed with average loss: 0.0015\n",
      "Epoch [770/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 770 completed with average loss: 0.0015\n",
      "Epoch [771/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 771 completed with average loss: 0.0015\n",
      "Epoch [772/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 772 completed with average loss: 0.0015\n",
      "Epoch [773/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 773 completed with average loss: 0.0015\n",
      "Epoch [774/1000], Step [0/1], Loss: 0.0015\n",
      "Epoch 774 completed with average loss: 0.0015\n",
      "Epoch [775/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 775 completed with average loss: 0.0014\n",
      "Epoch [776/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 776 completed with average loss: 0.0014\n",
      "Epoch [777/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 777 completed with average loss: 0.0014\n",
      "Epoch [778/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 778 completed with average loss: 0.0014\n",
      "Epoch [779/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 779 completed with average loss: 0.0014\n",
      "Epoch [780/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 780 completed with average loss: 0.0014\n",
      "Epoch [781/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 781 completed with average loss: 0.0014\n",
      "Epoch [782/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 782 completed with average loss: 0.0014\n",
      "Epoch [783/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 783 completed with average loss: 0.0014\n",
      "Epoch [784/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 784 completed with average loss: 0.0014\n",
      "Epoch [785/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 785 completed with average loss: 0.0014\n",
      "Epoch [786/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 786 completed with average loss: 0.0014\n",
      "Epoch [787/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 787 completed with average loss: 0.0014\n",
      "Epoch [788/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 788 completed with average loss: 0.0014\n",
      "Epoch [789/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 789 completed with average loss: 0.0014\n",
      "Epoch [790/1000], Step [0/1], Loss: 0.0014\n",
      "Epoch 790 completed with average loss: 0.0014\n",
      "Epoch [791/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 791 completed with average loss: 0.0013\n",
      "Epoch [792/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 792 completed with average loss: 0.0013\n",
      "Epoch [793/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 793 completed with average loss: 0.0013\n",
      "Epoch [794/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 794 completed with average loss: 0.0013\n",
      "Epoch [795/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 795 completed with average loss: 0.0013\n",
      "Epoch [796/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 796 completed with average loss: 0.0013\n",
      "Epoch [797/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 797 completed with average loss: 0.0013\n",
      "Epoch [798/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 798 completed with average loss: 0.0013\n",
      "Epoch [799/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 799 completed with average loss: 0.0013\n",
      "Epoch [800/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 800 completed with average loss: 0.0013\n",
      "Epoch [801/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 801 completed with average loss: 0.0013\n",
      "Epoch [802/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 802 completed with average loss: 0.0013\n",
      "Epoch [803/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 803 completed with average loss: 0.0013\n",
      "Epoch [804/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 804 completed with average loss: 0.0013\n",
      "Epoch [805/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 805 completed with average loss: 0.0013\n",
      "Epoch [806/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 806 completed with average loss: 0.0013\n",
      "Epoch [807/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 807 completed with average loss: 0.0013\n",
      "Epoch [808/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 808 completed with average loss: 0.0013\n",
      "Epoch [809/1000], Step [0/1], Loss: 0.0013\n",
      "Epoch 809 completed with average loss: 0.0013\n",
      "Epoch [810/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 810 completed with average loss: 0.0012\n",
      "Epoch [811/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 811 completed with average loss: 0.0012\n",
      "Epoch [812/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 812 completed with average loss: 0.0012\n",
      "Epoch [813/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 813 completed with average loss: 0.0012\n",
      "Epoch [814/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 814 completed with average loss: 0.0012\n",
      "Epoch [815/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 815 completed with average loss: 0.0012\n",
      "Epoch [816/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 816 completed with average loss: 0.0012\n",
      "Epoch [817/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 817 completed with average loss: 0.0012\n",
      "Epoch [818/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 818 completed with average loss: 0.0012\n",
      "Epoch [819/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 819 completed with average loss: 0.0012\n",
      "Epoch [820/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 820 completed with average loss: 0.0012\n",
      "Epoch [821/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 821 completed with average loss: 0.0012\n",
      "Epoch [822/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 822 completed with average loss: 0.0012\n",
      "Epoch [823/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 823 completed with average loss: 0.0012\n",
      "Epoch [824/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 824 completed with average loss: 0.0012\n",
      "Epoch [825/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 825 completed with average loss: 0.0012\n",
      "Epoch [826/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 826 completed with average loss: 0.0012\n",
      "Epoch [827/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 827 completed with average loss: 0.0012\n",
      "Epoch [828/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 828 completed with average loss: 0.0012\n",
      "Epoch [829/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 829 completed with average loss: 0.0012\n",
      "Epoch [830/1000], Step [0/1], Loss: 0.0012\n",
      "Epoch 830 completed with average loss: 0.0012\n",
      "Epoch [831/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 831 completed with average loss: 0.0011\n",
      "Epoch [832/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 832 completed with average loss: 0.0011\n",
      "Epoch [833/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 833 completed with average loss: 0.0011\n",
      "Epoch [834/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 834 completed with average loss: 0.0011\n",
      "Epoch [835/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 835 completed with average loss: 0.0011\n",
      "Epoch [836/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 836 completed with average loss: 0.0011\n",
      "Epoch [837/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 837 completed with average loss: 0.0011\n",
      "Epoch [838/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 838 completed with average loss: 0.0011\n",
      "Epoch [839/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 839 completed with average loss: 0.0011\n",
      "Epoch [840/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 840 completed with average loss: 0.0011\n",
      "Epoch [841/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 841 completed with average loss: 0.0011\n",
      "Epoch [842/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 842 completed with average loss: 0.0011\n",
      "Epoch [843/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 843 completed with average loss: 0.0011\n",
      "Epoch [844/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 844 completed with average loss: 0.0011\n",
      "Epoch [845/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 845 completed with average loss: 0.0011\n",
      "Epoch [846/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 846 completed with average loss: 0.0011\n",
      "Epoch [847/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 847 completed with average loss: 0.0011\n",
      "Epoch [848/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 848 completed with average loss: 0.0011\n",
      "Epoch [849/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 849 completed with average loss: 0.0011\n",
      "Epoch [850/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 850 completed with average loss: 0.0011\n",
      "Epoch [851/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 851 completed with average loss: 0.0011\n",
      "Epoch [852/1000], Step [0/1], Loss: 0.0011\n",
      "Epoch 852 completed with average loss: 0.0011\n",
      "Epoch [853/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 853 completed with average loss: 0.0010\n",
      "Epoch [854/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 854 completed with average loss: 0.0010\n",
      "Epoch [855/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 855 completed with average loss: 0.0010\n",
      "Epoch [856/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 856 completed with average loss: 0.0010\n",
      "Epoch [857/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 857 completed with average loss: 0.0010\n",
      "Epoch [858/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 858 completed with average loss: 0.0010\n",
      "Epoch [859/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 859 completed with average loss: 0.0010\n",
      "Epoch [860/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 860 completed with average loss: 0.0010\n",
      "Epoch [861/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 861 completed with average loss: 0.0010\n",
      "Epoch [862/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 862 completed with average loss: 0.0010\n",
      "Epoch [863/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 863 completed with average loss: 0.0010\n",
      "Epoch [864/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 864 completed with average loss: 0.0010\n",
      "Epoch [865/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 865 completed with average loss: 0.0010\n",
      "Epoch [866/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 866 completed with average loss: 0.0010\n",
      "Epoch [867/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 867 completed with average loss: 0.0010\n",
      "Epoch [868/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 868 completed with average loss: 0.0010\n",
      "Epoch [869/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 869 completed with average loss: 0.0010\n",
      "Epoch [870/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 870 completed with average loss: 0.0010\n",
      "Epoch [871/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 871 completed with average loss: 0.0010\n",
      "Epoch [872/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 872 completed with average loss: 0.0010\n",
      "Epoch [873/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 873 completed with average loss: 0.0010\n",
      "Epoch [874/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 874 completed with average loss: 0.0010\n",
      "Epoch [875/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 875 completed with average loss: 0.0010\n",
      "Epoch [876/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 876 completed with average loss: 0.0010\n",
      "Epoch [877/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 877 completed with average loss: 0.0010\n",
      "Epoch [878/1000], Step [0/1], Loss: 0.0010\n",
      "Epoch 878 completed with average loss: 0.0010\n",
      "Epoch [879/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 879 completed with average loss: 0.0009\n",
      "Epoch [880/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 880 completed with average loss: 0.0009\n",
      "Epoch [881/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 881 completed with average loss: 0.0009\n",
      "Epoch [882/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 882 completed with average loss: 0.0009\n",
      "Epoch [883/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 883 completed with average loss: 0.0009\n",
      "Epoch [884/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 884 completed with average loss: 0.0009\n",
      "Epoch [885/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 885 completed with average loss: 0.0009\n",
      "Epoch [886/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 886 completed with average loss: 0.0009\n",
      "Epoch [887/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 887 completed with average loss: 0.0009\n",
      "Epoch [888/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 888 completed with average loss: 0.0009\n",
      "Epoch [889/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 889 completed with average loss: 0.0009\n",
      "Epoch [890/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 890 completed with average loss: 0.0009\n",
      "Epoch [891/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 891 completed with average loss: 0.0009\n",
      "Epoch [892/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 892 completed with average loss: 0.0009\n",
      "Epoch [893/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 893 completed with average loss: 0.0009\n",
      "Epoch [894/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 894 completed with average loss: 0.0009\n",
      "Epoch [895/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 895 completed with average loss: 0.0009\n",
      "Epoch [896/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 896 completed with average loss: 0.0009\n",
      "Epoch [897/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 897 completed with average loss: 0.0009\n",
      "Epoch [898/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 898 completed with average loss: 0.0009\n",
      "Epoch [899/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 899 completed with average loss: 0.0009\n",
      "Epoch [900/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 900 completed with average loss: 0.0009\n",
      "Epoch [901/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 901 completed with average loss: 0.0009\n",
      "Epoch [902/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 902 completed with average loss: 0.0009\n",
      "Epoch [903/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 903 completed with average loss: 0.0009\n",
      "Epoch [904/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 904 completed with average loss: 0.0009\n",
      "Epoch [905/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 905 completed with average loss: 0.0009\n",
      "Epoch [906/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 906 completed with average loss: 0.0009\n",
      "Epoch [907/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 907 completed with average loss: 0.0009\n",
      "Epoch [908/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 908 completed with average loss: 0.0009\n",
      "Epoch [909/1000], Step [0/1], Loss: 0.0009\n",
      "Epoch 909 completed with average loss: 0.0009\n",
      "Epoch [910/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 910 completed with average loss: 0.0008\n",
      "Epoch [911/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 911 completed with average loss: 0.0008\n",
      "Epoch [912/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 912 completed with average loss: 0.0008\n",
      "Epoch [913/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 913 completed with average loss: 0.0008\n",
      "Epoch [914/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 914 completed with average loss: 0.0008\n",
      "Epoch [915/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 915 completed with average loss: 0.0008\n",
      "Epoch [916/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 916 completed with average loss: 0.0008\n",
      "Epoch [917/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 917 completed with average loss: 0.0008\n",
      "Epoch [918/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 918 completed with average loss: 0.0008\n",
      "Epoch [919/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 919 completed with average loss: 0.0008\n",
      "Epoch [920/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 920 completed with average loss: 0.0008\n",
      "Epoch [921/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 921 completed with average loss: 0.0008\n",
      "Epoch [922/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 922 completed with average loss: 0.0008\n",
      "Epoch [923/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 923 completed with average loss: 0.0008\n",
      "Epoch [924/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 924 completed with average loss: 0.0008\n",
      "Epoch [925/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 925 completed with average loss: 0.0008\n",
      "Epoch [926/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 926 completed with average loss: 0.0008\n",
      "Epoch [927/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 927 completed with average loss: 0.0008\n",
      "Epoch [928/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 928 completed with average loss: 0.0008\n",
      "Epoch [929/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 929 completed with average loss: 0.0008\n",
      "Epoch [930/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 930 completed with average loss: 0.0008\n",
      "Epoch [931/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 931 completed with average loss: 0.0008\n",
      "Epoch [932/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 932 completed with average loss: 0.0008\n",
      "Epoch [933/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 933 completed with average loss: 0.0008\n",
      "Epoch [934/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 934 completed with average loss: 0.0008\n",
      "Epoch [935/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 935 completed with average loss: 0.0008\n",
      "Epoch [936/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 936 completed with average loss: 0.0008\n",
      "Epoch [937/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 937 completed with average loss: 0.0008\n",
      "Epoch [938/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 938 completed with average loss: 0.0008\n",
      "Epoch [939/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 939 completed with average loss: 0.0008\n",
      "Epoch [940/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 940 completed with average loss: 0.0008\n",
      "Epoch [941/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 941 completed with average loss: 0.0008\n",
      "Epoch [942/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 942 completed with average loss: 0.0008\n",
      "Epoch [943/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 943 completed with average loss: 0.0008\n",
      "Epoch [944/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 944 completed with average loss: 0.0008\n",
      "Epoch [945/1000], Step [0/1], Loss: 0.0008\n",
      "Epoch 945 completed with average loss: 0.0008\n",
      "Epoch [946/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 946 completed with average loss: 0.0007\n",
      "Epoch [947/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 947 completed with average loss: 0.0007\n",
      "Epoch [948/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 948 completed with average loss: 0.0007\n",
      "Epoch [949/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 949 completed with average loss: 0.0007\n",
      "Epoch [950/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 950 completed with average loss: 0.0007\n",
      "Epoch [951/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 951 completed with average loss: 0.0007\n",
      "Epoch [952/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 952 completed with average loss: 0.0007\n",
      "Epoch [953/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 953 completed with average loss: 0.0007\n",
      "Epoch [954/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 954 completed with average loss: 0.0007\n",
      "Epoch [955/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 955 completed with average loss: 0.0007\n",
      "Epoch [956/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 956 completed with average loss: 0.0007\n",
      "Epoch [957/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 957 completed with average loss: 0.0007\n",
      "Epoch [958/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 958 completed with average loss: 0.0007\n",
      "Epoch [959/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 959 completed with average loss: 0.0007\n",
      "Epoch [960/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 960 completed with average loss: 0.0007\n",
      "Epoch [961/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 961 completed with average loss: 0.0007\n",
      "Epoch [962/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 962 completed with average loss: 0.0007\n",
      "Epoch [963/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 963 completed with average loss: 0.0007\n",
      "Epoch [964/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 964 completed with average loss: 0.0007\n",
      "Epoch [965/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 965 completed with average loss: 0.0007\n",
      "Epoch [966/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 966 completed with average loss: 0.0007\n",
      "Epoch [967/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 967 completed with average loss: 0.0007\n",
      "Epoch [968/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 968 completed with average loss: 0.0007\n",
      "Epoch [969/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 969 completed with average loss: 0.0007\n",
      "Epoch [970/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 970 completed with average loss: 0.0007\n",
      "Epoch [971/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 971 completed with average loss: 0.0007\n",
      "Epoch [972/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 972 completed with average loss: 0.0007\n",
      "Epoch [973/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 973 completed with average loss: 0.0007\n",
      "Epoch [974/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 974 completed with average loss: 0.0007\n",
      "Epoch [975/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 975 completed with average loss: 0.0007\n",
      "Epoch [976/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 976 completed with average loss: 0.0007\n",
      "Epoch [977/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 977 completed with average loss: 0.0007\n",
      "Epoch [978/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 978 completed with average loss: 0.0007\n",
      "Epoch [979/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 979 completed with average loss: 0.0007\n",
      "Epoch [980/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 980 completed with average loss: 0.0007\n",
      "Epoch [981/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 981 completed with average loss: 0.0007\n",
      "Epoch [982/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 982 completed with average loss: 0.0007\n",
      "Epoch [983/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 983 completed with average loss: 0.0007\n",
      "Epoch [984/1000], Step [0/1], Loss: 0.0007\n",
      "Epoch 984 completed with average loss: 0.0007\n",
      "Epoch [985/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 985 completed with average loss: 0.0006\n",
      "Epoch [986/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 986 completed with average loss: 0.0006\n",
      "Epoch [987/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 987 completed with average loss: 0.0006\n",
      "Epoch [988/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 988 completed with average loss: 0.0006\n",
      "Epoch [989/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 989 completed with average loss: 0.0006\n",
      "Epoch [990/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 990 completed with average loss: 0.0006\n",
      "Epoch [991/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 991 completed with average loss: 0.0006\n",
      "Epoch [992/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 992 completed with average loss: 0.0006\n",
      "Epoch [993/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 993 completed with average loss: 0.0006\n",
      "Epoch [994/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 994 completed with average loss: 0.0006\n",
      "Epoch [995/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 995 completed with average loss: 0.0006\n",
      "Epoch [996/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 996 completed with average loss: 0.0006\n",
      "Epoch [997/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 997 completed with average loss: 0.0006\n",
      "Epoch [998/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 998 completed with average loss: 0.0006\n",
      "Epoch [999/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 999 completed with average loss: 0.0006\n",
      "Epoch [1000/1000], Step [0/1], Loss: 0.0006\n",
      "Epoch 1000 completed with average loss: 0.0006\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = hw1_helper.train(model, optimizer, train_dataloader, criterion, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x16 and 15x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_data), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (history, future) \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# prediction = model(history)         # Forward pass\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(prediction, future)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# print out test loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[98], line 21\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x16 and 15x20)"
     ]
    }
   ],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    # prediction = model(history)         # Forward pass\n",
    "    prediction = model(future)\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_data)-1, step=1, description='Index:')\n",
    "xlims = [-11, 15]\n",
    "ylims = [-2,2]\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0933, -0.0766, -0.0716, -0.0694, -0.0686, -0.0683, -0.0682, -0.0682,\n",
      "        -0.0682, -0.0682, -0.0682, -0.0682, -0.0682, -0.0682, -0.0682],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([-0.0321,  0.0690,  0.0109, -0.0345, -0.0699, -0.0931, -0.1070, -0.1143,\n",
      "        -0.1172, -0.1173, -0.1156, -0.1128, -0.1094, -0.1057, -0.1019, -0.0980])\n"
     ]
    }
   ],
   "source": [
    "predict = prediction.detach().numpy()\n",
    "tester = future.detach().numpy()\n",
    "print(prediction[0, :])\n",
    "print(future[0, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (16,) and (15,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(t, tester[\u001b[38;5;241m0\u001b[39m, :], label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuture\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m prediction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcceleration\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/matplotlib/pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/matplotlib/axes/_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/matplotlib/axes/_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/matplotlib/axes/_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (16,) and (15,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.linspace(0,16,16)\n",
    "plt.plot(t, tester[0, :], label = 'future')\n",
    "plt.plot(t, predict[0, :], label = ' prediction')\n",
    "plt.legend()\n",
    "plt.title('Acceleration')\n",
    "plt.xlabel('iterations')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (16,) and (15,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(t, tester[\u001b[38;5;241m1\u001b[39m, :], label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuture\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m prediction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSteering\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/matplotlib/pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/matplotlib/axes/_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/matplotlib/axes/_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Decision Making and Control for Safe Autonomy/AA598-aut24/aa598_venv/lib/python3.12/site-packages/matplotlib/axes/_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (16,) and (15,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t, tester[1, :], label = 'future')\n",
    "plt.plot(t, predict[1, :], label = ' prediction')\n",
    "plt.legend()\n",
    "plt.title('Steering')\n",
    "plt.xlabel('iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
